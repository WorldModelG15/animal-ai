{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"UIO-nj5GP-0m"},"outputs":[],"source":["# 初回のみ\n","#!wget -P '/content/drive/My Drive/' http://mdcrosby.com/builds/AnimalAI_LINUX_3.0.zip\n","#!unzip '/content/drive/MyDrive/AnimalAI_LINUX_3.0.zip' -d \"/content/drive/MyDrive/最終課題/animal-ai/\"\n","#!mv /content/drive/MyDrive/最終課題/animal-ai/AnimalAI_LINUX_3.0/* /content/drive/MyDrive/最終課題/animal-ai/env/"]},{"cell_type":"markdown","metadata":{"id":"h2uoBfcWP-0m"},"source":["- tensorboard\n","```\n","tensorboard --logdir /content/drive/MyDrive/最終課題/animal-ai/examples/dqn_tensorboard\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":305,"status":"ok","timestamp":1648269737301,"user":{"displayName":"渋谷和樹","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08698868765567874732"},"user_tz":-540},"id":"MPc0x0PEiDEV","outputId":"5383d44a-ec24-44d8-82ca-48eea287ae26"},"outputs":[{"name":"stdout","output_type":"stream","text":["ls: cannot open directory '.': Transport endpoint is not connected\n"]}],"source":[""]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":105834,"status":"ok","timestamp":1648309722924,"user":{"displayName":"渋谷和樹","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08698868765567874732"},"user_tz":-540},"id":"gpPTLUSYP-0n","outputId":"42b9e034-51e6-4099-d768-20a0a6a0e67d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at ./drive\n","/content/drive/My Drive/最終課題/animal-ai\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.8.1 which is incompatible.\n","torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.8.1 which is incompatible.\n","torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.8.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","stable-baselines3 1.5.0 requires gym==0.21, but you have gym 0.20.0 which is incompatible.\n","mlagents 0.27.0 requires mlagents-envs==0.27.0, but you have mlagents-envs 0.28.0 which is incompatible.\u001b[0m\n","\n","WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n","\n","\n","WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n","\n"]}],"source":["from google.colab import drive\n","drive.mount(\"./drive\")\n","%cd \"/content/drive/My Drive/最終課題/animal-ai\"\n","!pip install -e animalai 1\u003e /dev/null \n","!pip install stable_baselines3 1\u003e /dev/null\n","!pip install gym_unity 1\u003e /dev/null\n","!apt update 1\u003e /dev/null\n","!apt install xvfb 1\u003e /dev/null\n","!pip install stable-baselines 1\u003e /dev/null"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":410,"status":"ok","timestamp":1648309723314,"user":{"displayName":"渋谷和樹","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08698868765567874732"},"user_tz":-540},"id":"cpnd91fwP-0m"},"outputs":[],"source":["!chmod +x /content/drive/MyDrive/animal-ai/env/UnityPlayer.so\n","!chmod -R 755 /content/drive/MyDrive/最終課題/animal-ai/env/AnimalAI.x86_64"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iuNdlRaPP-0n"},"outputs":[],"source":["from stable_baselines3 import DQN\n","from stable_baselines.bench import Monitor\n","import torch as th\n","\n","import sys\n","import random\n","import os\n","# from stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv\n","from gym_unity.envs import UnityToGymWrapper\n","from animalai.animalai.envs.environment import AnimalAIEnvironment"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"7USh4CAeP-0n"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","update_step:  30 model loss: 3.00125, kl_loss: 3.00000, obs_loss: 0.00125, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95475\n","update_step:  40 model loss: 3.00105, kl_loss: 3.00000, obs_loss: 0.00105, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95545\n","update_step:  50 model loss: 3.00100, kl_loss: 3.00000, obs_loss: 0.00100, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95501\n","elasped time for update: 54.27s\n","episode [  70/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.37s\n","update_step:  10 model loss: 3.00095, kl_loss: 3.00000, obs_loss: 0.00095, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95458\n","update_step:  20 model loss: 3.00093, kl_loss: 3.00000, obs_loss: 0.00093, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95422\n","update_step:  30 model loss: 3.00093, kl_loss: 3.00000, obs_loss: 0.00093, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95393\n","update_step:  40 model loss: 3.00091, kl_loss: 3.00000, obs_loss: 0.00091, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95408\n","update_step:  50 model loss: 3.00090, kl_loss: 3.00000, obs_loss: 0.00090, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95447\n","elasped time for update: 54.32s\n","Total test reward at episode [  70/1000] is -0.999200\n","elasped time for test: 1.34s\n","episode [  71/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.37s\n","update_step:  10 model loss: 3.00089, kl_loss: 3.00000, obs_loss: 0.00089, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95447\n","update_step:  20 model loss: 3.00088, kl_loss: 3.00000, obs_loss: 0.00088, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95425\n","update_step:  30 model loss: 3.00087, kl_loss: 3.00000, obs_loss: 0.00087, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95407\n","update_step:  40 model loss: 3.00087, kl_loss: 3.00000, obs_loss: 0.00087, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95343\n","update_step:  50 model loss: 3.00086, kl_loss: 3.00000, obs_loss: 0.00086, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95343\n","elasped time for update: 54.30s\n","episode [  72/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00086, kl_loss: 3.00000, obs_loss: 0.00086, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95327\n","update_step:  20 model loss: 3.00165, kl_loss: 3.00000, obs_loss: 0.00165, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95265\n","update_step:  30 model loss: 3.14189, kl_loss: 3.00000, obs_loss: 0.14189, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95072\n","update_step:  40 model loss: 3.50001, kl_loss: 3.00000, obs_loss: 0.50001, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.93945\n","update_step:  50 model loss: 3.19593, kl_loss: 3.00000, obs_loss: 0.19593, reward_loss: 0.00000, value_loss: 0.00004 action_loss: 0.94089\n","elasped time for update: 54.28s\n","episode [  73/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.44s\n","update_step:  10 model loss: 7.06876, kl_loss: 3.00241, obs_loss: 4.06634, reward_loss: 0.00000, value_loss: 0.00004 action_loss: 0.94391\n","update_step:  20 model loss: 7.70453, kl_loss: 3.00000, obs_loss: 4.70453, reward_loss: 0.00000, value_loss: 0.00008 action_loss: 0.93525\n","update_step:  30 model loss: 5.89938, kl_loss: 3.00159, obs_loss: 2.89779, reward_loss: 0.00000, value_loss: 0.00036 action_loss: 0.92065\n","update_step:  40 model loss: 4.75880, kl_loss: 3.00000, obs_loss: 1.75880, reward_loss: 0.00000, value_loss: 0.00023 action_loss: 0.92847\n","update_step:  50 model loss: 3.43538, kl_loss: 3.00000, obs_loss: 0.43538, reward_loss: 0.00000, value_loss: 0.00017 action_loss: 0.92505\n","elasped time for update: 54.27s\n","episode [  74/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.06032, kl_loss: 3.00000, obs_loss: 0.06032, reward_loss: 0.00000, value_loss: 0.00013 action_loss: 0.93571\n","update_step:  20 model loss: 3.02143, kl_loss: 3.00000, obs_loss: 0.02143, reward_loss: 0.00000, value_loss: 0.00010 action_loss: 0.94607\n","update_step:  30 model loss: 3.02077, kl_loss: 3.00000, obs_loss: 0.02077, reward_loss: 0.00000, value_loss: 0.00008 action_loss: 0.95400\n","update_step:  40 model loss: 3.01469, kl_loss: 3.00000, obs_loss: 0.01468, reward_loss: 0.00000, value_loss: 0.00007 action_loss: 0.95573\n","update_step:  50 model loss: 3.00732, kl_loss: 3.00000, obs_loss: 0.00732, reward_loss: 0.00000, value_loss: 0.00006 action_loss: 0.95554\n","elasped time for update: 54.26s\n","episode [  75/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00510, kl_loss: 3.00000, obs_loss: 0.00510, reward_loss: 0.00000, value_loss: 0.00006 action_loss: 0.95312\n","update_step:  20 model loss: 3.00468, kl_loss: 3.00000, obs_loss: 0.00468, reward_loss: 0.00000, value_loss: 0.00005 action_loss: 0.95431\n","update_step:  30 model loss: 3.00417, kl_loss: 3.00000, obs_loss: 0.00417, reward_loss: 0.00000, value_loss: 0.00005 action_loss: 0.95558\n","update_step:  40 model loss: 3.00376, kl_loss: 3.00000, obs_loss: 0.00376, reward_loss: 0.00000, value_loss: 0.00004 action_loss: 0.95647\n","update_step:  50 model loss: 3.00351, kl_loss: 3.00000, obs_loss: 0.00351, reward_loss: 0.00000, value_loss: 0.00004 action_loss: 0.95679\n","elasped time for update: 54.23s\n","episode [  76/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.37s\n","update_step:  10 model loss: 3.00329, kl_loss: 3.00000, obs_loss: 0.00329, reward_loss: 0.00000, value_loss: 0.00004 action_loss: 0.95598\n","update_step:  20 model loss: 3.00314, kl_loss: 3.00002, obs_loss: 0.00311, reward_loss: 0.00000, value_loss: 0.00004 action_loss: 0.95702\n","update_step:  30 model loss: 3.00296, kl_loss: 3.00000, obs_loss: 0.00295, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95669\n","update_step:  40 model loss: 3.00281, kl_loss: 3.00000, obs_loss: 0.00281, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95784\n","update_step:  50 model loss: 3.00269, kl_loss: 3.00000, obs_loss: 0.00269, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95798\n","elasped time for update: 54.25s\n","episode [  77/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00258, kl_loss: 3.00000, obs_loss: 0.00258, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95703\n","update_step:  20 model loss: 3.00247, kl_loss: 3.00000, obs_loss: 0.00247, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95716\n","update_step:  30 model loss: 3.00239, kl_loss: 3.00000, obs_loss: 0.00239, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95737\n","update_step:  40 model loss: 3.00231, kl_loss: 3.00000, obs_loss: 0.00231, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95710\n","update_step:  50 model loss: 3.00223, kl_loss: 3.00000, obs_loss: 0.00223, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95696\n","elasped time for update: 54.24s\n","episode [  78/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00217, kl_loss: 3.00000, obs_loss: 0.00217, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95699\n","update_step:  20 model loss: 3.00210, kl_loss: 3.00000, obs_loss: 0.00210, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95739\n","update_step:  30 model loss: 3.00203, kl_loss: 3.00000, obs_loss: 0.00203, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95820\n","update_step:  40 model loss: 3.00198, kl_loss: 3.00000, obs_loss: 0.00198, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95710\n","update_step:  50 model loss: 3.00193, kl_loss: 3.00000, obs_loss: 0.00193, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95668\n","elasped time for update: 54.31s\n","episode [  79/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00187, kl_loss: 3.00000, obs_loss: 0.00187, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95759\n","update_step:  20 model loss: 3.00183, kl_loss: 3.00000, obs_loss: 0.00183, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95731\n","update_step:  30 model loss: 3.00179, kl_loss: 3.00000, obs_loss: 0.00179, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95754\n","update_step:  40 model loss: 3.00175, kl_loss: 3.00000, obs_loss: 0.00175, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95727\n","update_step:  50 model loss: 3.00171, kl_loss: 3.00000, obs_loss: 0.00171, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95774\n","elasped time for update: 54.42s\n","episode [  80/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00167, kl_loss: 3.00000, obs_loss: 0.00167, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95711\n","update_step:  20 model loss: 3.00164, kl_loss: 3.00000, obs_loss: 0.00164, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95759\n","update_step:  30 model loss: 3.00160, kl_loss: 3.00000, obs_loss: 0.00160, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95792\n","update_step:  40 model loss: 3.00157, kl_loss: 3.00000, obs_loss: 0.00157, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95753\n","update_step:  50 model loss: 3.00154, kl_loss: 3.00000, obs_loss: 0.00154, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95675\n","elasped time for update: 54.25s\n","Total test reward at episode [  80/1000] is -0.999200\n","elasped time for test: 1.35s\n","episode [  81/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.36s\n","update_step:  10 model loss: 3.00151, kl_loss: 3.00000, obs_loss: 0.00151, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95607\n","update_step:  20 model loss: 3.00148, kl_loss: 3.00000, obs_loss: 0.00148, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95593\n","update_step:  30 model loss: 3.00146, kl_loss: 3.00000, obs_loss: 0.00145, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95533\n","update_step:  40 model loss: 3.00144, kl_loss: 3.00000, obs_loss: 0.00143, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95515\n","update_step:  50 model loss: 3.00142, kl_loss: 3.00000, obs_loss: 0.00141, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95513\n","elasped time for update: 54.24s\n","episode [  82/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00139, kl_loss: 3.00000, obs_loss: 0.00139, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95441\n","update_step:  20 model loss: 3.00136, kl_loss: 3.00000, obs_loss: 0.00136, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95392\n","update_step:  30 model loss: 3.00135, kl_loss: 3.00000, obs_loss: 0.00135, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95278\n","update_step:  40 model loss: 3.00134, kl_loss: 3.00000, obs_loss: 0.00134, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95164\n","update_step:  50 model loss: 3.00130, kl_loss: 3.00000, obs_loss: 0.00130, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95093\n","elasped time for update: 54.20s\n","episode [  83/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.44s\n","update_step:  10 model loss: 3.00129, kl_loss: 3.00000, obs_loss: 0.00129, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.94966\n","update_step:  20 model loss: 3.00126, kl_loss: 3.00000, obs_loss: 0.00126, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.94864\n","update_step:  30 model loss: 3.00125, kl_loss: 3.00000, obs_loss: 0.00125, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.94798\n","update_step:  40 model loss: 3.00124, kl_loss: 3.00000, obs_loss: 0.00124, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.94809\n","update_step:  50 model loss: 3.00122, kl_loss: 3.00000, obs_loss: 0.00122, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.94706\n","elasped time for update: 54.24s\n","episode [  84/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.44s\n","update_step:  10 model loss: 3.00120, kl_loss: 3.00000, obs_loss: 0.00120, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.94600\n","update_step:  20 model loss: 3.00118, kl_loss: 3.00000, obs_loss: 0.00118, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.94605\n","update_step:  30 model loss: 3.00117, kl_loss: 3.00000, obs_loss: 0.00117, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94521\n","update_step:  40 model loss: 3.00116, kl_loss: 3.00000, obs_loss: 0.00116, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94428\n","update_step:  50 model loss: 3.00114, kl_loss: 3.00000, obs_loss: 0.00114, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94383\n","elasped time for update: 54.21s\n","episode [  85/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00112, kl_loss: 3.00000, obs_loss: 0.00112, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94435\n","update_step:  20 model loss: 3.00111, kl_loss: 3.00000, obs_loss: 0.00111, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94434\n","update_step:  30 model loss: 3.00110, kl_loss: 3.00000, obs_loss: 0.00110, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94407\n","update_step:  40 model loss: 3.00108, kl_loss: 3.00000, obs_loss: 0.00108, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94415\n","update_step:  50 model loss: 3.00108, kl_loss: 3.00000, obs_loss: 0.00108, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94413\n","elasped time for update: 54.20s\n","episode [  86/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00106, kl_loss: 3.00000, obs_loss: 0.00106, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94444\n","update_step:  20 model loss: 3.00105, kl_loss: 3.00000, obs_loss: 0.00105, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94495\n","update_step:  30 model loss: 3.00104, kl_loss: 3.00000, obs_loss: 0.00104, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94526\n","update_step:  40 model loss: 3.00103, kl_loss: 3.00000, obs_loss: 0.00103, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94666\n","update_step:  50 model loss: 3.00101, kl_loss: 3.00000, obs_loss: 0.00101, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94793\n","elasped time for update: 54.14s\n","episode [  87/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.37s\n","update_step:  10 model loss: 3.00100, kl_loss: 3.00000, obs_loss: 0.00100, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94902\n","update_step:  20 model loss: 3.00099, kl_loss: 3.00000, obs_loss: 0.00099, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94885\n","update_step:  30 model loss: 3.00098, kl_loss: 3.00000, obs_loss: 0.00098, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94859\n","update_step:  40 model loss: 3.00099, kl_loss: 3.00002, obs_loss: 0.00097, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94950\n","update_step:  50 model loss: 3.00096, kl_loss: 3.00000, obs_loss: 0.00096, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94967\n","elasped time for update: 54.18s\n","episode [  88/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.37s\n","update_step:  10 model loss: 3.00095, kl_loss: 3.00000, obs_loss: 0.00095, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95091\n","update_step:  20 model loss: 3.00094, kl_loss: 3.00000, obs_loss: 0.00094, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95126\n","update_step:  30 model loss: 3.00093, kl_loss: 3.00000, obs_loss: 0.00093, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95141\n","update_step:  40 model loss: 3.00092, kl_loss: 3.00000, obs_loss: 0.00092, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95162\n","update_step:  50 model loss: 3.00091, kl_loss: 3.00000, obs_loss: 0.00091, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95195\n","elasped time for update: 54.17s\n","episode [  89/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.33s\n","update_step:  10 model loss: 3.00090, kl_loss: 3.00000, obs_loss: 0.00090, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95285\n","update_step:  20 model loss: 3.00090, kl_loss: 3.00000, obs_loss: 0.00090, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95311\n","update_step:  30 model loss: 3.00089, kl_loss: 3.00000, obs_loss: 0.00089, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95316\n","update_step:  40 model loss: 3.00088, kl_loss: 3.00000, obs_loss: 0.00088, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95405\n","update_step:  50 model loss: 3.00087, kl_loss: 3.00000, obs_loss: 0.00087, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95436\n","elasped time for update: 54.37s\n","episode [  90/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.34s\n","update_step:  10 model loss: 3.00086, kl_loss: 3.00000, obs_loss: 0.00086, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95504\n","update_step:  20 model loss: 3.00085, kl_loss: 3.00000, obs_loss: 0.00085, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95416\n","update_step:  30 model loss: 3.00084, kl_loss: 3.00000, obs_loss: 0.00084, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95396\n","update_step:  40 model loss: 3.00084, kl_loss: 3.00000, obs_loss: 0.00084, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95384\n","update_step:  50 model loss: 3.00083, kl_loss: 3.00000, obs_loss: 0.00083, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95372\n","elasped time for update: 54.27s\n","Total test reward at episode [  90/1000] is -0.999200\n","elasped time for test: 1.30s\n","episode [  91/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00082, kl_loss: 3.00000, obs_loss: 0.00082, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95399\n","update_step:  20 model loss: 3.00082, kl_loss: 3.00000, obs_loss: 0.00082, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95361\n","update_step:  30 model loss: 3.00080, kl_loss: 3.00000, obs_loss: 0.00080, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95399\n","update_step:  40 model loss: 3.00081, kl_loss: 3.00000, obs_loss: 0.00081, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95589\n","update_step:  50 model loss: 3.00079, kl_loss: 3.00000, obs_loss: 0.00079, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95601\n","elasped time for update: 54.29s\n","episode [  92/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.36s\n","update_step:  10 model loss: 3.00079, kl_loss: 3.00000, obs_loss: 0.00079, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95514\n","update_step:  20 model loss: 3.00079, kl_loss: 3.00000, obs_loss: 0.00079, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95448\n","update_step:  30 model loss: 3.00078, kl_loss: 3.00000, obs_loss: 0.00078, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95415\n","update_step:  40 model loss: 3.00077, kl_loss: 3.00000, obs_loss: 0.00077, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95368\n","update_step:  50 model loss: 3.00076, kl_loss: 3.00000, obs_loss: 0.00076, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95404\n","elasped time for update: 54.13s\n","episode [  93/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00076, kl_loss: 3.00000, obs_loss: 0.00076, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95381\n","update_step:  20 model loss: 3.00075, kl_loss: 3.00000, obs_loss: 0.00075, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95322\n","update_step:  30 model loss: 3.00074, kl_loss: 3.00000, obs_loss: 0.00074, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95305\n","update_step:  40 model loss: 3.00074, kl_loss: 3.00000, obs_loss: 0.00074, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95380\n","update_step:  50 model loss: 3.00073, kl_loss: 3.00000, obs_loss: 0.00073, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95397\n","elasped time for update: 54.21s\n","episode [  94/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00072, kl_loss: 3.00000, obs_loss: 0.00072, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95311\n","update_step:  20 model loss: 3.00072, kl_loss: 3.00000, obs_loss: 0.00072, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95158\n","update_step:  30 model loss: 3.00071, kl_loss: 3.00000, obs_loss: 0.00071, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95159\n","update_step:  40 model loss: 3.00071, kl_loss: 3.00000, obs_loss: 0.00071, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95178\n","update_step:  50 model loss: 3.00070, kl_loss: 3.00000, obs_loss: 0.00070, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95103\n","elasped time for update: 54.30s\n","episode [  95/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00070, kl_loss: 3.00000, obs_loss: 0.00070, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95026\n","update_step:  20 model loss: 3.00069, kl_loss: 3.00000, obs_loss: 0.00069, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95005\n","update_step:  30 model loss: 3.00069, kl_loss: 3.00000, obs_loss: 0.00069, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94988\n","update_step:  40 model loss: 3.00068, kl_loss: 3.00000, obs_loss: 0.00068, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94971\n","update_step:  50 model loss: 3.00067, kl_loss: 3.00000, obs_loss: 0.00067, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94974\n","elasped time for update: 54.40s\n","episode [  96/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00067, kl_loss: 3.00000, obs_loss: 0.00067, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94899\n","update_step:  20 model loss: 3.00068, kl_loss: 3.00000, obs_loss: 0.00068, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94888\n","update_step:  30 model loss: 3.00067, kl_loss: 3.00000, obs_loss: 0.00067, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94848\n","update_step:  40 model loss: 3.00065, kl_loss: 3.00000, obs_loss: 0.00065, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94848\n","update_step:  50 model loss: 3.00065, kl_loss: 3.00000, obs_loss: 0.00065, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94827\n","elasped time for update: 54.28s\n","episode [  97/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00064, kl_loss: 3.00000, obs_loss: 0.00064, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94657\n","update_step:  20 model loss: 3.00065, kl_loss: 3.00000, obs_loss: 0.00065, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94576\n","update_step:  30 model loss: 3.00065, kl_loss: 3.00000, obs_loss: 0.00065, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94624\n","update_step:  40 model loss: 3.00064, kl_loss: 3.00000, obs_loss: 0.00064, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94535\n","update_step:  50 model loss: 3.00133, kl_loss: 3.00000, obs_loss: 0.00133, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94527\n","elasped time for update: 54.21s\n","episode [  98/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.03533, kl_loss: 3.00000, obs_loss: 0.03533, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94478\n","update_step:  20 model loss: 3.00805, kl_loss: 3.00000, obs_loss: 0.00805, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94374\n","update_step:  30 model loss: 3.00978, kl_loss: 3.00000, obs_loss: 0.00978, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94596\n","update_step:  40 model loss: 3.01039, kl_loss: 3.00000, obs_loss: 0.01039, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94993\n","update_step:  50 model loss: 3.00115, kl_loss: 3.00000, obs_loss: 0.00115, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95066\n","elasped time for update: 54.16s\n","episode [  99/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00549, kl_loss: 3.00000, obs_loss: 0.00549, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94864\n","update_step:  20 model loss: 3.00093, kl_loss: 3.00000, obs_loss: 0.00093, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94666\n","update_step:  30 model loss: 3.00093, kl_loss: 3.00000, obs_loss: 0.00093, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94669\n","update_step:  40 model loss: 3.00100, kl_loss: 3.00000, obs_loss: 0.00100, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94629\n","update_step:  50 model loss: 3.00084, kl_loss: 3.00000, obs_loss: 0.00084, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94575\n","elasped time for update: 54.20s\n","episode [ 100/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00074, kl_loss: 3.00000, obs_loss: 0.00074, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94580\n","update_step:  20 model loss: 3.00067, kl_loss: 3.00000, obs_loss: 0.00067, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94566\n","update_step:  30 model loss: 3.00060, kl_loss: 3.00000, obs_loss: 0.00060, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94673\n","update_step:  40 model loss: 3.00058, kl_loss: 3.00000, obs_loss: 0.00058, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94597\n","update_step:  50 model loss: 3.00057, kl_loss: 3.00000, obs_loss: 0.00057, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94487\n","elasped time for update: 54.18s\n","Total test reward at episode [ 100/1000] is -0.999200\n","elasped time for test: 1.37s\n","episode [ 101/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.45s\n","update_step:  10 model loss: 3.00057, kl_loss: 3.00000, obs_loss: 0.00057, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94517\n","update_step:  20 model loss: 3.00057, kl_loss: 3.00000, obs_loss: 0.00057, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94556\n","update_step:  30 model loss: 3.00063, kl_loss: 3.00000, obs_loss: 0.00063, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94506\n","update_step:  40 model loss: 3.00331, kl_loss: 3.00000, obs_loss: 0.00331, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94710\n","update_step:  50 model loss: 3.13428, kl_loss: 3.00000, obs_loss: 0.13428, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94717\n","elasped time for update: 54.14s\n","episode [ 102/1000] is collected. Total reward is 0.397400\n","elasped time for interaction: 0.85s\n","update_step:  10 model loss: 3.02089, kl_loss: 3.00006, obs_loss: 0.02083, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94670\n","update_step:  20 model loss: 3.02792, kl_loss: 3.00000, obs_loss: 0.02792, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94923\n","update_step:  30 model loss: 3.00288, kl_loss: 3.00000, obs_loss: 0.00288, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95093\n","update_step:  40 model loss: 3.00655, kl_loss: 3.00001, obs_loss: 0.00654, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94965\n","update_step:  50 model loss: 3.00088, kl_loss: 3.00000, obs_loss: 0.00088, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94802\n","elasped time for update: 54.25s\n","episode [ 103/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.47s\n","update_step:  10 model loss: 3.00108, kl_loss: 3.00000, obs_loss: 0.00108, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94789\n","update_step:  20 model loss: 3.00070, kl_loss: 3.00000, obs_loss: 0.00070, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94911\n","update_step:  30 model loss: 3.00070, kl_loss: 3.00000, obs_loss: 0.00070, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94967\n","update_step:  40 model loss: 3.00054, kl_loss: 3.00000, obs_loss: 0.00054, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.94997\n","update_step:  50 model loss: 3.00053, kl_loss: 3.00000, obs_loss: 0.00053, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95009\n","elasped time for update: 54.31s\n","episode [ 104/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00054, kl_loss: 3.00000, obs_loss: 0.00054, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.94984\n","update_step:  20 model loss: 3.00056, kl_loss: 3.00000, obs_loss: 0.00056, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95052\n","update_step:  30 model loss: 3.00055, kl_loss: 3.00000, obs_loss: 0.00055, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95104\n","update_step:  40 model loss: 3.00051, kl_loss: 3.00000, obs_loss: 0.00051, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95180\n","update_step:  50 model loss: 3.00054, kl_loss: 3.00000, obs_loss: 0.00054, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95242\n","elasped time for update: 54.29s\n","episode [ 105/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00064, kl_loss: 3.00000, obs_loss: 0.00064, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95209\n","update_step:  20 model loss: 3.00642, kl_loss: 3.00000, obs_loss: 0.00642, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95102\n","update_step:  30 model loss: 3.22017, kl_loss: 3.00000, obs_loss: 0.22017, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95100\n","update_step:  40 model loss: 3.09713, kl_loss: 3.00000, obs_loss: 0.09713, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95053\n","update_step:  50 model loss: 3.00649, kl_loss: 3.00000, obs_loss: 0.00649, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95491\n","elasped time for update: 54.22s\n","episode [ 106/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00540, kl_loss: 3.00001, obs_loss: 0.00539, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95714\n","update_step:  20 model loss: 3.00245, kl_loss: 3.00003, obs_loss: 0.00242, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95636\n","update_step:  30 model loss: 3.00071, kl_loss: 3.00000, obs_loss: 0.00071, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95449\n","update_step:  40 model loss: 3.00068, kl_loss: 3.00000, obs_loss: 0.00068, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95405\n","update_step:  50 model loss: 3.00078, kl_loss: 3.00000, obs_loss: 0.00078, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95417\n","elasped time for update: 54.27s\n","episode [ 107/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00051, kl_loss: 3.00000, obs_loss: 0.00051, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95381\n","update_step:  20 model loss: 3.00054, kl_loss: 3.00000, obs_loss: 0.00054, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95383\n","update_step:  30 model loss: 3.00050, kl_loss: 3.00000, obs_loss: 0.00050, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95377\n","update_step:  40 model loss: 3.00048, kl_loss: 3.00000, obs_loss: 0.00048, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95298\n","update_step:  50 model loss: 3.00047, kl_loss: 3.00000, obs_loss: 0.00047, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95326\n","elasped time for update: 54.18s\n","episode [ 108/1000] is collected. Total reward is 0.014200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00047, kl_loss: 3.00000, obs_loss: 0.00047, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95382\n","update_step:  20 model loss: 3.00047, kl_loss: 3.00000, obs_loss: 0.00047, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95337\n","update_step:  30 model loss: 3.00046, kl_loss: 3.00000, obs_loss: 0.00046, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95285\n","update_step:  40 model loss: 3.00046, kl_loss: 3.00000, obs_loss: 0.00046, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95299\n","update_step:  50 model loss: 3.00046, kl_loss: 3.00000, obs_loss: 0.00046, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95310\n","elasped time for update: 54.20s\n","episode [ 109/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00045, kl_loss: 3.00000, obs_loss: 0.00045, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95272\n","update_step:  20 model loss: 3.00045, kl_loss: 3.00000, obs_loss: 0.00045, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95277\n","update_step:  30 model loss: 3.00046, kl_loss: 3.00000, obs_loss: 0.00046, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95322\n","update_step:  40 model loss: 3.00080, kl_loss: 3.00000, obs_loss: 0.00080, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95362\n","update_step:  50 model loss: 3.02842, kl_loss: 3.00000, obs_loss: 0.02842, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95350\n","elasped time for update: 54.19s\n","episode [ 110/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.10411, kl_loss: 3.00000, obs_loss: 0.10411, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.94997\n","update_step:  20 model loss: 3.08391, kl_loss: 3.00000, obs_loss: 0.08391, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95163\n","update_step:  30 model loss: 3.02989, kl_loss: 3.00000, obs_loss: 0.02989, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95606\n","update_step:  40 model loss: 3.00766, kl_loss: 3.00000, obs_loss: 0.00766, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95635\n","update_step:  50 model loss: 3.00717, kl_loss: 3.00000, obs_loss: 0.00717, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95364\n","elasped time for update: 54.21s\n","Total test reward at episode [ 110/1000] is -0.999200\n","elasped time for test: 1.44s\n","episode [ 111/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00398, kl_loss: 3.00000, obs_loss: 0.00397, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95186\n","update_step:  20 model loss: 3.00131, kl_loss: 3.00000, obs_loss: 0.00131, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95295\n","update_step:  30 model loss: 3.00055, kl_loss: 3.00000, obs_loss: 0.00055, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95483\n","update_step:  40 model loss: 3.00047, kl_loss: 3.00000, obs_loss: 0.00047, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95539\n","update_step:  50 model loss: 3.00044, kl_loss: 3.00000, obs_loss: 0.00044, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95423\n","elasped time for update: 54.14s\n","episode [ 112/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00044, kl_loss: 3.00000, obs_loss: 0.00044, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95399\n","update_step:  20 model loss: 3.00043, kl_loss: 3.00000, obs_loss: 0.00043, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95382\n","update_step:  30 model loss: 3.00043, kl_loss: 3.00000, obs_loss: 0.00043, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95389\n","update_step:  40 model loss: 3.00042, kl_loss: 3.00000, obs_loss: 0.00042, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95347\n","update_step:  50 model loss: 3.00042, kl_loss: 3.00000, obs_loss: 0.00042, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95341\n","elasped time for update: 54.19s\n","episode [ 113/1000] is collected. Total reward is 0.084600\n","elasped time for interaction: 1.34s\n","update_step:  10 model loss: 3.00042, kl_loss: 3.00000, obs_loss: 0.00042, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95398\n","update_step:  20 model loss: 3.00041, kl_loss: 3.00000, obs_loss: 0.00041, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95404\n","update_step:  30 model loss: 3.00041, kl_loss: 3.00000, obs_loss: 0.00041, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95321\n","update_step:  40 model loss: 3.00041, kl_loss: 3.00000, obs_loss: 0.00041, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95279\n","update_step:  50 model loss: 3.00040, kl_loss: 3.00000, obs_loss: 0.00040, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95369\n","elasped time for update: 54.25s\n","episode [ 114/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.45s\n","update_step:  10 model loss: 3.00040, kl_loss: 3.00000, obs_loss: 0.00040, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95369\n","update_step:  20 model loss: 3.00040, kl_loss: 3.00000, obs_loss: 0.00040, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95376\n","update_step:  30 model loss: 3.00039, kl_loss: 3.00000, obs_loss: 0.00039, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95350\n","update_step:  40 model loss: 3.00040, kl_loss: 3.00000, obs_loss: 0.00040, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95323\n","update_step:  50 model loss: 3.00040, kl_loss: 3.00000, obs_loss: 0.00040, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95373\n","elasped time for update: 54.30s\n","episode [ 115/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00041, kl_loss: 3.00000, obs_loss: 0.00041, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95380\n","update_step:  20 model loss: 3.00087, kl_loss: 3.00000, obs_loss: 0.00088, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95363\n","update_step:  30 model loss: 3.01947, kl_loss: 3.00000, obs_loss: 0.01947, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95343\n","update_step:  40 model loss: 3.12623, kl_loss: 3.00000, obs_loss: 0.12623, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95197\n","update_step:  50 model loss: 3.02078, kl_loss: 3.00000, obs_loss: 0.02078, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95365\n","elasped time for update: 54.30s\n","episode [ 116/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00610, kl_loss: 3.00009, obs_loss: 0.00600, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95529\n","update_step:  20 model loss: 3.00952, kl_loss: 3.00000, obs_loss: 0.00952, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95647\n","update_step:  30 model loss: 3.00093, kl_loss: 3.00000, obs_loss: 0.00093, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95636\n","update_step:  40 model loss: 3.00244, kl_loss: 3.00000, obs_loss: 0.00244, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95474\n","update_step:  50 model loss: 3.00073, kl_loss: 3.00000, obs_loss: 0.00073, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95424\n","elasped time for update: 54.30s\n","episode [ 117/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00044, kl_loss: 3.00000, obs_loss: 0.00044, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95447\n","update_step:  20 model loss: 3.00042, kl_loss: 3.00000, obs_loss: 0.00042, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95457\n","update_step:  30 model loss: 3.00039, kl_loss: 3.00000, obs_loss: 0.00039, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95429\n","update_step:  40 model loss: 3.00037, kl_loss: 3.00000, obs_loss: 0.00037, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95456\n","update_step:  50 model loss: 3.00038, kl_loss: 3.00000, obs_loss: 0.00038, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95550\n","elasped time for update: 54.31s\n","episode [ 118/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00039, kl_loss: 3.00000, obs_loss: 0.00039, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95481\n","update_step:  20 model loss: 3.00036, kl_loss: 3.00000, obs_loss: 0.00036, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95419\n","update_step:  30 model loss: 3.00036, kl_loss: 3.00000, obs_loss: 0.00036, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95402\n","update_step:  40 model loss: 3.00037, kl_loss: 3.00000, obs_loss: 0.00037, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95436\n","update_step:  50 model loss: 3.00044, kl_loss: 3.00000, obs_loss: 0.00044, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95489\n","elasped time for update: 54.18s\n","episode [ 119/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00236, kl_loss: 3.00000, obs_loss: 0.00236, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95540\n","update_step:  20 model loss: 3.09845, kl_loss: 3.00000, obs_loss: 0.09845, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95498\n","update_step:  30 model loss: 3.02544, kl_loss: 3.00007, obs_loss: 0.02537, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95221\n","update_step:  40 model loss: 3.01462, kl_loss: 3.00000, obs_loss: 0.01462, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95491\n","update_step:  50 model loss: 3.00353, kl_loss: 3.00000, obs_loss: 0.00353, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95782\n","elasped time for update: 54.21s\n","episode [ 120/1000] is collected. Total reward is 0.114200\n","elasped time for interaction: 1.24s\n","update_step:  10 model loss: 3.00092, kl_loss: 3.00000, obs_loss: 0.00092, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95720\n","update_step:  20 model loss: 3.00058, kl_loss: 3.00000, obs_loss: 0.00058, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95536\n","update_step:  30 model loss: 3.00044, kl_loss: 3.00000, obs_loss: 0.00044, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95502\n","update_step:  40 model loss: 3.00039, kl_loss: 3.00000, obs_loss: 0.00039, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95458\n","update_step:  50 model loss: 3.00037, kl_loss: 3.00000, obs_loss: 0.00037, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95392\n","elasped time for update: 54.15s\n","Total test reward at episode [ 120/1000] is -0.999200\n","elasped time for test: 1.34s\n","episode [ 121/1000] is collected. Total reward is 0.651000\n","elasped time for interaction: 0.53s\n","update_step:  10 model loss: 3.00035, kl_loss: 3.00000, obs_loss: 0.00035, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95318\n","update_step:  20 model loss: 3.00035, kl_loss: 3.00000, obs_loss: 0.00035, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95371\n","update_step:  30 model loss: 3.00034, kl_loss: 3.00000, obs_loss: 0.00034, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95429\n","update_step:  40 model loss: 3.00034, kl_loss: 3.00000, obs_loss: 0.00034, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95404\n","update_step:  50 model loss: 3.00034, kl_loss: 3.00000, obs_loss: 0.00034, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95410\n","elasped time for update: 54.22s\n","episode [ 122/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00033, kl_loss: 3.00000, obs_loss: 0.00033, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95374\n","update_step:  20 model loss: 3.00033, kl_loss: 3.00000, obs_loss: 0.00033, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95378\n","update_step:  30 model loss: 3.00033, kl_loss: 3.00000, obs_loss: 0.00033, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95389\n","update_step:  40 model loss: 3.00033, kl_loss: 3.00000, obs_loss: 0.00033, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95335\n","update_step:  50 model loss: 3.00033, kl_loss: 3.00000, obs_loss: 0.00033, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95330\n","elasped time for update: 54.19s\n","episode [ 123/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00037, kl_loss: 3.00000, obs_loss: 0.00037, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95327\n","update_step:  20 model loss: 3.00255, kl_loss: 3.00000, obs_loss: 0.00255, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95344\n","update_step:  30 model loss: 3.12665, kl_loss: 3.00000, obs_loss: 0.12665, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95285\n","update_step:  40 model loss: 3.01807, kl_loss: 3.00000, obs_loss: 0.01807, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95064\n","update_step:  50 model loss: 3.06039, kl_loss: 3.00000, obs_loss: 0.06039, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95372\n","elasped time for update: 54.18s\n","episode [ 124/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.02383, kl_loss: 3.00000, obs_loss: 0.02383, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95592\n","update_step:  20 model loss: 3.00354, kl_loss: 3.00000, obs_loss: 0.00354, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95503\n","update_step:  30 model loss: 3.00081, kl_loss: 3.00000, obs_loss: 0.00081, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95386\n","update_step:  40 model loss: 3.00167, kl_loss: 3.00000, obs_loss: 0.00167, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95301\n","update_step:  50 model loss: 3.00126, kl_loss: 3.00000, obs_loss: 0.00126, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95306\n","elasped time for update: 54.20s\n","episode [ 125/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.37s\n","update_step:  10 model loss: 3.00037, kl_loss: 3.00000, obs_loss: 0.00037, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95359\n","update_step:  20 model loss: 3.00044, kl_loss: 3.00000, obs_loss: 0.00044, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95397\n","update_step:  30 model loss: 3.00035, kl_loss: 3.00000, obs_loss: 0.00035, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95365\n","update_step:  40 model loss: 3.00032, kl_loss: 3.00000, obs_loss: 0.00032, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95323\n","update_step:  50 model loss: 3.00033, kl_loss: 3.00000, obs_loss: 0.00033, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95403\n","elasped time for update: 54.23s\n","episode [ 126/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00032, kl_loss: 3.00000, obs_loss: 0.00032, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95479\n","update_step:  20 model loss: 3.00031, kl_loss: 3.00000, obs_loss: 0.00031, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95466\n","update_step:  30 model loss: 3.00031, kl_loss: 3.00000, obs_loss: 0.00031, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95398\n","update_step:  40 model loss: 3.00030, kl_loss: 3.00000, obs_loss: 0.00030, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95397\n","update_step:  50 model loss: 3.00030, kl_loss: 3.00000, obs_loss: 0.00030, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95404\n","elasped time for update: 54.10s\n","episode [ 127/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00030, kl_loss: 3.00000, obs_loss: 0.00030, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95386\n","update_step:  20 model loss: 3.00030, kl_loss: 3.00000, obs_loss: 0.00030, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95426\n","update_step:  30 model loss: 3.00030, kl_loss: 3.00000, obs_loss: 0.00030, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95377\n","update_step:  40 model loss: 3.00029, kl_loss: 3.00000, obs_loss: 0.00029, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95357\n","update_step:  50 model loss: 3.00029, kl_loss: 3.00000, obs_loss: 0.00029, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95395\n","elasped time for update: 54.22s\n","episode [ 128/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00038, kl_loss: 3.00000, obs_loss: 0.00038, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95359\n","update_step:  20 model loss: 3.00499, kl_loss: 3.00000, obs_loss: 0.00499, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95374\n","update_step:  30 model loss: 3.19955, kl_loss: 3.00000, obs_loss: 0.19955, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95302\n","update_step:  40 model loss: 3.09155, kl_loss: 3.00000, obs_loss: 0.09155, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95147\n","update_step:  50 model loss: 3.00312, kl_loss: 3.00001, obs_loss: 0.00311, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95387\n","elasped time for update: 54.24s\n","episode [ 129/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00903, kl_loss: 3.00000, obs_loss: 0.00903, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95492\n","update_step:  20 model loss: 3.00104, kl_loss: 3.00000, obs_loss: 0.00104, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95430\n","update_step:  30 model loss: 3.00046, kl_loss: 3.00000, obs_loss: 0.00046, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95385\n","update_step:  40 model loss: 3.00035, kl_loss: 3.00000, obs_loss: 0.00035, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95336\n","update_step:  50 model loss: 3.00035, kl_loss: 3.00000, obs_loss: 0.00035, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95345\n","elasped time for update: 54.20s\n","episode [ 130/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00031, kl_loss: 3.00000, obs_loss: 0.00031, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95357\n","update_step:  20 model loss: 3.00030, kl_loss: 3.00000, obs_loss: 0.00030, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95330\n","update_step:  30 model loss: 3.00029, kl_loss: 3.00000, obs_loss: 0.00029, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95325\n","update_step:  40 model loss: 3.00029, kl_loss: 3.00000, obs_loss: 0.00029, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95379\n","update_step:  50 model loss: 3.00029, kl_loss: 3.00000, obs_loss: 0.00029, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95428\n","elasped time for update: 54.26s\n","Total test reward at episode [ 130/1000] is -0.999200\n","elasped time for test: 1.40s\n","episode [ 131/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00028, kl_loss: 3.00000, obs_loss: 0.00028, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95365\n","update_step:  20 model loss: 3.00028, kl_loss: 3.00000, obs_loss: 0.00028, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95328\n","update_step:  30 model loss: 3.00028, kl_loss: 3.00000, obs_loss: 0.00028, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95343\n","update_step:  40 model loss: 3.00027, kl_loss: 3.00000, obs_loss: 0.00027, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95368\n","update_step:  50 model loss: 3.00027, kl_loss: 3.00000, obs_loss: 0.00027, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95382\n","elasped time for update: 54.20s\n","episode [ 132/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00027, kl_loss: 3.00000, obs_loss: 0.00027, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95400\n","update_step:  20 model loss: 3.00027, kl_loss: 3.00000, obs_loss: 0.00027, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95402\n","update_step:  30 model loss: 3.00042, kl_loss: 3.00005, obs_loss: 0.00037, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95408\n","update_step:  40 model loss: 3.00462, kl_loss: 3.00000, obs_loss: 0.00462, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95416\n","update_step:  50 model loss: 3.25676, kl_loss: 3.00000, obs_loss: 0.25676, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95249\n","elasped time for update: 54.13s\n","episode [ 133/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.09361, kl_loss: 3.00000, obs_loss: 0.09361, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95144\n","update_step:  20 model loss: 3.03838, kl_loss: 3.00000, obs_loss: 0.03838, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95828\n","update_step:  30 model loss: 3.00754, kl_loss: 3.00000, obs_loss: 0.00754, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.96253\n","update_step:  40 model loss: 3.00172, kl_loss: 3.00000, obs_loss: 0.00172, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95965\n","update_step:  50 model loss: 3.00071, kl_loss: 3.00000, obs_loss: 0.00071, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95633\n","elasped time for update: 54.17s\n","episode [ 134/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00059, kl_loss: 3.00000, obs_loss: 0.00059, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95580\n","update_step:  20 model loss: 3.00044, kl_loss: 3.00000, obs_loss: 0.00044, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95606\n","update_step:  30 model loss: 3.00038, kl_loss: 3.00000, obs_loss: 0.00038, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95589\n","update_step:  40 model loss: 3.00032, kl_loss: 3.00000, obs_loss: 0.00032, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95522\n","update_step:  50 model loss: 3.00031, kl_loss: 3.00000, obs_loss: 0.00031, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95557\n","elasped time for update: 54.20s\n","episode [ 135/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00029, kl_loss: 3.00000, obs_loss: 0.00029, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95591\n","update_step:  20 model loss: 3.00027, kl_loss: 3.00000, obs_loss: 0.00027, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95583\n","update_step:  30 model loss: 3.00027, kl_loss: 3.00000, obs_loss: 0.00027, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95552\n","update_step:  40 model loss: 3.00027, kl_loss: 3.00000, obs_loss: 0.00027, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95522\n","update_step:  50 model loss: 3.00026, kl_loss: 3.00000, obs_loss: 0.00026, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95554\n","elasped time for update: 54.24s\n","episode [ 136/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00026, kl_loss: 3.00000, obs_loss: 0.00026, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95571\n","update_step:  20 model loss: 3.00026, kl_loss: 3.00000, obs_loss: 0.00026, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95530\n","update_step:  30 model loss: 3.00026, kl_loss: 3.00000, obs_loss: 0.00026, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95563\n","update_step:  40 model loss: 3.00025, kl_loss: 3.00000, obs_loss: 0.00025, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95587\n","update_step:  50 model loss: 3.00025, kl_loss: 3.00000, obs_loss: 0.00025, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95542\n","elasped time for update: 54.21s\n","episode [ 137/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00025, kl_loss: 3.00000, obs_loss: 0.00025, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95498\n","update_step:  20 model loss: 3.00025, kl_loss: 3.00000, obs_loss: 0.00025, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95522\n","update_step:  30 model loss: 3.00025, kl_loss: 3.00000, obs_loss: 0.00025, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95532\n","update_step:  40 model loss: 3.00025, kl_loss: 3.00000, obs_loss: 0.00025, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95530\n","update_step:  50 model loss: 3.00025, kl_loss: 3.00000, obs_loss: 0.00025, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95508\n","elasped time for update: 54.16s\n","episode [ 138/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00024, kl_loss: 3.00000, obs_loss: 0.00024, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95458\n","update_step:  20 model loss: 3.00025, kl_loss: 3.00000, obs_loss: 0.00025, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95473\n","update_step:  30 model loss: 3.00026, kl_loss: 3.00000, obs_loss: 0.00026, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95502\n","update_step:  40 model loss: 3.00075, kl_loss: 3.00000, obs_loss: 0.00075, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95528\n","update_step:  50 model loss: 3.03800, kl_loss: 3.00000, obs_loss: 0.03800, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95457\n","elasped time for update: 54.20s\n","episode [ 139/1000] is collected. Total reward is 0.153400\n","elasped time for interaction: 1.18s\n","update_step:  10 model loss: 3.02885, kl_loss: 3.00000, obs_loss: 0.02885, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95113\n","update_step:  20 model loss: 3.05227, kl_loss: 3.00000, obs_loss: 0.05227, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95335\n","update_step:  30 model loss: 3.00424, kl_loss: 3.00000, obs_loss: 0.00424, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95609\n","update_step:  40 model loss: 3.00150, kl_loss: 3.00000, obs_loss: 0.00150, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95656\n","update_step:  50 model loss: 3.00168, kl_loss: 3.00000, obs_loss: 0.00168, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95580\n","elasped time for update: 54.27s\n","episode [ 140/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00116, kl_loss: 3.00001, obs_loss: 0.00114, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95528\n","update_step:  20 model loss: 3.00049, kl_loss: 3.00000, obs_loss: 0.00049, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95529\n","update_step:  30 model loss: 3.00028, kl_loss: 3.00000, obs_loss: 0.00028, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95534\n","update_step:  40 model loss: 3.00029, kl_loss: 3.00000, obs_loss: 0.00029, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95500\n","update_step:  50 model loss: 3.00025, kl_loss: 3.00000, obs_loss: 0.00025, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95476\n","elasped time for update: 54.35s\n","Total test reward at episode [ 140/1000] is -0.999200\n","elasped time for test: 1.32s\n","episode [ 141/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00026, kl_loss: 3.00000, obs_loss: 0.00026, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95471\n","update_step:  20 model loss: 3.00024, kl_loss: 3.00000, obs_loss: 0.00024, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95515\n","update_step:  30 model loss: 3.00024, kl_loss: 3.00000, obs_loss: 0.00024, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95526\n","update_step:  40 model loss: 3.00024, kl_loss: 3.00000, obs_loss: 0.00024, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95510\n","update_step:  50 model loss: 3.00024, kl_loss: 3.00000, obs_loss: 0.00024, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95456\n","elasped time for update: 54.36s\n","episode [ 142/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00023, kl_loss: 3.00000, obs_loss: 0.00023, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95410\n","update_step:  20 model loss: 3.00023, kl_loss: 3.00000, obs_loss: 0.00023, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95421\n","update_step:  30 model loss: 3.00023, kl_loss: 3.00000, obs_loss: 0.00023, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95429\n","update_step:  40 model loss: 3.00023, kl_loss: 3.00000, obs_loss: 0.00023, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95508\n","update_step:  50 model loss: 3.00023, kl_loss: 3.00000, obs_loss: 0.00023, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95522\n","elasped time for update: 54.24s\n","episode [ 143/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.37s\n","update_step:  10 model loss: 3.00022, kl_loss: 3.00000, obs_loss: 0.00022, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95451\n","update_step:  20 model loss: 3.00022, kl_loss: 3.00000, obs_loss: 0.00022, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95414\n","update_step:  30 model loss: 3.00022, kl_loss: 3.00000, obs_loss: 0.00022, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95467\n","update_step:  40 model loss: 3.00022, kl_loss: 3.00000, obs_loss: 0.00022, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95512\n","update_step:  50 model loss: 3.00024, kl_loss: 3.00000, obs_loss: 0.00024, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95490\n","elasped time for update: 54.25s\n","episode [ 144/1000] is collected. Total reward is 0.891800\n","elasped time for interaction: 0.16s\n","update_step:  10 model loss: 3.00297, kl_loss: 3.00000, obs_loss: 0.00297, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95447\n","update_step:  20 model loss: 3.31988, kl_loss: 3.00000, obs_loss: 0.31988, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95308\n","update_step:  30 model loss: 3.01596, kl_loss: 3.00000, obs_loss: 0.01596, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95230\n","update_step:  40 model loss: 3.00880, kl_loss: 3.00000, obs_loss: 0.00879, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95664\n","update_step:  50 model loss: 3.00253, kl_loss: 3.00009, obs_loss: 0.00244, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95741\n","elasped time for update: 54.26s\n","episode [ 145/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00081, kl_loss: 3.00000, obs_loss: 0.00081, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95574\n","update_step:  20 model loss: 3.00042, kl_loss: 3.00000, obs_loss: 0.00042, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95451\n","update_step:  30 model loss: 3.00033, kl_loss: 3.00000, obs_loss: 0.00033, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95482\n","update_step:  40 model loss: 3.00027, kl_loss: 3.00000, obs_loss: 0.00027, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95497\n","update_step:  50 model loss: 3.00025, kl_loss: 3.00000, obs_loss: 0.00025, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95470\n","elasped time for update: 54.22s\n","episode [ 146/1000] is collected. Total reward is 0.220600\n","elasped time for interaction: 1.10s\n","update_step:  10 model loss: 3.00024, kl_loss: 3.00000, obs_loss: 0.00024, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95436\n","update_step:  20 model loss: 3.00023, kl_loss: 3.00000, obs_loss: 0.00023, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95409\n","update_step:  30 model loss: 3.00023, kl_loss: 3.00000, obs_loss: 0.00023, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95438\n","update_step:  40 model loss: 3.00023, kl_loss: 3.00000, obs_loss: 0.00023, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95464\n","update_step:  50 model loss: 3.00022, kl_loss: 3.00000, obs_loss: 0.00022, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95452\n","elasped time for update: 54.28s\n","episode [ 147/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00022, kl_loss: 3.00000, obs_loss: 0.00022, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95420\n","update_step:  20 model loss: 3.00022, kl_loss: 3.00000, obs_loss: 0.00022, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95454\n","update_step:  30 model loss: 3.00022, kl_loss: 3.00000, obs_loss: 0.00022, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95496\n","update_step:  40 model loss: 3.00021, kl_loss: 3.00000, obs_loss: 0.00021, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95481\n","update_step:  50 model loss: 3.00021, kl_loss: 3.00000, obs_loss: 0.00021, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95400\n","elasped time for update: 54.32s\n","episode [ 148/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00021, kl_loss: 3.00000, obs_loss: 0.00021, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95401\n","update_step:  20 model loss: 3.00021, kl_loss: 3.00000, obs_loss: 0.00021, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95462\n","update_step:  30 model loss: 3.00021, kl_loss: 3.00000, obs_loss: 0.00021, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95463\n","update_step:  40 model loss: 3.00021, kl_loss: 3.00000, obs_loss: 0.00021, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95476\n","update_step:  50 model loss: 3.00021, kl_loss: 3.00000, obs_loss: 0.00021, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95473\n","elasped time for update: 54.30s\n","episode [ 149/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00021, kl_loss: 3.00000, obs_loss: 0.00021, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95497\n","update_step:  20 model loss: 3.00020, kl_loss: 3.00000, obs_loss: 0.00020, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95501\n","update_step:  30 model loss: 3.00020, kl_loss: 3.00000, obs_loss: 0.00020, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95478\n","update_step:  40 model loss: 3.00022, kl_loss: 3.00000, obs_loss: 0.00022, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95477\n","update_step:  50 model loss: 3.00051, kl_loss: 3.00000, obs_loss: 0.00051, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95465\n","elasped time for update: 54.31s\n","episode [ 150/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.44s\n","update_step:  10 model loss: 3.01972, kl_loss: 3.00000, obs_loss: 0.01972, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95409\n","update_step:  20 model loss: 3.10209, kl_loss: 3.00000, obs_loss: 0.10209, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95022\n","update_step:  30 model loss: 3.01910, kl_loss: 3.00000, obs_loss: 0.01910, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95915\n","update_step:  40 model loss: 3.00112, kl_loss: 3.00000, obs_loss: 0.00112, reward_loss: 0.00000, value_loss: 0.00004 action_loss: 0.96277\n","update_step:  50 model loss: 3.00151, kl_loss: 3.00000, obs_loss: 0.00151, reward_loss: 0.00000, value_loss: 0.00004 action_loss: 0.95993\n","elasped time for update: 54.28s\n","Total test reward at episode [ 150/1000] is -0.999200\n","elasped time for test: 1.37s\n","episode [ 151/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.44s\n","update_step:  10 model loss: 3.00066, kl_loss: 3.00000, obs_loss: 0.00066, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95769\n","update_step:  20 model loss: 3.00038, kl_loss: 3.00000, obs_loss: 0.00038, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95566\n","update_step:  30 model loss: 3.00024, kl_loss: 3.00000, obs_loss: 0.00024, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95523\n","update_step:  40 model loss: 3.00022, kl_loss: 3.00000, obs_loss: 0.00022, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95593\n","update_step:  50 model loss: 3.00021, kl_loss: 3.00000, obs_loss: 0.00021, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95595\n","elasped time for update: 54.17s\n","episode [ 152/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00021, kl_loss: 3.00000, obs_loss: 0.00021, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95523\n","update_step:  20 model loss: 3.00020, kl_loss: 3.00000, obs_loss: 0.00020, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95556\n","update_step:  30 model loss: 3.00021, kl_loss: 3.00000, obs_loss: 0.00021, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95560\n","update_step:  40 model loss: 3.00020, kl_loss: 3.00000, obs_loss: 0.00020, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95566\n","update_step:  50 model loss: 3.00020, kl_loss: 3.00000, obs_loss: 0.00020, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95581\n","elasped time for update: 54.30s\n","episode [ 153/1000] is collected. Total reward is 0.920600\n","elasped time for interaction: 0.12s\n","update_step:  10 model loss: 3.00020, kl_loss: 3.00000, obs_loss: 0.00020, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95580\n","update_step:  20 model loss: 3.00037, kl_loss: 3.00000, obs_loss: 0.00037, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95581\n","update_step:  30 model loss: 3.01255, kl_loss: 3.00000, obs_loss: 0.01255, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95535\n","update_step:  40 model loss: 3.16321, kl_loss: 3.00000, obs_loss: 0.16321, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95039\n","update_step:  50 model loss: 3.00696, kl_loss: 3.00000, obs_loss: 0.00696, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95055\n","elasped time for update: 54.23s\n","episode [ 154/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00426, kl_loss: 3.00000, obs_loss: 0.00426, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95630\n","update_step:  20 model loss: 3.00480, kl_loss: 3.00000, obs_loss: 0.00480, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95723\n","update_step:  30 model loss: 3.00128, kl_loss: 3.00000, obs_loss: 0.00128, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95417\n","update_step:  40 model loss: 3.00034, kl_loss: 3.00000, obs_loss: 0.00034, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95376\n","update_step:  50 model loss: 3.00034, kl_loss: 3.00000, obs_loss: 0.00034, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95416\n","elasped time for update: 54.23s\n","episode [ 155/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00022, kl_loss: 3.00000, obs_loss: 0.00022, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95419\n","update_step:  20 model loss: 3.00024, kl_loss: 3.00000, obs_loss: 0.00024, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95370\n","update_step:  30 model loss: 3.00020, kl_loss: 3.00000, obs_loss: 0.00020, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95368\n","update_step:  40 model loss: 3.00020, kl_loss: 3.00000, obs_loss: 0.00020, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95423\n","update_step:  50 model loss: 3.00019, kl_loss: 3.00000, obs_loss: 0.00019, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95395\n","elasped time for update: 54.29s\n","episode [ 156/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.44s\n","update_step:  10 model loss: 3.00019, kl_loss: 3.00000, obs_loss: 0.00019, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95354\n","update_step:  20 model loss: 3.00019, kl_loss: 3.00000, obs_loss: 0.00019, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95390\n","update_step:  30 model loss: 3.00019, kl_loss: 3.00000, obs_loss: 0.00019, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95373\n","update_step:  40 model loss: 3.00019, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95342\n","update_step:  50 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95387\n","elasped time for update: 54.22s\n","episode [ 157/1000] is collected. Total reward is 0.927000\n","elasped time for interaction: 0.11s\n","update_step:  10 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95397\n","update_step:  20 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95416\n","update_step:  30 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95387\n","update_step:  40 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95421\n","update_step:  50 model loss: 3.00027, kl_loss: 3.00000, obs_loss: 0.00027, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95462\n","elasped time for update: 54.23s\n","episode [ 158/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.37s\n","update_step:  10 model loss: 3.00455, kl_loss: 3.00000, obs_loss: 0.00455, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95531\n","update_step:  20 model loss: 3.21615, kl_loss: 3.00000, obs_loss: 0.21615, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95272\n","update_step:  30 model loss: 3.06254, kl_loss: 3.00031, obs_loss: 0.06224, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95247\n","update_step:  40 model loss: 3.02206, kl_loss: 3.00000, obs_loss: 0.02206, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95851\n","update_step:  50 model loss: 3.00198, kl_loss: 3.00000, obs_loss: 0.00198, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95341\n","elasped time for update: 54.22s\n","episode [ 159/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00065, kl_loss: 3.00000, obs_loss: 0.00065, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.95107\n","update_step:  20 model loss: 3.00094, kl_loss: 3.00000, obs_loss: 0.00094, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95205\n","update_step:  30 model loss: 3.00058, kl_loss: 3.00000, obs_loss: 0.00058, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.95284\n","update_step:  40 model loss: 3.00023, kl_loss: 3.00000, obs_loss: 0.00023, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95341\n","update_step:  50 model loss: 3.00023, kl_loss: 3.00000, obs_loss: 0.00023, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95419\n","elasped time for update: 54.20s\n","episode [ 160/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00021, kl_loss: 3.00000, obs_loss: 0.00021, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95508\n","update_step:  20 model loss: 3.00020, kl_loss: 3.00000, obs_loss: 0.00020, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95539\n","update_step:  30 model loss: 3.00019, kl_loss: 3.00000, obs_loss: 0.00019, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95533\n","update_step:  40 model loss: 3.00019, kl_loss: 3.00000, obs_loss: 0.00019, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95479\n","update_step:  50 model loss: 3.00019, kl_loss: 3.00000, obs_loss: 0.00019, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95544\n","elasped time for update: 54.21s\n","Total test reward at episode [ 160/1000] is -0.999200\n","elasped time for test: 1.35s\n","episode [ 161/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95555\n","update_step:  20 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95491\n","update_step:  30 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95555\n","update_step:  40 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95628\n","update_step:  50 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95623\n","elasped time for update: 54.24s\n","episode [ 162/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00017, kl_loss: 3.00000, obs_loss: 0.00017, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95590\n","update_step:  20 model loss: 3.00017, kl_loss: 3.00000, obs_loss: 0.00017, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95590\n","update_step:  30 model loss: 3.00017, kl_loss: 3.00000, obs_loss: 0.00017, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95554\n","update_step:  40 model loss: 3.00017, kl_loss: 3.00000, obs_loss: 0.00017, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95503\n","update_step:  50 model loss: 3.00023, kl_loss: 3.00000, obs_loss: 0.00023, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95533\n","elasped time for update: 54.24s\n","episode [ 163/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.37s\n","update_step:  10 model loss: 3.00207, kl_loss: 3.00000, obs_loss: 0.00207, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95546\n","update_step:  20 model loss: 3.09875, kl_loss: 3.00000, obs_loss: 0.09875, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95481\n","update_step:  30 model loss: 3.02179, kl_loss: 3.00000, obs_loss: 0.02179, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95211\n","update_step:  40 model loss: 3.01568, kl_loss: 3.00007, obs_loss: 0.01562, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95673\n","update_step:  50 model loss: 3.00222, kl_loss: 3.00000, obs_loss: 0.00222, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95993\n","elasped time for update: 54.20s\n","episode [ 164/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.36s\n","update_step:  10 model loss: 3.00100, kl_loss: 3.00000, obs_loss: 0.00100, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95692\n","update_step:  20 model loss: 3.00047, kl_loss: 3.00000, obs_loss: 0.00047, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95440\n","update_step:  30 model loss: 3.00028, kl_loss: 3.00000, obs_loss: 0.00028, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95423\n","update_step:  40 model loss: 3.00021, kl_loss: 3.00000, obs_loss: 0.00021, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95463\n","update_step:  50 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95447\n","elasped time for update: 54.25s\n","episode [ 165/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.37s\n","update_step:  10 model loss: 3.00017, kl_loss: 3.00000, obs_loss: 0.00017, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95446\n","update_step:  20 model loss: 3.00017, kl_loss: 3.00000, obs_loss: 0.00017, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95509\n","update_step:  30 model loss: 3.00017, kl_loss: 3.00000, obs_loss: 0.00017, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95558\n","update_step:  40 model loss: 3.00017, kl_loss: 3.00000, obs_loss: 0.00017, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95558\n","update_step:  50 model loss: 3.00016, kl_loss: 3.00000, obs_loss: 0.00016, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95458\n","elasped time for update: 54.19s\n","episode [ 166/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00016, kl_loss: 3.00000, obs_loss: 0.00016, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95438\n","update_step:  20 model loss: 3.00016, kl_loss: 3.00000, obs_loss: 0.00016, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95497\n","update_step:  30 model loss: 3.00016, kl_loss: 3.00000, obs_loss: 0.00016, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95520\n","update_step:  40 model loss: 3.00017, kl_loss: 3.00000, obs_loss: 0.00017, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95484\n","update_step:  50 model loss: 3.00052, kl_loss: 3.00000, obs_loss: 0.00052, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95433\n","elasped time for update: 54.17s\n","episode [ 167/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.01583, kl_loss: 3.00000, obs_loss: 0.01583, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95452\n","update_step:  20 model loss: 3.12436, kl_loss: 3.00000, obs_loss: 0.12436, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95273\n","update_step:  30 model loss: 3.00662, kl_loss: 3.00000, obs_loss: 0.00662, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95549\n","update_step:  40 model loss: 3.01778, kl_loss: 3.00000, obs_loss: 0.01778, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95867\n","update_step:  50 model loss: 3.00111, kl_loss: 3.00000, obs_loss: 0.00111, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95836\n","elasped time for update: 54.17s\n","episode [ 168/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00229, kl_loss: 3.00000, obs_loss: 0.00229, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95661\n","update_step:  20 model loss: 3.00129, kl_loss: 3.00000, obs_loss: 0.00129, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95572\n","update_step:  30 model loss: 3.00022, kl_loss: 3.00000, obs_loss: 0.00022, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95470\n","update_step:  40 model loss: 3.00037, kl_loss: 3.00000, obs_loss: 0.00037, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95343\n","update_step:  50 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95386\n","elasped time for update: 54.09s\n","episode [ 169/1000] is collected. Total reward is 0.915800\n","elasped time for interaction: 0.14s\n","update_step:  10 model loss: 3.00020, kl_loss: 3.00000, obs_loss: 0.00020, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95496\n","update_step:  20 model loss: 3.00016, kl_loss: 3.00000, obs_loss: 0.00016, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95540\n","update_step:  30 model loss: 3.00016, kl_loss: 3.00000, obs_loss: 0.00016, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95563\n","update_step:  40 model loss: 3.00016, kl_loss: 3.00000, obs_loss: 0.00016, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95538\n","update_step:  50 model loss: 3.00016, kl_loss: 3.00000, obs_loss: 0.00016, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95505\n","elasped time for update: 54.22s\n","episode [ 170/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.36s\n","update_step:  10 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95564\n","update_step:  20 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95589\n","update_step:  30 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95571\n","update_step:  40 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95513\n","update_step:  50 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95552\n","elasped time for update: 54.21s\n","Total test reward at episode [ 170/1000] is -0.999200\n","elasped time for test: 1.33s\n","episode [ 171/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95596\n","update_step:  20 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95614\n","update_step:  30 model loss: 3.00019, kl_loss: 3.00000, obs_loss: 0.00019, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95572\n","update_step:  40 model loss: 3.00657, kl_loss: 3.00000, obs_loss: 0.00657, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95529\n","update_step:  50 model loss: 3.30801, kl_loss: 3.00000, obs_loss: 0.30801, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95119\n","elasped time for update: 54.23s\n","episode [ 172/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00699, kl_loss: 3.00000, obs_loss: 0.00699, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95242\n","update_step:  20 model loss: 3.00222, kl_loss: 3.00000, obs_loss: 0.00222, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95959\n","update_step:  30 model loss: 3.00272, kl_loss: 3.00000, obs_loss: 0.00272, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.96305\n","update_step:  40 model loss: 3.00389, kl_loss: 3.00000, obs_loss: 0.00389, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95882\n","update_step:  50 model loss: 3.00207, kl_loss: 3.00000, obs_loss: 0.00207, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95843\n","elasped time for update: 54.28s\n","episode [ 173/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00083, kl_loss: 3.00000, obs_loss: 0.00083, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.96043\n","update_step:  20 model loss: 3.00033, kl_loss: 3.00000, obs_loss: 0.00033, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.96095\n","update_step:  30 model loss: 3.00022, kl_loss: 3.00000, obs_loss: 0.00022, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.96021\n","update_step:  40 model loss: 3.00019, kl_loss: 3.00000, obs_loss: 0.00019, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95970\n","update_step:  50 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95991\n","elasped time for update: 54.19s\n","episode [ 174/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.36s\n","update_step:  10 model loss: 3.00017, kl_loss: 3.00000, obs_loss: 0.00017, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95988\n","update_step:  20 model loss: 3.00016, kl_loss: 3.00000, obs_loss: 0.00016, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96002\n","update_step:  30 model loss: 3.00016, kl_loss: 3.00000, obs_loss: 0.00016, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95991\n","update_step:  40 model loss: 3.00016, kl_loss: 3.00000, obs_loss: 0.00016, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95980\n","update_step:  50 model loss: 3.00016, kl_loss: 3.00000, obs_loss: 0.00016, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95982\n","elasped time for update: 54.12s\n","episode [ 175/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.35s\n","update_step:  10 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95930\n","update_step:  20 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95937\n","update_step:  30 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95921\n","update_step:  40 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95917\n","update_step:  50 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95965\n","elasped time for update: 54.17s\n","episode [ 176/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.37s\n","update_step:  10 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95982\n","update_step:  20 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95968\n","update_step:  30 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95914\n","update_step:  40 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95889\n","update_step:  50 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95882\n","elasped time for update: 54.29s\n","episode [ 177/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.37s\n","update_step:  10 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95953\n","update_step:  20 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95945\n","update_step:  30 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95906\n","update_step:  40 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95890\n","update_step:  50 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95876\n","elasped time for update: 54.21s\n","episode [ 178/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95893\n","update_step:  20 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95892\n","update_step:  30 model loss: 3.00052, kl_loss: 3.00000, obs_loss: 0.00052, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95873\n","update_step:  40 model loss: 3.05876, kl_loss: 3.00000, obs_loss: 0.05876, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95715\n","update_step:  50 model loss: 3.01569, kl_loss: 3.00000, obs_loss: 0.01569, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.94928\n","elasped time for update: 54.24s\n","episode [ 179/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00628, kl_loss: 3.00000, obs_loss: 0.00628, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95601\n","update_step:  20 model loss: 3.00349, kl_loss: 3.00000, obs_loss: 0.00349, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95806\n","update_step:  30 model loss: 3.00080, kl_loss: 3.00000, obs_loss: 0.00080, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95535\n","update_step:  40 model loss: 3.00036, kl_loss: 3.00000, obs_loss: 0.00036, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95307\n","update_step:  50 model loss: 3.00027, kl_loss: 3.00000, obs_loss: 0.00027, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95369\n","elasped time for update: 54.15s\n","episode [ 180/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00020, kl_loss: 3.00000, obs_loss: 0.00020, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95425\n","update_step:  20 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95431\n","update_step:  30 model loss: 3.00017, kl_loss: 3.00000, obs_loss: 0.00017, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95441\n","update_step:  40 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95448\n","update_step:  50 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95460\n","elasped time for update: 54.23s\n","Total test reward at episode [ 180/1000] is -0.999200\n","elasped time for test: 1.32s\n","episode [ 181/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95467\n","update_step:  20 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95454\n","update_step:  30 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95516\n","update_step:  40 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95498\n","update_step:  50 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95474\n","elasped time for update: 54.17s\n","episode [ 182/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.35s\n","update_step:  10 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95483\n","update_step:  20 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95505\n","update_step:  30 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95513\n","update_step:  40 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95477\n","update_step:  50 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95514\n","elasped time for update: 54.17s\n","episode [ 183/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.36s\n","update_step:  10 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95544\n","update_step:  20 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95526\n","update_step:  30 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95498\n","update_step:  40 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95495\n","update_step:  50 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95511\n","elasped time for update: 54.17s\n","episode [ 184/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95528\n","update_step:  20 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95543\n","update_step:  30 model loss: 3.00052, kl_loss: 3.00000, obs_loss: 0.00052, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95545\n","update_step:  40 model loss: 3.02929, kl_loss: 3.00000, obs_loss: 0.02929, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95484\n","update_step:  50 model loss: 3.02171, kl_loss: 3.00000, obs_loss: 0.02171, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95291\n","elasped time for update: 54.17s\n","episode [ 185/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.03298, kl_loss: 3.00000, obs_loss: 0.03298, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95712\n","update_step:  20 model loss: 3.00162, kl_loss: 3.00000, obs_loss: 0.00162, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95646\n","update_step:  30 model loss: 3.00078, kl_loss: 3.00000, obs_loss: 0.00078, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95674\n","update_step:  40 model loss: 3.00114, kl_loss: 3.00000, obs_loss: 0.00114, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95934\n","update_step:  50 model loss: 3.00075, kl_loss: 3.00000, obs_loss: 0.00075, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96015\n","elasped time for update: 54.18s\n","episode [ 186/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.35s\n","update_step:  10 model loss: 3.00021, kl_loss: 3.00000, obs_loss: 0.00021, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96003\n","update_step:  20 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95913\n","update_step:  30 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95818\n","update_step:  40 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95865\n","update_step:  50 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95895\n","elasped time for update: 54.23s\n","episode [ 187/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95890\n","update_step:  20 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95859\n","update_step:  30 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95862\n","update_step:  40 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95842\n","update_step:  50 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95845\n","elasped time for update: 54.28s\n","episode [ 188/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95852\n","update_step:  20 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95880\n","update_step:  30 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95846\n","update_step:  40 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95802\n","update_step:  50 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95795\n","elasped time for update: 54.18s\n","episode [ 189/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95818\n","update_step:  20 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95817\n","update_step:  30 model loss: 3.00025, kl_loss: 3.00000, obs_loss: 0.00025, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95811\n","update_step:  40 model loss: 3.00828, kl_loss: 3.00000, obs_loss: 0.00828, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95791\n","update_step:  50 model loss: 3.13897, kl_loss: 3.00000, obs_loss: 0.13897, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95214\n","elasped time for update: 54.23s\n","episode [ 190/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00417, kl_loss: 3.00000, obs_loss: 0.00417, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95407\n","update_step:  20 model loss: 3.01490, kl_loss: 3.00000, obs_loss: 0.01490, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95961\n","update_step:  30 model loss: 3.00144, kl_loss: 3.00000, obs_loss: 0.00144, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.96224\n","update_step:  40 model loss: 3.00042, kl_loss: 3.00000, obs_loss: 0.00042, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95785\n","update_step:  50 model loss: 3.00057, kl_loss: 3.00000, obs_loss: 0.00057, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95589\n","elasped time for update: 54.19s\n","Total test reward at episode [ 190/1000] is -0.999200\n","elasped time for test: 1.34s\n","episode [ 191/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00026, kl_loss: 3.00000, obs_loss: 0.00026, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95668\n","update_step:  20 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95714\n","update_step:  30 model loss: 3.00016, kl_loss: 3.00000, obs_loss: 0.00016, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95698\n","update_step:  40 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95681\n","update_step:  50 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95655\n","elasped time for update: 54.24s\n","episode [ 192/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.45s\n","update_step:  10 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95663\n","update_step:  20 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95712\n","update_step:  30 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95712\n","update_step:  40 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95693\n","update_step:  50 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95678\n","elasped time for update: 54.17s\n","episode [ 193/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95701\n","update_step:  20 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95723\n","update_step:  30 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95702\n","update_step:  40 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95683\n","update_step:  50 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95691\n","elasped time for update: 54.28s\n","episode [ 194/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95699\n","update_step:  20 model loss: 3.00081, kl_loss: 3.00000, obs_loss: 0.00081, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95694\n","update_step:  30 model loss: 3.02839, kl_loss: 3.00000, obs_loss: 0.02839, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95609\n","update_step:  40 model loss: 3.01553, kl_loss: 3.00000, obs_loss: 0.01553, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95225\n","update_step:  50 model loss: 3.00628, kl_loss: 3.00000, obs_loss: 0.00628, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95644\n","elasped time for update: 54.18s\n","episode [ 195/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00767, kl_loss: 3.00000, obs_loss: 0.00767, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95909\n","update_step:  20 model loss: 3.00294, kl_loss: 3.00000, obs_loss: 0.00294, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95897\n","update_step:  30 model loss: 3.00145, kl_loss: 3.00000, obs_loss: 0.00145, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95909\n","update_step:  40 model loss: 3.00055, kl_loss: 3.00000, obs_loss: 0.00055, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95892\n","update_step:  50 model loss: 3.00029, kl_loss: 3.00000, obs_loss: 0.00029, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95859\n","elasped time for update: 54.25s\n","episode [ 196/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00021, kl_loss: 3.00000, obs_loss: 0.00021, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95780\n","update_step:  20 model loss: 3.00020, kl_loss: 3.00000, obs_loss: 0.00020, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95757\n","update_step:  30 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95815\n","update_step:  40 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95806\n","update_step:  50 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95760\n","elasped time for update: 54.23s\n","episode [ 197/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95758\n","update_step:  20 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95768\n","update_step:  30 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95746\n","update_step:  40 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95756\n","update_step:  50 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95783\n","elasped time for update: 54.16s\n","episode [ 198/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95752\n","update_step:  20 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95720\n","update_step:  30 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95743\n","update_step:  40 model loss: 3.00205, kl_loss: 3.00000, obs_loss: 0.00205, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95744\n","update_step:  50 model loss: 3.13817, kl_loss: 3.00000, obs_loss: 0.13817, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95540\n","elasped time for update: 54.18s\n","episode [ 199/1000] is collected. Total reward is 0.897400\n","elasped time for interaction: 0.16s\n","update_step:  10 model loss: 3.08085, kl_loss: 3.00000, obs_loss: 0.08085, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95574\n","update_step:  20 model loss: 3.00971, kl_loss: 3.00000, obs_loss: 0.00971, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95871\n","update_step:  30 model loss: 3.00200, kl_loss: 3.00000, obs_loss: 0.00200, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96254\n","update_step:  40 model loss: 3.00070, kl_loss: 3.00000, obs_loss: 0.00070, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96316\n","update_step:  50 model loss: 3.00047, kl_loss: 3.00000, obs_loss: 0.00047, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95793\n","elasped time for update: 54.27s\n","episode [ 200/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95671\n","update_step:  20 model loss: 3.00016, kl_loss: 3.00000, obs_loss: 0.00016, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95792\n","update_step:  30 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95829\n","update_step:  40 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95816\n","update_step:  50 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95859\n","elasped time for update: 54.17s\n","Total test reward at episode [ 200/1000] is -0.999200\n","elasped time for test: 1.40s\n","episode [ 201/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95863\n","update_step:  20 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95864\n","update_step:  30 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95884\n","update_step:  40 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95875\n","update_step:  50 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95865\n","elasped time for update: 54.22s\n","episode [ 202/1000] is collected. Total reward is 0.775800\n","elasped time for interaction: 0.36s\n","update_step:  10 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95859\n","update_step:  20 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95848\n","update_step:  30 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95839\n","update_step:  40 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95814\n","update_step:  50 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95830\n","elasped time for update: 54.31s\n","episode [ 203/1000] is collected. Total reward is 0.915800\n","elasped time for interaction: 0.13s\n","update_step:  10 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95843\n","update_step:  20 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95819\n","update_step:  30 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95828\n","update_step:  40 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95850\n","update_step:  50 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95845\n","elasped time for update: 54.45s\n","episode [ 204/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00020, kl_loss: 3.00000, obs_loss: 0.00020, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95801\n","update_step:  20 model loss: 3.00363, kl_loss: 3.00000, obs_loss: 0.00363, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95781\n","update_step:  30 model loss: 3.08865, kl_loss: 3.00000, obs_loss: 0.08865, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95257\n","update_step:  40 model loss: 3.02795, kl_loss: 3.00000, obs_loss: 0.02795, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95033\n","update_step:  50 model loss: 3.00970, kl_loss: 3.00000, obs_loss: 0.00970, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95727\n","elasped time for update: 54.37s\n","episode [ 205/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.45s\n","update_step:  10 model loss: 3.00029, kl_loss: 3.00000, obs_loss: 0.00029, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95741\n","update_step:  20 model loss: 3.00087, kl_loss: 3.00000, obs_loss: 0.00087, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95438\n","update_step:  30 model loss: 3.00044, kl_loss: 3.00000, obs_loss: 0.00044, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95332\n","update_step:  40 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95431\n","update_step:  50 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95423\n","elasped time for update: 54.23s\n","episode [ 206/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95377\n","update_step:  20 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95375\n","update_step:  30 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95422\n","update_step:  40 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95421\n","update_step:  50 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95417\n","elasped time for update: 54.20s\n","episode [ 207/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95432\n","update_step:  20 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95409\n","update_step:  30 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95425\n","update_step:  40 model loss: 3.00032, kl_loss: 3.00000, obs_loss: 0.00032, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95458\n","update_step:  50 model loss: 3.00732, kl_loss: 3.00000, obs_loss: 0.00732, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95420\n","elasped time for update: 54.37s\n","episode [ 208/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.11645, kl_loss: 3.00000, obs_loss: 0.11645, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95420\n","update_step:  20 model loss: 3.01956, kl_loss: 3.00000, obs_loss: 0.01956, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95592\n","update_step:  30 model loss: 3.00491, kl_loss: 3.00000, obs_loss: 0.00491, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95510\n","update_step:  40 model loss: 3.00298, kl_loss: 3.00000, obs_loss: 0.00298, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95774\n","update_step:  50 model loss: 3.00150, kl_loss: 3.00000, obs_loss: 0.00150, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95920\n","elasped time for update: 54.33s\n","episode [ 209/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00044, kl_loss: 3.00000, obs_loss: 0.00044, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95755\n","update_step:  20 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95673\n","update_step:  30 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95722\n","update_step:  40 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95798\n","update_step:  50 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95806\n","elasped time for update: 54.22s\n","episode [ 210/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95811\n","update_step:  20 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95864\n","update_step:  30 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95915\n","update_step:  40 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95890\n","update_step:  50 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95869\n","elasped time for update: 54.26s\n","Total test reward at episode [ 210/1000] is -0.999200\n","elasped time for test: 1.44s\n","episode [ 211/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.45s\n","update_step:  10 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95899\n","update_step:  20 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95900\n","update_step:  30 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95909\n","update_step:  40 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95921\n","update_step:  50 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95932\n","elasped time for update: 54.20s\n","episode [ 212/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00664, kl_loss: 3.00000, obs_loss: 0.00664, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95927\n","update_step:  20 model loss: 3.14006, kl_loss: 3.00000, obs_loss: 0.14006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95357\n","update_step:  30 model loss: 3.00816, kl_loss: 3.00000, obs_loss: 0.00816, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95645\n","update_step:  40 model loss: 3.01673, kl_loss: 3.00000, obs_loss: 0.01673, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96146\n","update_step:  50 model loss: 3.00608, kl_loss: 3.00000, obs_loss: 0.00608, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96001\n","elasped time for update: 54.18s\n","episode [ 213/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00173, kl_loss: 3.00000, obs_loss: 0.00173, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95745\n","update_step:  20 model loss: 3.00052, kl_loss: 3.00000, obs_loss: 0.00052, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95642\n","update_step:  30 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95621\n","update_step:  40 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95652\n","update_step:  50 model loss: 3.00016, kl_loss: 3.00000, obs_loss: 0.00016, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95656\n","elasped time for update: 54.28s\n","episode [ 214/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.47s\n","update_step:  10 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95634\n","update_step:  20 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95639\n","update_step:  30 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95667\n","update_step:  40 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95680\n","update_step:  50 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95692\n","elasped time for update: 54.25s\n","episode [ 215/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95683\n","update_step:  20 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95650\n","update_step:  30 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95664\n","update_step:  40 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95665\n","update_step:  50 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95651\n","elasped time for update: 54.15s\n","episode [ 216/1000] is collected. Total reward is 0.367800\n","elasped time for interaction: 0.92s\n","update_step:  10 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95649\n","update_step:  20 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95632\n","update_step:  30 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95656\n","update_step:  40 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95668\n","update_step:  50 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95641\n","elasped time for update: 54.24s\n","episode [ 217/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95648\n","update_step:  20 model loss: 3.00025, kl_loss: 3.00000, obs_loss: 0.00025, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95655\n","update_step:  30 model loss: 3.00723, kl_loss: 3.00000, obs_loss: 0.00723, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95636\n","update_step:  40 model loss: 3.11444, kl_loss: 3.00000, obs_loss: 0.11444, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95445\n","update_step:  50 model loss: 3.01956, kl_loss: 3.00000, obs_loss: 0.01955, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95582\n","elasped time for update: 54.32s\n","episode [ 218/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00930, kl_loss: 3.00000, obs_loss: 0.00930, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95884\n","update_step:  20 model loss: 3.00057, kl_loss: 3.00000, obs_loss: 0.00057, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95858\n","update_step:  30 model loss: 3.00123, kl_loss: 3.00000, obs_loss: 0.00123, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95712\n","update_step:  40 model loss: 3.00038, kl_loss: 3.00000, obs_loss: 0.00038, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95597\n","update_step:  50 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95587\n","elasped time for update: 54.30s\n","episode [ 219/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00016, kl_loss: 3.00000, obs_loss: 0.00016, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95575\n","update_step:  20 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95540\n","update_step:  30 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95551\n","update_step:  40 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95546\n","update_step:  50 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95549\n","elasped time for update: 54.22s\n","episode [ 220/1000] is collected. Total reward is 0.812600\n","elasped time for interaction: 0.27s\n","update_step:  10 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95552\n","update_step:  20 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95513\n","update_step:  30 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95537\n","update_step:  40 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95559\n","update_step:  50 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95572\n","elasped time for update: 54.25s\n","Total test reward at episode [ 220/1000] is -0.999200\n","elasped time for test: 1.34s\n","episode [ 221/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95555\n","update_step:  20 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95558\n","update_step:  30 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95544\n","update_step:  40 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95574\n","update_step:  50 model loss: 3.00032, kl_loss: 3.00000, obs_loss: 0.00032, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95593\n","elasped time for update: 54.23s\n","episode [ 222/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.01319, kl_loss: 3.00000, obs_loss: 0.01319, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95535\n","update_step:  20 model loss: 3.08616, kl_loss: 3.00000, obs_loss: 0.08616, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95243\n","update_step:  30 model loss: 3.00122, kl_loss: 3.00000, obs_loss: 0.00122, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95418\n","update_step:  40 model loss: 3.00764, kl_loss: 3.00000, obs_loss: 0.00764, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95969\n","update_step:  50 model loss: 3.00433, kl_loss: 3.00000, obs_loss: 0.00433, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95852\n","elasped time for update: 54.16s\n","episode [ 223/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00063, kl_loss: 3.00000, obs_loss: 0.00063, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95334\n","update_step:  20 model loss: 3.00029, kl_loss: 3.00000, obs_loss: 0.00029, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95331\n","update_step:  30 model loss: 3.00041, kl_loss: 3.00000, obs_loss: 0.00041, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95494\n","update_step:  40 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95466\n","update_step:  50 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95405\n","elasped time for update: 54.14s\n","episode [ 224/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95420\n","update_step:  20 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95464\n","update_step:  30 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95475\n","update_step:  40 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95446\n","update_step:  50 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95435\n","elasped time for update: 54.24s\n","episode [ 225/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.44s\n","update_step:  10 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95470\n","update_step:  20 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95445\n","update_step:  30 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95447\n","update_step:  40 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95474\n","update_step:  50 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95471\n","elasped time for update: 54.31s\n","episode [ 226/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95472\n","update_step:  20 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95474\n","update_step:  30 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95487\n","update_step:  40 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95496\n","update_step:  50 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95488\n","elasped time for update: 54.14s\n","episode [ 227/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.37s\n","update_step:  10 model loss: 3.00315, kl_loss: 3.00000, obs_loss: 0.00315, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95499\n","update_step:  20 model loss: 3.12581, kl_loss: 3.00000, obs_loss: 0.12581, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95235\n","update_step:  30 model loss: 3.04126, kl_loss: 3.00000, obs_loss: 0.04126, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95495\n","update_step:  40 model loss: 3.00465, kl_loss: 3.00000, obs_loss: 0.00465, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96050\n","update_step:  50 model loss: 3.00071, kl_loss: 3.00000, obs_loss: 0.00071, reward_loss: 0.00000, value_loss: 0.00004 action_loss: 0.97716\n","elasped time for update: 54.20s\n","episode [ 228/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 6.22991, kl_loss: 6.21339, obs_loss: 0.01652, reward_loss: 0.00000, value_loss: 0.00031 action_loss: 0.90028\n","update_step:  20 model loss: 3.44947, kl_loss: 3.00000, obs_loss: 0.44947, reward_loss: 0.00000, value_loss: 0.00154 action_loss: 0.85966\n","update_step:  30 model loss: 3.24025, kl_loss: 3.00028, obs_loss: 0.23997, reward_loss: 0.00000, value_loss: 0.00067 action_loss: 0.93837\n","update_step:  40 model loss: 3.09377, kl_loss: 3.00035, obs_loss: 0.09342, reward_loss: 0.00000, value_loss: 0.00032 action_loss: 0.96085\n","update_step:  50 model loss: 3.03124, kl_loss: 3.00000, obs_loss: 0.03124, reward_loss: 0.00000, value_loss: 0.00021 action_loss: 0.95650\n","elasped time for update: 54.27s\n","episode [ 229/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.01029, kl_loss: 3.00000, obs_loss: 0.01029, reward_loss: 0.00000, value_loss: 0.00015 action_loss: 0.95971\n","update_step:  20 model loss: 3.00525, kl_loss: 3.00000, obs_loss: 0.00525, reward_loss: 0.00000, value_loss: 0.00012 action_loss: 0.96807\n","update_step:  30 model loss: 3.00378, kl_loss: 3.00000, obs_loss: 0.00378, reward_loss: 0.00000, value_loss: 0.00010 action_loss: 0.97265\n","update_step:  40 model loss: 3.00283, kl_loss: 3.00000, obs_loss: 0.00283, reward_loss: 0.00000, value_loss: 0.00008 action_loss: 0.97155\n","update_step:  50 model loss: 3.00229, kl_loss: 3.00000, obs_loss: 0.00229, reward_loss: 0.00000, value_loss: 0.00007 action_loss: 0.96748\n","elasped time for update: 54.24s\n","episode [ 230/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.35s\n","update_step:  10 model loss: 3.00200, kl_loss: 3.00000, obs_loss: 0.00200, reward_loss: 0.00000, value_loss: 0.00006 action_loss: 0.96632\n","update_step:  20 model loss: 3.00181, kl_loss: 3.00000, obs_loss: 0.00181, reward_loss: 0.00000, value_loss: 0.00005 action_loss: 0.96698\n","update_step:  30 model loss: 3.00165, kl_loss: 3.00000, obs_loss: 0.00165, reward_loss: 0.00000, value_loss: 0.00005 action_loss: 0.96832\n","update_step:  40 model loss: 3.00152, kl_loss: 3.00000, obs_loss: 0.00152, reward_loss: 0.00000, value_loss: 0.00004 action_loss: 0.96824\n","update_step:  50 model loss: 3.00144, kl_loss: 3.00000, obs_loss: 0.00144, reward_loss: 0.00000, value_loss: 0.00004 action_loss: 0.96623\n","elasped time for update: 54.23s\n","Total test reward at episode [ 230/1000] is -0.999200\n","elasped time for test: 1.33s\n","episode [ 231/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.34s\n","update_step:  10 model loss: 3.00134, kl_loss: 3.00000, obs_loss: 0.00134, reward_loss: 0.00000, value_loss: 0.00004 action_loss: 0.96443\n","update_step:  20 model loss: 3.00127, kl_loss: 3.00000, obs_loss: 0.00127, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.96476\n","update_step:  30 model loss: 3.00119, kl_loss: 3.00000, obs_loss: 0.00119, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.96583\n","update_step:  40 model loss: 3.00114, kl_loss: 3.00000, obs_loss: 0.00114, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.96618\n","update_step:  50 model loss: 3.00109, kl_loss: 3.00000, obs_loss: 0.00109, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.96566\n","elasped time for update: 54.16s\n","episode [ 232/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.35s\n","update_step:  10 model loss: 3.00103, kl_loss: 3.00000, obs_loss: 0.00103, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.96504\n","update_step:  20 model loss: 3.00100, kl_loss: 3.00000, obs_loss: 0.00100, reward_loss: 0.00000, value_loss: 0.00003 action_loss: 0.96528\n","update_step:  30 model loss: 3.00095, kl_loss: 3.00000, obs_loss: 0.00095, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.96562\n","update_step:  40 model loss: 3.00092, kl_loss: 3.00000, obs_loss: 0.00092, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.96603\n","update_step:  50 model loss: 3.00089, kl_loss: 3.00000, obs_loss: 0.00089, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.96540\n","elasped time for update: 54.08s\n","episode [ 233/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.34s\n","update_step:  10 model loss: 3.00086, kl_loss: 3.00000, obs_loss: 0.00086, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.96437\n","update_step:  20 model loss: 3.00084, kl_loss: 3.00000, obs_loss: 0.00084, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.96452\n","update_step:  30 model loss: 3.00080, kl_loss: 3.00000, obs_loss: 0.00080, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.96445\n","update_step:  40 model loss: 3.00078, kl_loss: 3.00000, obs_loss: 0.00078, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.96435\n","update_step:  50 model loss: 3.00075, kl_loss: 3.00000, obs_loss: 0.00075, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.96395\n","elasped time for update: 54.08s\n","episode [ 234/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.34s\n","update_step:  10 model loss: 3.00074, kl_loss: 3.00000, obs_loss: 0.00074, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.96447\n","update_step:  20 model loss: 3.00071, kl_loss: 3.00000, obs_loss: 0.00071, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.96416\n","update_step:  30 model loss: 3.00069, kl_loss: 3.00000, obs_loss: 0.00069, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.96371\n","update_step:  40 model loss: 3.00068, kl_loss: 3.00000, obs_loss: 0.00068, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.96346\n","update_step:  50 model loss: 3.00066, kl_loss: 3.00000, obs_loss: 0.00066, reward_loss: 0.00000, value_loss: 0.00002 action_loss: 0.96369\n","elasped time for update: 54.20s\n","episode [ 235/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.34s\n","update_step:  10 model loss: 3.00065, kl_loss: 3.00000, obs_loss: 0.00065, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.96308\n","update_step:  20 model loss: 3.00063, kl_loss: 3.00000, obs_loss: 0.00063, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.96261\n","update_step:  30 model loss: 3.00061, kl_loss: 3.00000, obs_loss: 0.00061, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.96239\n","update_step:  40 model loss: 3.00060, kl_loss: 3.00000, obs_loss: 0.00060, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.96238\n","update_step:  50 model loss: 3.00059, kl_loss: 3.00000, obs_loss: 0.00059, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.96296\n","elasped time for update: 54.21s\n","episode [ 236/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.33s\n","update_step:  10 model loss: 3.00058, kl_loss: 3.00000, obs_loss: 0.00057, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.96301\n","update_step:  20 model loss: 3.00057, kl_loss: 3.00000, obs_loss: 0.00057, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.96276\n","update_step:  30 model loss: 3.00056, kl_loss: 3.00000, obs_loss: 0.00056, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.96233\n","update_step:  40 model loss: 3.00055, kl_loss: 3.00000, obs_loss: 0.00055, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.96210\n","update_step:  50 model loss: 3.00053, kl_loss: 3.00000, obs_loss: 0.00053, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.96131\n","elasped time for update: 54.26s\n","episode [ 237/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.33s\n","update_step:  10 model loss: 3.00052, kl_loss: 3.00000, obs_loss: 0.00052, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.96129\n","update_step:  20 model loss: 3.00051, kl_loss: 3.00000, obs_loss: 0.00051, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.96066\n","update_step:  30 model loss: 3.00050, kl_loss: 3.00000, obs_loss: 0.00050, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.96068\n","update_step:  40 model loss: 3.00050, kl_loss: 3.00000, obs_loss: 0.00050, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.96041\n","update_step:  50 model loss: 3.00048, kl_loss: 3.00000, obs_loss: 0.00048, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.96063\n","elasped time for update: 54.24s\n","episode [ 238/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.35s\n","update_step:  10 model loss: 3.00048, kl_loss: 3.00000, obs_loss: 0.00048, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.96036\n","update_step:  20 model loss: 3.00047, kl_loss: 3.00000, obs_loss: 0.00047, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95940\n","update_step:  30 model loss: 3.00045, kl_loss: 3.00000, obs_loss: 0.00045, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95972\n","update_step:  40 model loss: 3.00045, kl_loss: 3.00000, obs_loss: 0.00045, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95958\n","update_step:  50 model loss: 3.00044, kl_loss: 3.00000, obs_loss: 0.00044, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95946\n","elasped time for update: 54.30s\n","episode [ 239/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.33s\n","update_step:  10 model loss: 3.00044, kl_loss: 3.00000, obs_loss: 0.00044, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95940\n","update_step:  20 model loss: 3.00043, kl_loss: 3.00000, obs_loss: 0.00043, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95876\n","update_step:  30 model loss: 3.00042, kl_loss: 3.00000, obs_loss: 0.00042, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95808\n","update_step:  40 model loss: 3.00042, kl_loss: 3.00000, obs_loss: 0.00042, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95865\n","update_step:  50 model loss: 3.00041, kl_loss: 3.00000, obs_loss: 0.00041, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95849\n","elasped time for update: 54.25s\n","episode [ 240/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.37s\n","update_step:  10 model loss: 3.00040, kl_loss: 3.00000, obs_loss: 0.00040, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95827\n","update_step:  20 model loss: 3.00041, kl_loss: 3.00000, obs_loss: 0.00041, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95775\n","update_step:  30 model loss: 3.00039, kl_loss: 3.00000, obs_loss: 0.00039, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95792\n","update_step:  40 model loss: 3.00038, kl_loss: 3.00000, obs_loss: 0.00038, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95732\n","update_step:  50 model loss: 3.00038, kl_loss: 3.00000, obs_loss: 0.00038, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95676\n","elasped time for update: 54.22s\n","Total test reward at episode [ 240/1000] is -0.999200\n","elasped time for test: 1.31s\n","episode [ 241/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.35s\n","update_step:  10 model loss: 3.00038, kl_loss: 3.00000, obs_loss: 0.00038, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95710\n","update_step:  20 model loss: 3.00037, kl_loss: 3.00000, obs_loss: 0.00037, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95673\n","update_step:  30 model loss: 3.00037, kl_loss: 3.00000, obs_loss: 0.00037, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95697\n","update_step:  40 model loss: 3.00036, kl_loss: 3.00000, obs_loss: 0.00036, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95650\n","update_step:  50 model loss: 3.00036, kl_loss: 3.00000, obs_loss: 0.00036, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95628\n","elasped time for update: 54.20s\n","episode [ 242/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.33s\n","update_step:  10 model loss: 3.00035, kl_loss: 3.00000, obs_loss: 0.00035, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95574\n","update_step:  20 model loss: 3.00034, kl_loss: 3.00000, obs_loss: 0.00034, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95608\n","update_step:  30 model loss: 3.00034, kl_loss: 3.00000, obs_loss: 0.00034, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95611\n","update_step:  40 model loss: 3.00034, kl_loss: 3.00000, obs_loss: 0.00034, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95503\n","update_step:  50 model loss: 3.00033, kl_loss: 3.00000, obs_loss: 0.00033, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95513\n","elasped time for update: 54.29s\n","episode [ 243/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00034, kl_loss: 3.00000, obs_loss: 0.00034, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95586\n","update_step:  20 model loss: 3.00033, kl_loss: 3.00000, obs_loss: 0.00033, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95554\n","update_step:  30 model loss: 3.00032, kl_loss: 3.00000, obs_loss: 0.00032, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95495\n","update_step:  40 model loss: 3.00056, kl_loss: 3.00024, obs_loss: 0.00032, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95491\n","update_step:  50 model loss: 3.00031, kl_loss: 3.00000, obs_loss: 0.00031, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95505\n","elasped time for update: 54.30s\n","episode [ 244/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.37s\n","update_step:  10 model loss: 3.00031, kl_loss: 3.00000, obs_loss: 0.00031, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95484\n","update_step:  20 model loss: 3.00031, kl_loss: 3.00000, obs_loss: 0.00031, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95525\n","update_step:  30 model loss: 3.00030, kl_loss: 3.00000, obs_loss: 0.00030, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95571\n","update_step:  40 model loss: 3.00030, kl_loss: 3.00000, obs_loss: 0.00030, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95548\n","update_step:  50 model loss: 3.00029, kl_loss: 3.00000, obs_loss: 0.00029, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95516\n","elasped time for update: 54.29s\n","episode [ 245/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.33s\n","update_step:  10 model loss: 3.00029, kl_loss: 3.00000, obs_loss: 0.00029, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95470\n","update_step:  20 model loss: 3.00029, kl_loss: 3.00000, obs_loss: 0.00029, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95442\n","update_step:  30 model loss: 3.00029, kl_loss: 3.00000, obs_loss: 0.00029, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95434\n","update_step:  40 model loss: 3.00029, kl_loss: 3.00000, obs_loss: 0.00029, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95483\n","update_step:  50 model loss: 3.00030, kl_loss: 3.00000, obs_loss: 0.00030, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95429\n","elasped time for update: 54.31s\n","episode [ 246/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.33s\n","update_step:  10 model loss: 3.00028, kl_loss: 3.00000, obs_loss: 0.00028, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95403\n","update_step:  20 model loss: 3.00028, kl_loss: 3.00000, obs_loss: 0.00028, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95403\n","update_step:  30 model loss: 3.00028, kl_loss: 3.00000, obs_loss: 0.00028, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95354\n","update_step:  40 model loss: 3.00029, kl_loss: 3.00000, obs_loss: 0.00029, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95387\n","update_step:  50 model loss: 3.00030, kl_loss: 3.00000, obs_loss: 0.00030, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95368\n","elasped time for update: 54.27s\n","episode [ 247/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.36s\n","update_step:  10 model loss: 3.00027, kl_loss: 3.00000, obs_loss: 0.00027, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95364\n","update_step:  20 model loss: 3.00027, kl_loss: 3.00000, obs_loss: 0.00027, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95312\n","update_step:  30 model loss: 3.00027, kl_loss: 3.00000, obs_loss: 0.00027, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95321\n","update_step:  40 model loss: 3.00026, kl_loss: 3.00000, obs_loss: 0.00026, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95387\n","update_step:  50 model loss: 3.00025, kl_loss: 3.00000, obs_loss: 0.00025, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95321\n","elasped time for update: 54.22s\n","episode [ 248/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.34s\n","update_step:  10 model loss: 3.00025, kl_loss: 3.00000, obs_loss: 0.00025, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95304\n","update_step:  20 model loss: 3.00025, kl_loss: 3.00000, obs_loss: 0.00025, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95293\n","update_step:  30 model loss: 3.00038, kl_loss: 3.00000, obs_loss: 0.00038, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95261\n","update_step:  40 model loss: 3.00083, kl_loss: 3.00000, obs_loss: 0.00083, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95269\n","update_step:  50 model loss: 3.00924, kl_loss: 3.00000, obs_loss: 0.00924, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95320\n","elasped time for update: 54.16s\n","episode [ 249/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.34s\n","update_step:  10 model loss: 3.00821, kl_loss: 3.00000, obs_loss: 0.00821, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95423\n","update_step:  20 model loss: 3.00362, kl_loss: 3.00000, obs_loss: 0.00362, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95361\n","update_step:  30 model loss: 3.00099, kl_loss: 3.00000, obs_loss: 0.00099, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95283\n","update_step:  40 model loss: 3.00027, kl_loss: 3.00000, obs_loss: 0.00027, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95225\n","update_step:  50 model loss: 3.00025, kl_loss: 3.00000, obs_loss: 0.00025, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95251\n","elasped time for update: 54.21s\n","episode [ 250/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.35s\n","update_step:  10 model loss: 3.00024, kl_loss: 3.00000, obs_loss: 0.00024, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95303\n","update_step:  20 model loss: 3.00023, kl_loss: 3.00000, obs_loss: 0.00023, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95301\n","update_step:  30 model loss: 3.00025, kl_loss: 3.00000, obs_loss: 0.00025, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95302\n","update_step:  40 model loss: 3.00024, kl_loss: 3.00000, obs_loss: 0.00024, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95327\n","update_step:  50 model loss: 3.00034, kl_loss: 3.00000, obs_loss: 0.00034, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95292\n","elasped time for update: 54.28s\n","Total test reward at episode [ 250/1000] is -0.999200\n","elasped time for test: 1.32s\n","episode [ 251/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.34s\n","update_step:  10 model loss: 3.00030, kl_loss: 3.00000, obs_loss: 0.00030, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95287\n","update_step:  20 model loss: 3.00079, kl_loss: 3.00000, obs_loss: 0.00079, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95345\n","update_step:  30 model loss: 3.01072, kl_loss: 3.00000, obs_loss: 0.01072, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95310\n","update_step:  40 model loss: 3.01720, kl_loss: 3.00000, obs_loss: 0.01720, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95273\n","update_step:  50 model loss: 3.00165, kl_loss: 3.00000, obs_loss: 0.00165, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95226\n","elasped time for update: 54.28s\n","episode [ 252/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.33s\n","update_step:  10 model loss: 3.00113, kl_loss: 3.00000, obs_loss: 0.00113, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95241\n","update_step:  20 model loss: 3.00101, kl_loss: 3.00000, obs_loss: 0.00101, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95273\n","update_step:  30 model loss: 3.00032, kl_loss: 3.00000, obs_loss: 0.00032, reward_loss: 0.00000, value_loss: 0.00001 action_loss: 0.95304\n","update_step:  40 model loss: 3.00021, kl_loss: 3.00000, obs_loss: 0.00021, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95315\n","update_step:  50 model loss: 3.00021, kl_loss: 3.00000, obs_loss: 0.00021, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95304\n","elasped time for update: 54.36s\n","episode [ 253/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.34s\n","update_step:  10 model loss: 3.00021, kl_loss: 3.00000, obs_loss: 0.00021, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95224\n","update_step:  20 model loss: 3.00021, kl_loss: 3.00000, obs_loss: 0.00021, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95273\n","update_step:  30 model loss: 3.00020, kl_loss: 3.00000, obs_loss: 0.00020, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95272\n","update_step:  40 model loss: 3.00020, kl_loss: 3.00000, obs_loss: 0.00020, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95294\n","update_step:  50 model loss: 3.00023, kl_loss: 3.00000, obs_loss: 0.00023, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95341\n","elasped time for update: 54.30s\n","episode [ 254/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.35s\n","update_step:  10 model loss: 3.00020, kl_loss: 3.00000, obs_loss: 0.00020, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95273\n","update_step:  20 model loss: 3.00041, kl_loss: 3.00000, obs_loss: 0.00041, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95251\n","update_step:  30 model loss: 3.00481, kl_loss: 3.00000, obs_loss: 0.00481, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95287\n","update_step:  40 model loss: 3.03389, kl_loss: 3.00001, obs_loss: 0.03388, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95357\n","update_step:  50 model loss: 3.00386, kl_loss: 3.00000, obs_loss: 0.00386, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95485\n","elasped time for update: 54.28s\n","episode [ 255/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00032, kl_loss: 3.00000, obs_loss: 0.00032, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95353\n","update_step:  20 model loss: 3.00035, kl_loss: 3.00000, obs_loss: 0.00035, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95210\n","update_step:  30 model loss: 3.00022, kl_loss: 3.00000, obs_loss: 0.00022, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95231\n","update_step:  40 model loss: 3.00019, kl_loss: 3.00000, obs_loss: 0.00019, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95339\n","update_step:  50 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95406\n","elasped time for update: 54.37s\n","episode [ 256/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95398\n","update_step:  20 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95346\n","update_step:  30 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95322\n","update_step:  40 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95340\n","update_step:  50 model loss: 3.00017, kl_loss: 3.00000, obs_loss: 0.00017, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95329\n","elasped time for update: 54.37s\n","episode [ 257/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.36s\n","update_step:  10 model loss: 3.00024, kl_loss: 3.00000, obs_loss: 0.00024, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95297\n","update_step:  20 model loss: 3.00336, kl_loss: 3.00000, obs_loss: 0.00336, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95324\n","update_step:  30 model loss: 3.06984, kl_loss: 3.00000, obs_loss: 0.06984, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95314\n","update_step:  40 model loss: 3.00540, kl_loss: 3.00000, obs_loss: 0.00540, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95405\n","update_step:  50 model loss: 3.00425, kl_loss: 3.00000, obs_loss: 0.00425, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95357\n","elasped time for update: 54.40s\n","episode [ 258/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.35s\n","update_step:  10 model loss: 3.00029, kl_loss: 3.00000, obs_loss: 0.00029, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95331\n","update_step:  20 model loss: 3.00085, kl_loss: 3.00000, obs_loss: 0.00085, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95313\n","update_step:  30 model loss: 3.00022, kl_loss: 3.00000, obs_loss: 0.00022, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95274\n","update_step:  40 model loss: 3.00023, kl_loss: 3.00000, obs_loss: 0.00023, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95270\n","update_step:  50 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95295\n","elasped time for update: 54.37s\n","episode [ 259/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.36s\n","update_step:  10 model loss: 3.00017, kl_loss: 3.00000, obs_loss: 0.00017, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95296\n","update_step:  20 model loss: 3.00016, kl_loss: 3.00000, obs_loss: 0.00016, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95242\n","update_step:  30 model loss: 3.00016, kl_loss: 3.00000, obs_loss: 0.00016, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95300\n","update_step:  40 model loss: 3.00016, kl_loss: 3.00000, obs_loss: 0.00016, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95331\n","update_step:  50 model loss: 3.00016, kl_loss: 3.00000, obs_loss: 0.00016, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95404\n","elasped time for update: 54.27s\n","episode [ 260/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.35s\n","update_step:  10 model loss: 3.00016, kl_loss: 3.00000, obs_loss: 0.00016, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95380\n","update_step:  20 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95313\n","update_step:  30 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95373\n","update_step:  40 model loss: 3.00017, kl_loss: 3.00000, obs_loss: 0.00017, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95417\n","update_step:  50 model loss: 3.00019, kl_loss: 3.00000, obs_loss: 0.00019, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95404\n","elasped time for update: 54.22s\n","Total test reward at episode [ 260/1000] is -0.999200\n","elasped time for test: 1.33s\n","episode [ 261/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.35s\n","update_step:  10 model loss: 3.00019, kl_loss: 3.00000, obs_loss: 0.00019, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95330\n","update_step:  20 model loss: 3.00069, kl_loss: 3.00000, obs_loss: 0.00069, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95317\n","update_step:  30 model loss: 3.02256, kl_loss: 3.00000, obs_loss: 0.02256, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95332\n","update_step:  40 model loss: 3.00097, kl_loss: 3.00000, obs_loss: 0.00097, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95326\n","update_step:  50 model loss: 3.00843, kl_loss: 3.00000, obs_loss: 0.00843, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95307\n","elasped time for update: 54.17s\n","episode [ 262/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.34s\n","update_step:  10 model loss: 3.00139, kl_loss: 3.00000, obs_loss: 0.00139, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95353\n","update_step:  20 model loss: 3.00095, kl_loss: 3.00000, obs_loss: 0.00095, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95471\n","update_step:  30 model loss: 3.00078, kl_loss: 3.00000, obs_loss: 0.00078, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95516\n","update_step:  40 model loss: 3.00017, kl_loss: 3.00000, obs_loss: 0.00017, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95462\n","update_step:  50 model loss: 3.00020, kl_loss: 3.00000, obs_loss: 0.00020, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95395\n","elasped time for update: 54.19s\n","episode [ 263/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95370\n","update_step:  20 model loss: 3.00017, kl_loss: 3.00000, obs_loss: 0.00017, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95315\n","update_step:  30 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95397\n","update_step:  40 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95441\n","update_step:  50 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95413\n","elasped time for update: 54.12s\n","episode [ 264/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.36s\n","update_step:  10 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95418\n","update_step:  20 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95415\n","update_step:  30 model loss: 3.00017, kl_loss: 3.00000, obs_loss: 0.00017, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95360\n","update_step:  40 model loss: 3.00032, kl_loss: 3.00000, obs_loss: 0.00032, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95370\n","update_step:  50 model loss: 3.00417, kl_loss: 3.00000, obs_loss: 0.00417, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95387\n","elasped time for update: 54.24s\n","episode [ 265/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.33s\n","update_step:  10 model loss: 3.03370, kl_loss: 3.00000, obs_loss: 0.03370, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95497\n","update_step:  20 model loss: 3.01036, kl_loss: 3.00000, obs_loss: 0.01036, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95683\n","update_step:  30 model loss: 3.00228, kl_loss: 3.00000, obs_loss: 0.00228, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95650\n","update_step:  40 model loss: 3.00040, kl_loss: 3.00000, obs_loss: 0.00040, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95415\n","update_step:  50 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95339\n","elasped time for update: 54.24s\n","episode [ 266/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95393\n","update_step:  20 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95446\n","update_step:  30 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95485\n","update_step:  40 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95505\n","update_step:  50 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95540\n","elasped time for update: 54.33s\n","episode [ 267/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.36s\n","update_step:  10 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95472\n","update_step:  20 model loss: 3.00017, kl_loss: 3.00000, obs_loss: 0.00017, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95452\n","update_step:  30 model loss: 3.00085, kl_loss: 3.00000, obs_loss: 0.00085, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95571\n","update_step:  40 model loss: 3.02545, kl_loss: 3.00000, obs_loss: 0.02545, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95583\n","update_step:  50 model loss: 3.00467, kl_loss: 3.00000, obs_loss: 0.00467, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95478\n","elasped time for update: 54.28s\n","episode [ 268/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.32s\n","update_step:  10 model loss: 3.00966, kl_loss: 3.00000, obs_loss: 0.00966, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95398\n","update_step:  20 model loss: 3.00026, kl_loss: 3.00000, obs_loss: 0.00026, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95534\n","update_step:  30 model loss: 3.00125, kl_loss: 3.00000, obs_loss: 0.00125, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95673\n","update_step:  40 model loss: 3.00020, kl_loss: 3.00000, obs_loss: 0.00020, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95625\n","update_step:  50 model loss: 3.00035, kl_loss: 3.00000, obs_loss: 0.00035, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95537\n","elasped time for update: 54.33s\n","episode [ 269/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.37s\n","update_step:  10 model loss: 3.00022, kl_loss: 3.00000, obs_loss: 0.00022, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95497\n","update_step:  20 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95552\n","update_step:  30 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95556\n","update_step:  40 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95559\n","update_step:  50 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95534\n","elasped time for update: 54.27s\n","episode [ 270/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.35s\n","update_step:  10 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95552\n","update_step:  20 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95578\n","update_step:  30 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95572\n","update_step:  40 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95583\n","update_step:  50 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95572\n","elasped time for update: 54.22s\n","Total test reward at episode [ 270/1000] is -0.999200\n","elasped time for test: 1.33s\n","episode [ 271/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.34s\n","update_step:  10 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95534\n","update_step:  20 model loss: 3.00094, kl_loss: 3.00000, obs_loss: 0.00094, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95546\n","update_step:  30 model loss: 3.03916, kl_loss: 3.00000, obs_loss: 0.03916, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95542\n","update_step:  40 model loss: 3.02048, kl_loss: 3.00000, obs_loss: 0.02048, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95467\n","update_step:  50 model loss: 3.00105, kl_loss: 3.00000, obs_loss: 0.00105, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95588\n","elasped time for update: 54.26s\n","episode [ 272/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.36s\n","update_step:  10 model loss: 3.00253, kl_loss: 3.00000, obs_loss: 0.00253, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95838\n","update_step:  20 model loss: 3.00123, kl_loss: 3.00000, obs_loss: 0.00123, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95950\n","update_step:  30 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95854\n","update_step:  40 model loss: 3.00019, kl_loss: 3.00000, obs_loss: 0.00019, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95691\n","update_step:  50 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95598\n","elasped time for update: 54.23s\n","episode [ 273/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.35s\n","update_step:  10 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95588\n","update_step:  20 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95730\n","update_step:  30 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95729\n","update_step:  40 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95657\n","update_step:  50 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95564\n","elasped time for update: 54.23s\n","episode [ 274/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.35s\n","update_step:  10 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95593\n","update_step:  20 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95677\n","update_step:  30 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95716\n","update_step:  40 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95656\n","update_step:  50 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95577\n","elasped time for update: 54.19s\n","episode [ 275/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.34s\n","update_step:  10 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95564\n","update_step:  20 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95664\n","update_step:  30 model loss: 3.00046, kl_loss: 3.00000, obs_loss: 0.00046, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95669\n","update_step:  40 model loss: 3.01215, kl_loss: 3.00000, obs_loss: 0.01215, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95585\n","update_step:  50 model loss: 3.00528, kl_loss: 3.00000, obs_loss: 0.00528, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95582\n","elasped time for update: 54.12s\n","episode [ 276/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.36s\n","update_step:  10 model loss: 3.00352, kl_loss: 3.00000, obs_loss: 0.00352, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95644\n","update_step:  20 model loss: 3.00297, kl_loss: 3.00000, obs_loss: 0.00297, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95694\n","update_step:  30 model loss: 3.00039, kl_loss: 3.00000, obs_loss: 0.00039, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95707\n","update_step:  40 model loss: 3.00040, kl_loss: 3.00000, obs_loss: 0.00040, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95714\n","update_step:  50 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95691\n","elasped time for update: 54.26s\n","episode [ 277/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.37s\n","update_step:  10 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95676\n","update_step:  20 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95705\n","update_step:  30 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95701\n","update_step:  40 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95730\n","update_step:  50 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95658\n","elasped time for update: 54.29s\n","episode [ 278/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.35s\n","update_step:  10 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95604\n","update_step:  20 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95705\n","update_step:  30 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95754\n","update_step:  40 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95735\n","update_step:  50 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95697\n","elasped time for update: 54.34s\n","episode [ 279/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95727\n","update_step:  20 model loss: 3.00163, kl_loss: 3.00000, obs_loss: 0.00163, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95762\n","update_step:  30 model loss: 3.05295, kl_loss: 3.00000, obs_loss: 0.05295, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95740\n","update_step:  40 model loss: 3.01938, kl_loss: 3.00000, obs_loss: 0.01938, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95529\n","update_step:  50 model loss: 3.00094, kl_loss: 3.00000, obs_loss: 0.00094, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95327\n","elasped time for update: 54.39s\n","episode [ 280/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.32s\n","update_step:  10 model loss: 3.00118, kl_loss: 3.00000, obs_loss: 0.00118, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95516\n","update_step:  20 model loss: 3.00092, kl_loss: 3.00000, obs_loss: 0.00092, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95760\n","update_step:  30 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95797\n","update_step:  40 model loss: 3.00024, kl_loss: 3.00000, obs_loss: 0.00024, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95721\n","update_step:  50 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95687\n","elasped time for update: 54.25s\n","Total test reward at episode [ 280/1000] is -0.999200\n","elasped time for test: 1.32s\n","episode [ 281/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95644\n","update_step:  20 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95656\n","update_step:  30 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95727\n","update_step:  40 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95744\n","update_step:  50 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95707\n","elasped time for update: 54.31s\n","episode [ 282/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.36s\n","update_step:  10 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95616\n","update_step:  20 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95573\n","update_step:  30 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95557\n","update_step:  40 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95607\n","update_step:  50 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95673\n","elasped time for update: 54.32s\n","episode [ 283/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.36s\n","update_step:  10 model loss: 3.00025, kl_loss: 3.00000, obs_loss: 0.00025, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95641\n","update_step:  20 model loss: 3.00497, kl_loss: 3.00000, obs_loss: 0.00497, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95630\n","update_step:  30 model loss: 3.03228, kl_loss: 3.00000, obs_loss: 0.03228, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95705\n","update_step:  40 model loss: 3.00460, kl_loss: 3.00000, obs_loss: 0.00460, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95781\n","update_step:  50 model loss: 3.00031, kl_loss: 3.00000, obs_loss: 0.00031, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95734\n","elasped time for update: 54.33s\n","episode [ 284/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00134, kl_loss: 3.00000, obs_loss: 0.00134, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95666\n","update_step:  20 model loss: 3.00055, kl_loss: 3.00000, obs_loss: 0.00055, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95671\n","update_step:  30 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95706\n","update_step:  40 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95751\n","update_step:  50 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95714\n","elasped time for update: 54.33s\n","episode [ 285/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.35s\n","update_step:  10 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95766\n","update_step:  20 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95724\n","update_step:  30 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95668\n","update_step:  40 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95610\n","update_step:  50 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95664\n","elasped time for update: 54.28s\n","episode [ 286/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95756\n","update_step:  20 model loss: 3.00077, kl_loss: 3.00000, obs_loss: 0.00077, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95784\n","update_step:  30 model loss: 3.01696, kl_loss: 3.00000, obs_loss: 0.01696, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95668\n","update_step:  40 model loss: 3.00048, kl_loss: 3.00000, obs_loss: 0.00048, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95641\n","update_step:  50 model loss: 3.00383, kl_loss: 3.00000, obs_loss: 0.00383, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95777\n","elasped time for update: 54.37s\n","episode [ 287/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.35s\n","update_step:  10 model loss: 3.00301, kl_loss: 3.00000, obs_loss: 0.00301, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95880\n","update_step:  20 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95843\n","update_step:  30 model loss: 3.00019, kl_loss: 3.00000, obs_loss: 0.00019, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95770\n","update_step:  40 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95787\n","update_step:  50 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95804\n","elasped time for update: 54.40s\n","episode [ 288/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95820\n","update_step:  20 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95764\n","update_step:  30 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95664\n","update_step:  40 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95713\n","update_step:  50 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95783\n","elasped time for update: 54.34s\n","episode [ 289/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.37s\n","update_step:  10 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95703\n","update_step:  20 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95721\n","update_step:  30 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95726\n","update_step:  40 model loss: 3.00194, kl_loss: 3.00000, obs_loss: 0.00194, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95737\n","update_step:  50 model loss: 3.05601, kl_loss: 3.00000, obs_loss: 0.05601, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95777\n","elasped time for update: 54.32s\n","episode [ 290/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.34s\n","update_step:  10 model loss: 3.01303, kl_loss: 3.00000, obs_loss: 0.01303, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95905\n","update_step:  20 model loss: 3.00222, kl_loss: 3.00000, obs_loss: 0.00222, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95882\n","update_step:  30 model loss: 3.00103, kl_loss: 3.00000, obs_loss: 0.00103, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95826\n","update_step:  40 model loss: 3.00082, kl_loss: 3.00000, obs_loss: 0.00082, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95883\n","update_step:  50 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95963\n","elasped time for update: 54.30s\n","Total test reward at episode [ 290/1000] is -0.999200\n","elasped time for test: 1.31s\n","episode [ 291/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.35s\n","update_step:  10 model loss: 3.00019, kl_loss: 3.00000, obs_loss: 0.00019, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95970\n","update_step:  20 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95940\n","update_step:  30 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95900\n","update_step:  40 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95847\n","update_step:  50 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95840\n","elasped time for update: 54.19s\n","episode [ 292/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.32s\n","update_step:  10 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95847\n","update_step:  20 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95853\n","update_step:  30 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95863\n","update_step:  40 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95895\n","update_step:  50 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95950\n","elasped time for update: 54.23s\n","episode [ 293/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.35s\n","update_step:  10 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95883\n","update_step:  20 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95798\n","update_step:  30 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95789\n","update_step:  40 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95858\n","update_step:  50 model loss: 3.00021, kl_loss: 3.00000, obs_loss: 0.00021, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95887\n","elasped time for update: 54.25s\n","episode [ 294/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.34s\n","update_step:  10 model loss: 3.01416, kl_loss: 3.00000, obs_loss: 0.01416, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95851\n","update_step:  20 model loss: 3.00102, kl_loss: 3.00000, obs_loss: 0.00102, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95788\n","update_step:  30 model loss: 3.00875, kl_loss: 3.00000, obs_loss: 0.00875, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95858\n","update_step:  40 model loss: 3.00368, kl_loss: 3.00000, obs_loss: 0.00368, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96048\n","update_step:  50 model loss: 3.00065, kl_loss: 3.00000, obs_loss: 0.00065, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96215\n","elasped time for update: 54.24s\n","episode [ 295/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.34s\n","update_step:  10 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96243\n","update_step:  20 model loss: 3.00027, kl_loss: 3.00000, obs_loss: 0.00027, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96126\n","update_step:  30 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96061\n","update_step:  40 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96129\n","update_step:  50 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96124\n","elasped time for update: 54.22s\n","episode [ 296/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96007\n","update_step:  20 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96025\n","update_step:  30 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96048\n","update_step:  40 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96043\n","update_step:  50 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96079\n","elasped time for update: 54.24s\n","episode [ 297/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.32s\n","update_step:  10 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96043\n","update_step:  20 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96008\n","update_step:  30 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96000\n","update_step:  40 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96026\n","update_step:  50 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96048\n","elasped time for update: 54.27s\n","episode [ 298/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95989\n","update_step:  20 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95982\n","update_step:  30 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95979\n","update_step:  40 model loss: 3.00240, kl_loss: 3.00000, obs_loss: 0.00240, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96068\n","update_step:  50 model loss: 3.05267, kl_loss: 3.00000, obs_loss: 0.05267, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96059\n","elasped time for update: 54.35s\n","episode [ 299/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.34s\n","update_step:  10 model loss: 3.00823, kl_loss: 3.00000, obs_loss: 0.00823, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96070\n","update_step:  20 model loss: 3.00133, kl_loss: 3.00000, obs_loss: 0.00133, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96071\n","update_step:  30 model loss: 3.00107, kl_loss: 3.00000, obs_loss: 0.00107, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96037\n","update_step:  40 model loss: 3.00024, kl_loss: 3.00000, obs_loss: 0.00024, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95988\n","update_step:  50 model loss: 3.00019, kl_loss: 3.00000, obs_loss: 0.00019, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96036\n","elasped time for update: 54.28s\n","episode [ 300/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.34s\n","update_step:  10 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95990\n","update_step:  20 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95939\n","update_step:  30 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95974\n","update_step:  40 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96014\n","update_step:  50 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96023\n","elasped time for update: 54.24s\n","Total test reward at episode [ 300/1000] is -0.999200\n","elasped time for test: 1.37s\n","episode [ 301/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.37s\n","update_step:  10 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95955\n","update_step:  20 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95977\n","update_step:  30 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96007\n","update_step:  40 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96030\n","update_step:  50 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95957\n","elasped time for update: 54.36s\n","episode [ 302/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.34s\n","update_step:  10 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95953\n","update_step:  20 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96012\n","update_step:  30 model loss: 3.00379, kl_loss: 3.00000, obs_loss: 0.00379, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96067\n","update_step:  40 model loss: 3.03774, kl_loss: 3.00000, obs_loss: 0.03774, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96068\n","update_step:  50 model loss: 3.00274, kl_loss: 3.00000, obs_loss: 0.00274, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96086\n","elasped time for update: 54.26s\n","episode [ 303/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.35s\n","update_step:  10 model loss: 3.00467, kl_loss: 3.00000, obs_loss: 0.00467, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96142\n","update_step:  20 model loss: 3.00039, kl_loss: 3.00000, obs_loss: 0.00039, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96124\n","update_step:  30 model loss: 3.00085, kl_loss: 3.00000, obs_loss: 0.00085, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96055\n","update_step:  40 model loss: 3.00020, kl_loss: 3.00000, obs_loss: 0.00020, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96001\n","update_step:  50 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96001\n","elasped time for update: 54.26s\n","episode [ 304/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.33s\n","update_step:  10 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96064\n","update_step:  20 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96078\n","update_step:  30 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96064\n","update_step:  40 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96044\n","update_step:  50 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96078\n","elasped time for update: 54.58s\n","episode [ 305/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.35s\n","update_step:  10 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96038\n","update_step:  20 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95994\n","update_step:  30 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96042\n","update_step:  40 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96071\n","update_step:  50 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96036\n","elasped time for update: 54.64s\n","episode [ 306/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.35s\n","update_step:  10 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95986\n","update_step:  20 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95966\n","update_step:  30 model loss: 3.00109, kl_loss: 3.00000, obs_loss: 0.00109, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96008\n","update_step:  40 model loss: 3.05467, kl_loss: 3.00000, obs_loss: 0.05467, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96051\n","update_step:  50 model loss: 3.01691, kl_loss: 3.00000, obs_loss: 0.01691, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96173\n","elasped time for update: 54.38s\n","episode [ 307/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.34s\n","update_step:  10 model loss: 3.00319, kl_loss: 3.00000, obs_loss: 0.00319, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96287\n","update_step:  20 model loss: 3.00048, kl_loss: 3.00000, obs_loss: 0.00048, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96317\n","update_step:  30 model loss: 3.00091, kl_loss: 3.00000, obs_loss: 0.00091, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96230\n","update_step:  40 model loss: 3.00021, kl_loss: 3.00000, obs_loss: 0.00021, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96183\n","update_step:  50 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96204\n","elasped time for update: 54.29s\n","episode [ 308/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.36s\n","update_step:  10 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96228\n","update_step:  20 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96196\n","update_step:  30 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96209\n","update_step:  40 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96269\n","update_step:  50 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96301\n","elasped time for update: 54.26s\n","episode [ 309/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.36s\n","update_step:  10 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96205\n","update_step:  20 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96132\n","update_step:  30 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96190\n","update_step:  40 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96196\n","update_step:  50 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96209\n","elasped time for update: 54.33s\n","episode [ 310/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.37s\n","update_step:  10 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96167\n","update_step:  20 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96134\n","update_step:  30 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96082\n","update_step:  40 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96085\n","update_step:  50 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96117\n","elasped time for update: 54.18s\n","Total test reward at episode [ 310/1000] is -0.999200\n","elasped time for test: 1.34s\n","episode [ 311/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.36s\n","update_step:  10 model loss: 3.00371, kl_loss: 3.00000, obs_loss: 0.00371, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96115\n","update_step:  20 model loss: 3.04064, kl_loss: 3.00000, obs_loss: 0.04064, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96047\n","update_step:  30 model loss: 3.00625, kl_loss: 3.00000, obs_loss: 0.00625, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96065\n","update_step:  40 model loss: 3.00025, kl_loss: 3.00000, obs_loss: 0.00025, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96059\n","update_step:  50 model loss: 3.00084, kl_loss: 3.00000, obs_loss: 0.00084, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96110\n","elasped time for update: 54.06s\n","episode [ 312/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00095, kl_loss: 3.00000, obs_loss: 0.00095, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96134\n","update_step:  20 model loss: 3.00024, kl_loss: 3.00000, obs_loss: 0.00024, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96078\n","update_step:  30 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96081\n","update_step:  40 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96055\n","update_step:  50 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95982\n","elasped time for update: 54.11s\n","episode [ 313/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95924\n","update_step:  20 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95867\n","update_step:  30 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95845\n","update_step:  40 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95893\n","update_step:  50 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95863\n","elasped time for update: 54.16s\n","episode [ 314/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95871\n","update_step:  20 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95841\n","update_step:  30 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95822\n","update_step:  40 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95905\n","update_step:  50 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95884\n","elasped time for update: 54.21s\n","episode [ 315/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95786\n","update_step:  20 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95856\n","update_step:  30 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95919\n","update_step:  40 model loss: 3.00037, kl_loss: 3.00000, obs_loss: 0.00037, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95910\n","update_step:  50 model loss: 3.01270, kl_loss: 3.00000, obs_loss: 0.01270, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95864\n","elasped time for update: 54.14s\n","episode [ 316/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00046, kl_loss: 3.00000, obs_loss: 0.00046, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95834\n","update_step:  20 model loss: 3.00701, kl_loss: 3.00000, obs_loss: 0.00701, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95875\n","update_step:  30 model loss: 3.00047, kl_loss: 3.00000, obs_loss: 0.00047, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95906\n","update_step:  40 model loss: 3.00124, kl_loss: 3.00000, obs_loss: 0.00124, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95914\n","update_step:  50 model loss: 3.00022, kl_loss: 3.00000, obs_loss: 0.00022, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95901\n","elasped time for update: 54.16s\n","episode [ 317/1000] is collected. Total reward is 0.667000\n","elasped time for interaction: 0.47s\n","update_step:  10 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95843\n","update_step:  20 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95818\n","update_step:  30 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95827\n","update_step:  40 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95878\n","update_step:  50 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95940\n","elasped time for update: 54.32s\n","episode [ 318/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95877\n","update_step:  20 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95800\n","update_step:  30 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95895\n","update_step:  40 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95921\n","update_step:  50 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95839\n","elasped time for update: 54.21s\n","episode [ 319/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95827\n","update_step:  20 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95907\n","update_step:  30 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95955\n","update_step:  40 model loss: 3.00510, kl_loss: 3.00000, obs_loss: 0.00510, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95977\n","update_step:  50 model loss: 3.02486, kl_loss: 3.00000, obs_loss: 0.02486, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95965\n","elasped time for update: 54.23s\n","episode [ 320/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00310, kl_loss: 3.00000, obs_loss: 0.00310, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96021\n","update_step:  20 model loss: 3.00122, kl_loss: 3.00000, obs_loss: 0.00122, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95930\n","update_step:  30 model loss: 3.00111, kl_loss: 3.00000, obs_loss: 0.00111, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95818\n","update_step:  40 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95818\n","update_step:  50 model loss: 3.00020, kl_loss: 3.00000, obs_loss: 0.00020, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95893\n","elasped time for update: 54.22s\n","Total test reward at episode [ 320/1000] is -0.999200\n","elasped time for test: 1.31s\n","episode [ 321/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95911\n","update_step:  20 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95912\n","update_step:  30 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95906\n","update_step:  40 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95959\n","update_step:  50 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95954\n","elasped time for update: 54.13s\n","episode [ 322/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95948\n","update_step:  20 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95951\n","update_step:  30 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95946\n","update_step:  40 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95927\n","update_step:  50 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95843\n","elasped time for update: 54.08s\n","episode [ 323/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95844\n","update_step:  20 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95940\n","update_step:  30 model loss: 3.00078, kl_loss: 3.00000, obs_loss: 0.00078, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96015\n","update_step:  40 model loss: 3.02047, kl_loss: 3.00000, obs_loss: 0.02047, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95986\n","update_step:  50 model loss: 3.00366, kl_loss: 3.00000, obs_loss: 0.00366, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95915\n","elasped time for update: 54.05s\n","episode [ 324/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00423, kl_loss: 3.00000, obs_loss: 0.00423, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95879\n","update_step:  20 model loss: 3.00124, kl_loss: 3.00000, obs_loss: 0.00124, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95895\n","update_step:  30 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95970\n","update_step:  40 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95945\n","update_step:  50 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95856\n","elasped time for update: 54.10s\n","episode [ 325/1000] is collected. Total reward is 0.455800\n","elasped time for interaction: 0.76s\n","update_step:  10 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95881\n","update_step:  20 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95862\n","update_step:  30 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95841\n","update_step:  40 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95864\n","update_step:  50 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95890\n","elasped time for update: 54.09s\n","episode [ 326/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95942\n","update_step:  20 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95919\n","update_step:  30 model loss: 3.00044, kl_loss: 3.00000, obs_loss: 0.00044, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95884\n","update_step:  40 model loss: 3.01380, kl_loss: 3.00000, obs_loss: 0.01380, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95874\n","update_step:  50 model loss: 3.00034, kl_loss: 3.00000, obs_loss: 0.00034, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95887\n","elasped time for update: 54.13s\n","episode [ 327/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00289, kl_loss: 3.00000, obs_loss: 0.00289, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95881\n","update_step:  20 model loss: 3.00218, kl_loss: 3.00000, obs_loss: 0.00218, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95887\n","update_step:  30 model loss: 3.00029, kl_loss: 3.00000, obs_loss: 0.00029, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95894\n","update_step:  40 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95920\n","update_step:  50 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95912\n","elasped time for update: 54.20s\n","episode [ 328/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95896\n","update_step:  20 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95926\n","update_step:  30 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95879\n","update_step:  40 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95859\n","update_step:  50 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95919\n","elasped time for update: 54.18s\n","episode [ 329/1000] is collected. Total reward is 0.267000\n","elasped time for interaction: 1.05s\n","update_step:  10 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95962\n","update_step:  20 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95951\n","update_step:  30 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95923\n","update_step:  40 model loss: 3.00069, kl_loss: 3.00000, obs_loss: 0.00069, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95915\n","update_step:  50 model loss: 3.02370, kl_loss: 3.00000, obs_loss: 0.02370, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95950\n","elasped time for update: 54.24s\n","episode [ 330/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00574, kl_loss: 3.00000, obs_loss: 0.00574, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96081\n","update_step:  20 model loss: 3.00392, kl_loss: 3.00000, obs_loss: 0.00392, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96067\n","update_step:  30 model loss: 3.00023, kl_loss: 3.00000, obs_loss: 0.00023, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95924\n","update_step:  40 model loss: 3.00052, kl_loss: 3.00000, obs_loss: 0.00052, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95867\n","update_step:  50 model loss: 3.00033, kl_loss: 3.00000, obs_loss: 0.00033, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95916\n","elasped time for update: 54.21s\n","Total test reward at episode [ 330/1000] is -0.999200\n","elasped time for test: 1.36s\n","episode [ 331/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95972\n","update_step:  20 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95994\n","update_step:  30 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95936\n","update_step:  40 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95900\n","update_step:  50 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95941\n","elasped time for update: 54.25s\n","episode [ 332/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.37s\n","update_step:  10 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96027\n","update_step:  20 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96056\n","update_step:  30 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95964\n","update_step:  40 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95912\n","update_step:  50 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95932\n","elasped time for update: 54.20s\n","episode [ 333/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00055, kl_loss: 3.00000, obs_loss: 0.00055, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95965\n","update_step:  20 model loss: 3.02679, kl_loss: 3.00000, obs_loss: 0.02679, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95983\n","update_step:  30 model loss: 3.01272, kl_loss: 3.00000, obs_loss: 0.01272, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96015\n","update_step:  40 model loss: 3.00226, kl_loss: 3.00000, obs_loss: 0.00226, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95971\n","update_step:  50 model loss: 3.00177, kl_loss: 3.00000, obs_loss: 0.00177, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95956\n","elasped time for update: 54.08s\n","episode [ 334/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95996\n","update_step:  20 model loss: 3.00033, kl_loss: 3.00000, obs_loss: 0.00033, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96001\n","update_step:  30 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96047\n","update_step:  40 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96072\n","update_step:  50 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96026\n","elasped time for update: 54.19s\n","episode [ 335/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.45s\n","update_step:  10 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96058\n","update_step:  20 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96122\n","update_step:  30 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96054\n","update_step:  40 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95996\n","update_step:  50 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96025\n","elasped time for update: 54.15s\n","episode [ 336/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96063\n","update_step:  20 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96045\n","update_step:  30 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96037\n","update_step:  40 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96070\n","update_step:  50 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96007\n","elasped time for update: 54.22s\n","episode [ 337/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00022, kl_loss: 3.00000, obs_loss: 0.00022, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96000\n","update_step:  20 model loss: 3.01043, kl_loss: 3.00000, obs_loss: 0.01043, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96061\n","update_step:  30 model loss: 3.00099, kl_loss: 3.00000, obs_loss: 0.00099, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96130\n","update_step:  40 model loss: 3.00591, kl_loss: 3.00000, obs_loss: 0.00591, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96123\n","update_step:  50 model loss: 3.00057, kl_loss: 3.00000, obs_loss: 0.00057, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96054\n","elasped time for update: 54.20s\n","episode [ 338/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00109, kl_loss: 3.00000, obs_loss: 0.00109, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95982\n","update_step:  20 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96055\n","update_step:  30 model loss: 3.00017, kl_loss: 3.00000, obs_loss: 0.00017, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96119\n","update_step:  40 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96061\n","update_step:  50 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95987\n","elasped time for update: 54.13s\n","episode [ 339/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95957\n","update_step:  20 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95956\n","update_step:  30 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96042\n","update_step:  40 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96065\n","update_step:  50 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96029\n","elasped time for update: 54.19s\n","episode [ 340/1000] is collected. Total reward is 0.414200\n","elasped time for interaction: 0.85s\n","update_step:  10 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96009\n","update_step:  20 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96044\n","update_step:  30 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96059\n","update_step:  40 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96070\n","update_step:  50 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96078\n","elasped time for update: 54.23s\n","Total test reward at episode [ 340/1000] is -0.999200\n","elasped time for test: 1.36s\n","episode [ 341/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00024, kl_loss: 3.00000, obs_loss: 0.00024, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96060\n","update_step:  20 model loss: 3.01066, kl_loss: 3.00000, obs_loss: 0.01066, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96054\n","update_step:  30 model loss: 3.00071, kl_loss: 3.00000, obs_loss: 0.00071, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96049\n","update_step:  40 model loss: 3.00652, kl_loss: 3.00000, obs_loss: 0.00652, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96106\n","update_step:  50 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96158\n","elasped time for update: 54.14s\n","episode [ 342/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00073, kl_loss: 3.00000, obs_loss: 0.00073, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96209\n","update_step:  20 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96235\n","update_step:  30 model loss: 3.00025, kl_loss: 3.00000, obs_loss: 0.00025, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96200\n","update_step:  40 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96170\n","update_step:  50 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96130\n","elasped time for update: 54.14s\n","episode [ 343/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96093\n","update_step:  20 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96143\n","update_step:  30 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96135\n","update_step:  40 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96118\n","update_step:  50 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96098\n","elasped time for update: 54.09s\n","episode [ 344/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96164\n","update_step:  20 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96230\n","update_step:  30 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96186\n","update_step:  40 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96134\n","update_step:  50 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96154\n","elasped time for update: 54.06s\n","episode [ 345/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00092, kl_loss: 3.00000, obs_loss: 0.00092, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96184\n","update_step:  20 model loss: 3.02916, kl_loss: 3.00000, obs_loss: 0.02916, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96194\n","update_step:  30 model loss: 3.01131, kl_loss: 3.00000, obs_loss: 0.01131, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96140\n","update_step:  40 model loss: 3.00143, kl_loss: 3.00000, obs_loss: 0.00143, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96121\n","update_step:  50 model loss: 3.00069, kl_loss: 3.00000, obs_loss: 0.00069, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96166\n","elasped time for update: 54.16s\n","episode [ 346/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00042, kl_loss: 3.00000, obs_loss: 0.00042, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96132\n","update_step:  20 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96077\n","update_step:  30 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96099\n","update_step:  40 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96137\n","update_step:  50 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96109\n","elasped time for update: 54.21s\n","episode [ 347/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96076\n","update_step:  20 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96073\n","update_step:  30 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96140\n","update_step:  40 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96128\n","update_step:  50 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96158\n","elasped time for update: 54.18s\n","episode [ 348/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96161\n","update_step:  20 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96127\n","update_step:  30 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96125\n","update_step:  40 model loss: 3.00166, kl_loss: 3.00000, obs_loss: 0.00166, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96092\n","update_step:  50 model loss: 3.03407, kl_loss: 3.00000, obs_loss: 0.03407, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96113\n","elasped time for update: 54.22s\n","episode [ 349/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.01053, kl_loss: 3.00000, obs_loss: 0.01053, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96151\n","update_step:  20 model loss: 3.00020, kl_loss: 3.00000, obs_loss: 0.00020, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96109\n","update_step:  30 model loss: 3.00132, kl_loss: 3.00000, obs_loss: 0.00132, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96094\n","update_step:  40 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96114\n","update_step:  50 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96111\n","elasped time for update: 54.25s\n","episode [ 350/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96069\n","update_step:  20 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96028\n","update_step:  30 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96043\n","update_step:  40 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96067\n","update_step:  50 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96029\n","elasped time for update: 54.24s\n","Total test reward at episode [ 350/1000] is -0.999200\n","elasped time for test: 1.44s\n","episode [ 351/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96031\n","update_step:  20 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96061\n","update_step:  30 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96074\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96058\n","update_step:  50 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96014\n","elasped time for update: 54.21s\n","episode [ 352/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96006\n","update_step:  20 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96025\n","update_step:  30 model loss: 3.00527, kl_loss: 3.00000, obs_loss: 0.00527, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96036\n","update_step:  40 model loss: 3.01341, kl_loss: 3.00000, obs_loss: 0.01341, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96040\n","update_step:  50 model loss: 3.00299, kl_loss: 3.00000, obs_loss: 0.00299, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95925\n","elasped time for update: 54.22s\n","episode [ 353/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.46s\n","update_step:  10 model loss: 3.00142, kl_loss: 3.00000, obs_loss: 0.00142, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95903\n","update_step:  20 model loss: 3.00097, kl_loss: 3.00000, obs_loss: 0.00097, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95832\n","update_step:  30 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95827\n","update_step:  40 model loss: 3.00019, kl_loss: 3.00000, obs_loss: 0.00019, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95861\n","update_step:  50 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95886\n","elasped time for update: 54.27s\n","episode [ 354/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95927\n","update_step:  20 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95944\n","update_step:  30 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95987\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95961\n","update_step:  50 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95926\n","elasped time for update: 54.12s\n","episode [ 355/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.45s\n","update_step:  10 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95922\n","update_step:  20 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95970\n","update_step:  30 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96011\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96023\n","update_step:  50 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96011\n","elasped time for update: 54.11s\n","episode [ 356/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.52s\n","update_step:  10 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95964\n","update_step:  20 model loss: 3.00031, kl_loss: 3.00000, obs_loss: 0.00031, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95959\n","update_step:  30 model loss: 3.00818, kl_loss: 3.00000, obs_loss: 0.00818, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95957\n","update_step:  40 model loss: 3.00329, kl_loss: 3.00000, obs_loss: 0.00329, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95983\n","update_step:  50 model loss: 3.00038, kl_loss: 3.00000, obs_loss: 0.00038, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95976\n","elasped time for update: 54.07s\n","episode [ 357/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.49s\n","update_step:  10 model loss: 3.00128, kl_loss: 3.00000, obs_loss: 0.00128, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95915\n","update_step:  20 model loss: 3.00071, kl_loss: 3.00000, obs_loss: 0.00071, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95945\n","update_step:  30 model loss: 3.00022, kl_loss: 3.00000, obs_loss: 0.00022, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95927\n","update_step:  40 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95930\n","update_step:  50 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95935\n","elasped time for update: 54.12s\n","episode [ 358/1000] is collected. Total reward is 0.255000\n","elasped time for interaction: 1.05s\n","update_step:  10 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95934\n","update_step:  20 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95968\n","update_step:  30 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95981\n","update_step:  40 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95982\n","update_step:  50 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95997\n","elasped time for update: 54.15s\n","episode [ 359/1000] is collected. Total reward is 0.211000\n","elasped time for interaction: 1.10s\n","update_step:  10 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95929\n","update_step:  20 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95912\n","update_step:  30 model loss: 3.00171, kl_loss: 3.00000, obs_loss: 0.00171, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95964\n","update_step:  40 model loss: 3.03681, kl_loss: 3.00000, obs_loss: 0.03681, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95996\n","update_step:  50 model loss: 3.00470, kl_loss: 3.00000, obs_loss: 0.00470, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95982\n","elasped time for update: 54.16s\n","episode [ 360/1000] is collected. Total reward is 0.375000\n","elasped time for interaction: 0.90s\n","update_step:  10 model loss: 3.00233, kl_loss: 3.00000, obs_loss: 0.00233, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96008\n","update_step:  20 model loss: 3.00039, kl_loss: 3.00000, obs_loss: 0.00039, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95987\n","update_step:  30 model loss: 3.00040, kl_loss: 3.00000, obs_loss: 0.00040, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95972\n","update_step:  40 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95924\n","update_step:  50 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95943\n","elasped time for update: 54.27s\n","Total test reward at episode [ 360/1000] is -0.999200\n","elasped time for test: 1.41s\n","episode [ 361/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.44s\n","update_step:  10 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95975\n","update_step:  20 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95944\n","update_step:  30 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96005\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96086\n","update_step:  50 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95976\n","elasped time for update: 54.27s\n","episode [ 362/1000] is collected. Total reward is 0.348600\n","elasped time for interaction: 0.90s\n","update_step:  10 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95921\n","update_step:  20 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95991\n","update_step:  30 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96053\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96022\n","update_step:  50 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96010\n","elasped time for update: 54.20s\n","episode [ 363/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96003\n","update_step:  20 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96038\n","update_step:  30 model loss: 3.00104, kl_loss: 3.00000, obs_loss: 0.00104, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96052\n","update_step:  40 model loss: 3.04168, kl_loss: 3.00000, obs_loss: 0.04168, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96020\n","update_step:  50 model loss: 3.01019, kl_loss: 3.00000, obs_loss: 0.01019, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96012\n","elasped time for update: 54.23s\n","episode [ 364/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00362, kl_loss: 3.00000, obs_loss: 0.00362, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96058\n","update_step:  20 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96075\n","update_step:  30 model loss: 3.00054, kl_loss: 3.00000, obs_loss: 0.00054, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96001\n","update_step:  40 model loss: 3.00016, kl_loss: 3.00000, obs_loss: 0.00016, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95991\n","update_step:  50 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96067\n","elasped time for update: 54.21s\n","episode [ 365/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96103\n","update_step:  20 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96053\n","update_step:  30 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96059\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96061\n","update_step:  50 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96050\n","elasped time for update: 54.16s\n","episode [ 366/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.44s\n","update_step:  10 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96109\n","update_step:  20 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96105\n","update_step:  30 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96088\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96109\n","update_step:  50 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96132\n","elasped time for update: 54.15s\n","episode [ 367/1000] is collected. Total reward is 0.611000\n","elasped time for interaction: 0.57s\n","update_step:  10 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96150\n","update_step:  20 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96120\n","update_step:  30 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96059\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96053\n","update_step:  50 model loss: 3.00017, kl_loss: 3.00000, obs_loss: 0.00017, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96102\n","elasped time for update: 54.16s\n","episode [ 368/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00886, kl_loss: 3.00000, obs_loss: 0.00886, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96131\n","update_step:  20 model loss: 3.00079, kl_loss: 3.00000, obs_loss: 0.00079, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96171\n","update_step:  30 model loss: 3.00826, kl_loss: 3.00000, obs_loss: 0.00826, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96173\n","update_step:  40 model loss: 3.00020, kl_loss: 3.00000, obs_loss: 0.00020, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96056\n","update_step:  50 model loss: 3.00088, kl_loss: 3.00000, obs_loss: 0.00088, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96028\n","elasped time for update: 54.11s\n","episode [ 369/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.48s\n","update_step:  10 model loss: 3.00041, kl_loss: 3.00000, obs_loss: 0.00041, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96008\n","update_step:  20 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95995\n","update_step:  30 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96042\n","update_step:  40 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96029\n","update_step:  50 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96040\n","elasped time for update: 54.01s\n","episode [ 370/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96049\n","update_step:  20 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96057\n","update_step:  30 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96084\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96084\n","update_step:  50 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96072\n","elasped time for update: 54.11s\n","Total test reward at episode [ 370/1000] is -0.999200\n","elasped time for test: 1.39s\n","episode [ 371/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96081\n","update_step:  20 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96086\n","update_step:  30 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96081\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96078\n","update_step:  50 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96108\n","elasped time for update: 54.03s\n","episode [ 372/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96118\n","update_step:  20 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96113\n","update_step:  30 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96143\n","update_step:  40 model loss: 3.01529, kl_loss: 3.00000, obs_loss: 0.01529, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96107\n","update_step:  50 model loss: 3.01763, kl_loss: 3.00000, obs_loss: 0.01763, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96071\n","elasped time for update: 54.07s\n","episode [ 373/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00156, kl_loss: 3.00000, obs_loss: 0.00157, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96119\n","update_step:  20 model loss: 3.00070, kl_loss: 3.00000, obs_loss: 0.00070, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96096\n","update_step:  30 model loss: 3.00037, kl_loss: 3.00000, obs_loss: 0.00037, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96061\n","update_step:  40 model loss: 3.00017, kl_loss: 3.00000, obs_loss: 0.00017, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96013\n","update_step:  50 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96041\n","elasped time for update: 54.17s\n","episode [ 374/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96043\n","update_step:  20 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96000\n","update_step:  30 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96038\n","update_step:  40 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96098\n","update_step:  50 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96098\n","elasped time for update: 54.09s\n","episode [ 375/1000] is collected. Total reward is 0.567800\n","elasped time for interaction: 0.61s\n","update_step:  10 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96051\n","update_step:  20 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96065\n","update_step:  30 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96094\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96082\n","update_step:  50 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96096\n","elasped time for update: 54.15s\n","episode [ 376/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96107\n","update_step:  20 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96062\n","update_step:  30 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96077\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96093\n","update_step:  50 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96129\n","elasped time for update: 54.14s\n","episode [ 377/1000] is collected. Total reward is 0.083000\n","elasped time for interaction: 1.30s\n","update_step:  10 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96122\n","update_step:  20 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96090\n","update_step:  30 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96041\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96096\n","update_step:  50 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96178\n","elasped time for update: 54.06s\n","episode [ 378/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96096\n","update_step:  20 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96073\n","update_step:  30 model loss: 3.00334, kl_loss: 3.00000, obs_loss: 0.00334, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96100\n","update_step:  40 model loss: 3.01918, kl_loss: 3.00000, obs_loss: 0.01918, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96128\n","update_step:  50 model loss: 3.00151, kl_loss: 3.00000, obs_loss: 0.00151, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96100\n","elasped time for update: 54.15s\n","episode [ 379/1000] is collected. Total reward is 0.714200\n","elasped time for interaction: 0.41s\n","update_step:  10 model loss: 3.00188, kl_loss: 3.00000, obs_loss: 0.00188, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96074\n","update_step:  20 model loss: 3.00092, kl_loss: 3.00000, obs_loss: 0.00092, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96081\n","update_step:  30 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96111\n","update_step:  40 model loss: 3.00022, kl_loss: 3.00000, obs_loss: 0.00022, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96081\n","update_step:  50 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96114\n","elasped time for update: 54.24s\n","episode [ 380/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96129\n","update_step:  20 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96105\n","update_step:  30 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96076\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96073\n","update_step:  50 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96091\n","elasped time for update: 54.18s\n","Total test reward at episode [ 380/1000] is -0.999200\n","elasped time for test: 1.38s\n","episode [ 381/1000] is collected. Total reward is 0.421400\n","elasped time for interaction: 0.83s\n","update_step:  10 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96107\n","update_step:  20 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96122\n","update_step:  30 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96062\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96099\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96141\n","elasped time for update: 54.15s\n","episode [ 382/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96125\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96128\n","update_step:  30 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96123\n","update_step:  40 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96071\n","update_step:  50 model loss: 3.00277, kl_loss: 3.00000, obs_loss: 0.00277, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96076\n","elasped time for update: 54.11s\n","episode [ 383/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.45s\n","update_step:  10 model loss: 3.02423, kl_loss: 3.00000, obs_loss: 0.02423, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96073\n","update_step:  20 model loss: 3.00619, kl_loss: 3.00000, obs_loss: 0.00619, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95999\n","update_step:  30 model loss: 3.00016, kl_loss: 3.00000, obs_loss: 0.00016, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96038\n","update_step:  40 model loss: 3.00039, kl_loss: 3.00000, obs_loss: 0.00039, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96091\n","update_step:  50 model loss: 3.00051, kl_loss: 3.00000, obs_loss: 0.00051, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96079\n","elasped time for update: 54.02s\n","episode [ 384/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96049\n","update_step:  20 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96093\n","update_step:  30 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96049\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96022\n","update_step:  50 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96064\n","elasped time for update: 54.13s\n","episode [ 385/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96119\n","update_step:  20 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96131\n","update_step:  30 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96094\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96070\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96063\n","elasped time for update: 54.14s\n","episode [ 386/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96046\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96069\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96060\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96056\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96036\n","elasped time for update: 54.10s\n","episode [ 387/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96024\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96012\n","update_step:  30 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96019\n","update_step:  40 model loss: 3.00068, kl_loss: 3.00000, obs_loss: 0.00068, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96018\n","update_step:  50 model loss: 3.02370, kl_loss: 3.00000, obs_loss: 0.02370, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95979\n","elasped time for update: 54.26s\n","episode [ 388/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.01085, kl_loss: 3.00000, obs_loss: 0.01085, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95981\n","update_step:  20 model loss: 3.00083, kl_loss: 3.00000, obs_loss: 0.00083, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96057\n","update_step:  30 model loss: 3.00116, kl_loss: 3.00000, obs_loss: 0.00116, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96052\n","update_step:  40 model loss: 3.00022, kl_loss: 3.00000, obs_loss: 0.00022, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95998\n","update_step:  50 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95967\n","elasped time for update: 54.08s\n","episode [ 389/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95989\n","update_step:  20 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96006\n","update_step:  30 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96023\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95965\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95957\n","elasped time for update: 54.17s\n","episode [ 390/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95976\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95965\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95983\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95985\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95977\n","elasped time for update: 54.20s\n","Total test reward at episode [ 390/1000] is -0.999200\n","elasped time for test: 1.38s\n","episode [ 391/1000] is collected. Total reward is 0.508600\n","elasped time for interaction: 0.71s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96002\n","update_step:  20 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96015\n","update_step:  30 model loss: 3.00041, kl_loss: 3.00000, obs_loss: 0.00041, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95979\n","update_step:  40 model loss: 3.02031, kl_loss: 3.00000, obs_loss: 0.02031, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95992\n","update_step:  50 model loss: 3.01050, kl_loss: 3.00000, obs_loss: 0.01050, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96025\n","elasped time for update: 54.09s\n","episode [ 392/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.37s\n","update_step:  10 model loss: 3.00175, kl_loss: 3.00000, obs_loss: 0.00175, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96035\n","update_step:  20 model loss: 3.00144, kl_loss: 3.00000, obs_loss: 0.00144, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96015\n","update_step:  30 model loss: 3.00020, kl_loss: 3.00000, obs_loss: 0.00020, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95998\n","update_step:  40 model loss: 3.00025, kl_loss: 3.00000, obs_loss: 0.00025, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95990\n","update_step:  50 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95987\n","elasped time for update: 54.02s\n","episode [ 393/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96018\n","update_step:  20 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96029\n","update_step:  30 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96042\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96014\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95954\n","elasped time for update: 54.91s\n","episode [ 394/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95985\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96001\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96032\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96025\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96045\n","elasped time for update: 55.11s\n","episode [ 395/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96060\n","update_step:  20 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96076\n","update_step:  30 model loss: 3.00045, kl_loss: 3.00000, obs_loss: 0.00045, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96059\n","update_step:  40 model loss: 3.01796, kl_loss: 3.00000, obs_loss: 0.01796, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96011\n","update_step:  50 model loss: 3.00605, kl_loss: 3.00000, obs_loss: 0.00605, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96018\n","elasped time for update: 55.07s\n","episode [ 396/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00313, kl_loss: 3.00000, obs_loss: 0.00313, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96015\n","update_step:  20 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96032\n","update_step:  30 model loss: 3.00045, kl_loss: 3.00000, obs_loss: 0.00045, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96011\n","update_step:  40 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96014\n","update_step:  50 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96034\n","elasped time for update: 55.10s\n","episode [ 397/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96065\n","update_step:  20 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96063\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96042\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96052\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96054\n","elasped time for update: 55.26s\n","episode [ 398/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96041\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96028\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96036\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96048\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96063\n","elasped time for update: 55.26s\n","episode [ 399/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96039\n","update_step:  20 model loss: 3.00643, kl_loss: 3.00000, obs_loss: 0.00643, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96040\n","update_step:  30 model loss: 3.00134, kl_loss: 3.00000, obs_loss: 0.00134, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96034\n","update_step:  40 model loss: 3.00514, kl_loss: 3.00000, obs_loss: 0.00514, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96009\n","update_step:  50 model loss: 3.00073, kl_loss: 3.00000, obs_loss: 0.00073, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96012\n","elasped time for update: 55.17s\n","episode [ 400/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00079, kl_loss: 3.00000, obs_loss: 0.00079, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96000\n","update_step:  20 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95937\n","update_step:  30 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95913\n","update_step:  40 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95886\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95861\n","elasped time for update: 55.14s\n","Total test reward at episode [ 400/1000] is -0.999200\n","elasped time for test: 1.43s\n","episode [ 401/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95847\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95849\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95886\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95865\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95884\n","elasped time for update: 55.26s\n","episode [ 402/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95864\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95826\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95838\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95904\n","update_step:  50 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95915\n","elasped time for update: 55.07s\n","episode [ 403/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00020, kl_loss: 3.00000, obs_loss: 0.00020, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95925\n","update_step:  20 model loss: 3.01008, kl_loss: 3.00000, obs_loss: 0.01008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95913\n","update_step:  30 model loss: 3.00114, kl_loss: 3.00000, obs_loss: 0.00114, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95863\n","update_step:  40 model loss: 3.00659, kl_loss: 3.00000, obs_loss: 0.00659, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95827\n","update_step:  50 model loss: 3.00052, kl_loss: 3.00000, obs_loss: 0.00052, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95806\n","elasped time for update: 55.18s\n","episode [ 404/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00047, kl_loss: 3.00000, obs_loss: 0.00047, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95809\n","update_step:  20 model loss: 3.00020, kl_loss: 3.00000, obs_loss: 0.00020, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95790\n","update_step:  30 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95773\n","update_step:  40 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95819\n","update_step:  50 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95886\n","elasped time for update: 55.20s\n","episode [ 405/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.44s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95893\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95832\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95843\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95889\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95901\n","elasped time for update: 55.25s\n","episode [ 406/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.45s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95896\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95901\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95895\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95895\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95884\n","elasped time for update: 55.19s\n","episode [ 407/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95874\n","update_step:  20 model loss: 3.00049, kl_loss: 3.00000, obs_loss: 0.00049, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95869\n","update_step:  30 model loss: 3.01842, kl_loss: 3.00000, obs_loss: 0.01842, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95870\n","update_step:  40 model loss: 3.00690, kl_loss: 3.00000, obs_loss: 0.00690, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95892\n","update_step:  50 model loss: 3.00238, kl_loss: 3.00000, obs_loss: 0.00238, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95911\n","elasped time for update: 55.26s\n","episode [ 408/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00027, kl_loss: 3.00000, obs_loss: 0.00027, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95936\n","update_step:  20 model loss: 3.00051, kl_loss: 3.00000, obs_loss: 0.00051, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95960\n","update_step:  30 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95944\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95941\n","update_step:  50 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95980\n","elasped time for update: 55.20s\n","episode [ 409/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95980\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95978\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95945\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95922\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95917\n","elasped time for update: 55.27s\n","episode [ 410/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.45s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95922\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95881\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95869\n","update_step:  40 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95909\n","update_step:  50 model loss: 3.00163, kl_loss: 3.00000, obs_loss: 0.00163, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95931\n","elasped time for update: 55.22s\n","Total test reward at episode [ 410/1000] is -0.999200\n","elasped time for test: 1.37s\n","episode [ 411/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.02903, kl_loss: 3.00000, obs_loss: 0.02903, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95906\n","update_step:  20 model loss: 3.00152, kl_loss: 3.00000, obs_loss: 0.00152, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95886\n","update_step:  30 model loss: 3.00201, kl_loss: 3.00000, obs_loss: 0.00201, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95869\n","update_step:  40 model loss: 3.00020, kl_loss: 3.00000, obs_loss: 0.00020, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95888\n","update_step:  50 model loss: 3.00028, kl_loss: 3.00000, obs_loss: 0.00028, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95897\n","elasped time for update: 55.38s\n","episode [ 412/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.44s\n","update_step:  10 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95898\n","update_step:  20 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95862\n","update_step:  30 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95861\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95866\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95921\n","elasped time for update: 55.17s\n","episode [ 413/1000] is collected. Total reward is 0.055000\n","elasped time for interaction: 1.32s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95922\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95900\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95910\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95907\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95892\n","elasped time for update: 55.34s\n","episode [ 414/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95892\n","update_step:  20 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95915\n","update_step:  30 model loss: 3.00021, kl_loss: 3.00000, obs_loss: 0.00021, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95907\n","update_step:  40 model loss: 3.00516, kl_loss: 3.00000, obs_loss: 0.00516, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95882\n","update_step:  50 model loss: 3.00405, kl_loss: 3.00000, obs_loss: 0.00405, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95910\n","elasped time for update: 55.17s\n","episode [ 415/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.44s\n","update_step:  10 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95959\n","update_step:  20 model loss: 3.00061, kl_loss: 3.00000, obs_loss: 0.00061, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95981\n","update_step:  30 model loss: 3.00053, kl_loss: 3.00000, obs_loss: 0.00053, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95929\n","update_step:  40 model loss: 3.00022, kl_loss: 3.00000, obs_loss: 0.00022, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95881\n","update_step:  50 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95903\n","elasped time for update: 55.23s\n","episode [ 416/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95936\n","update_step:  20 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95961\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95925\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95902\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95946\n","elasped time for update: 55.13s\n","episode [ 417/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.44s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95947\n","update_step:  20 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95945\n","update_step:  30 model loss: 3.00057, kl_loss: 3.00000, obs_loss: 0.00057, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95960\n","update_step:  40 model loss: 3.01396, kl_loss: 3.00000, obs_loss: 0.01396, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95948\n","update_step:  50 model loss: 3.00315, kl_loss: 3.00000, obs_loss: 0.00315, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95911\n","elasped time for update: 55.22s\n","episode [ 418/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00274, kl_loss: 3.00000, obs_loss: 0.00274, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95874\n","update_step:  20 model loss: 3.00050, kl_loss: 3.00000, obs_loss: 0.00050, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95925\n","update_step:  30 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95994\n","update_step:  40 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96058\n","update_step:  50 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96054\n","elasped time for update: 55.18s\n","episode [ 419/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.44s\n","update_step:  10 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96037\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96011\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96018\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96007\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95989\n","elasped time for update: 55.21s\n","episode [ 420/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.37s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95996\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95985\n","update_step:  30 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95977\n","update_step:  40 model loss: 3.00085, kl_loss: 3.00000, obs_loss: 0.00085, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95979\n","update_step:  50 model loss: 3.02526, kl_loss: 3.00000, obs_loss: 0.02526, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95977\n","elasped time for update: 55.19s\n","Total test reward at episode [ 420/1000] is -0.999200\n","elasped time for test: 1.36s\n","episode [ 421/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00918, kl_loss: 3.00000, obs_loss: 0.00918, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95979\n","update_step:  20 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95939\n","update_step:  30 model loss: 3.00114, kl_loss: 3.00000, obs_loss: 0.00114, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95908\n","update_step:  40 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95940\n","update_step:  50 model loss: 3.00016, kl_loss: 3.00000, obs_loss: 0.00016, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95934\n","elasped time for update: 55.18s\n","episode [ 422/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.50s\n","update_step:  10 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95911\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95871\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95861\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95885\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95898\n","elasped time for update: 55.14s\n","episode [ 423/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95907\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95907\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95899\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95874\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95902\n","elasped time for update: 55.20s\n","episode [ 424/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95926\n","update_step:  20 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95963\n","update_step:  30 model loss: 3.00081, kl_loss: 3.00000, obs_loss: 0.00081, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95946\n","update_step:  40 model loss: 3.02386, kl_loss: 3.00000, obs_loss: 0.02386, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95926\n","update_step:  50 model loss: 3.00955, kl_loss: 3.00000, obs_loss: 0.00955, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95959\n","elasped time for update: 55.16s\n","episode [ 425/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00034, kl_loss: 3.00000, obs_loss: 0.00034, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95980\n","update_step:  20 model loss: 3.00091, kl_loss: 3.00000, obs_loss: 0.00091, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95928\n","update_step:  30 model loss: 3.00022, kl_loss: 3.00000, obs_loss: 0.00022, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95925\n","update_step:  40 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95953\n","update_step:  50 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95948\n","elasped time for update: 55.22s\n","episode [ 426/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95960\n","update_step:  20 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95945\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95910\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95912\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95923\n","elasped time for update: 55.22s\n","episode [ 427/1000] is collected. Total reward is 0.265400\n","elasped time for interaction: 1.05s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95936\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95934\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95942\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95930\n","update_step:  50 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95940\n","elasped time for update: 55.28s\n","episode [ 428/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00163, kl_loss: 3.00000, obs_loss: 0.00163, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95928\n","update_step:  20 model loss: 3.02243, kl_loss: 3.00000, obs_loss: 0.02243, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95912\n","update_step:  30 model loss: 3.00420, kl_loss: 3.00000, obs_loss: 0.00420, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95876\n","update_step:  40 model loss: 3.00052, kl_loss: 3.00000, obs_loss: 0.00052, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95839\n","update_step:  50 model loss: 3.00080, kl_loss: 3.00000, obs_loss: 0.00080, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95822\n","elasped time for update: 55.23s\n","episode [ 429/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95869\n","update_step:  20 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95865\n","update_step:  30 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95898\n","update_step:  40 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95869\n","update_step:  50 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95830\n","elasped time for update: 55.16s\n","episode [ 430/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95870\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95913\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95864\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95835\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95840\n","elasped time for update: 55.15s\n","Total test reward at episode [ 430/1000] is -0.999200\n","elasped time for test: 1.36s\n","episode [ 431/1000] is collected. Total reward is 0.815000\n","elasped time for interaction: 0.26s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95861\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95873\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95849\n","update_step:  40 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95853\n","update_step:  50 model loss: 3.00648, kl_loss: 3.00000, obs_loss: 0.00648, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95906\n","elasped time for update: 55.30s\n","episode [ 432/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00033, kl_loss: 3.00000, obs_loss: 0.00033, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95866\n","update_step:  20 model loss: 3.00493, kl_loss: 3.00000, obs_loss: 0.00493, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95847\n","update_step:  30 model loss: 3.00277, kl_loss: 3.00000, obs_loss: 0.00277, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95788\n","update_step:  40 model loss: 3.00070, kl_loss: 3.00000, obs_loss: 0.00070, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95755\n","update_step:  50 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95668\n","elasped time for update: 55.10s\n","episode [ 433/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95626\n","update_step:  20 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95730\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95750\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95736\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95712\n","elasped time for update: 55.21s\n","episode [ 434/1000] is collected. Total reward is 0.655800\n","elasped time for interaction: 0.53s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95734\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95775\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95800\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95766\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95755\n","elasped time for update: 55.17s\n","episode [ 435/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95756\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95782\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95808\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95806\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95778\n","elasped time for update: 55.21s\n","episode [ 436/1000] is collected. Total reward is 0.388600\n","elasped time for interaction: 0.86s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95791\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95813\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95794\n","update_step:  40 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95777\n","update_step:  50 model loss: 3.00876, kl_loss: 3.00000, obs_loss: 0.00876, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95783\n","elasped time for update: 55.19s\n","episode [ 437/1000] is collected. Total reward is 0.392600\n","elasped time for interaction: 0.88s\n","update_step:  10 model loss: 3.00073, kl_loss: 3.00000, obs_loss: 0.00073, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95736\n","update_step:  20 model loss: 3.00544, kl_loss: 3.00000, obs_loss: 0.00544, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95705\n","update_step:  30 model loss: 3.00030, kl_loss: 3.00000, obs_loss: 0.00030, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95690\n","update_step:  40 model loss: 3.00042, kl_loss: 3.00000, obs_loss: 0.00042, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95669\n","update_step:  50 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95653\n","elasped time for update: 55.28s\n","episode [ 438/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95653\n","update_step:  20 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95688\n","update_step:  30 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95718\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95711\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95716\n","elasped time for update: 55.15s\n","episode [ 439/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95761\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95799\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95779\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95795\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95792\n","elasped time for update: 55.29s\n","episode [ 440/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95791\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95827\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95857\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95857\n","update_step:  50 model loss: 3.00054, kl_loss: 3.00000, obs_loss: 0.00054, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95829\n","elasped time for update: 55.13s\n","Total test reward at episode [ 440/1000] is -0.999200\n","elasped time for test: 1.39s\n","episode [ 441/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.02594, kl_loss: 3.00000, obs_loss: 0.02594, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95832\n","update_step:  20 model loss: 3.01054, kl_loss: 3.00000, obs_loss: 0.01054, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95831\n","update_step:  30 model loss: 3.00130, kl_loss: 3.00000, obs_loss: 0.00130, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95796\n","update_step:  40 model loss: 3.00037, kl_loss: 3.00000, obs_loss: 0.00037, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95781\n","update_step:  50 model loss: 3.00056, kl_loss: 3.00000, obs_loss: 0.00056, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95787\n","elasped time for update: 55.26s\n","episode [ 442/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95788\n","update_step:  20 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95808\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95828\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95777\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95820\n","elasped time for update: 55.14s\n","episode [ 443/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95872\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95857\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95832\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95825\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95832\n","elasped time for update: 55.20s\n","episode [ 444/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95850\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95829\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95858\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95852\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95853\n","elasped time for update: 55.24s\n","episode [ 445/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95866\n","update_step:  20 model loss: 3.00110, kl_loss: 3.00000, obs_loss: 0.00110, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95854\n","update_step:  30 model loss: 3.02557, kl_loss: 3.00000, obs_loss: 0.02557, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95816\n","update_step:  40 model loss: 3.00455, kl_loss: 3.00000, obs_loss: 0.00455, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95885\n","update_step:  50 model loss: 3.00053, kl_loss: 3.00000, obs_loss: 0.00053, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95939\n","elasped time for update: 55.31s\n","episode [ 446/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00097, kl_loss: 3.00000, obs_loss: 0.00097, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95933\n","update_step:  20 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95932\n","update_step:  30 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95952\n","update_step:  40 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95950\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95900\n","elasped time for update: 55.22s\n","episode [ 447/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95921\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95945\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95900\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95890\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95880\n","elasped time for update: 55.20s\n","episode [ 448/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95852\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95855\n","update_step:  30 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95910\n","update_step:  40 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95923\n","update_step:  50 model loss: 3.00234, kl_loss: 3.00000, obs_loss: 0.00234, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95900\n","elasped time for update: 55.08s\n","episode [ 449/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.44s\n","update_step:  10 model loss: 3.01279, kl_loss: 3.00000, obs_loss: 0.01279, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95826\n","update_step:  20 model loss: 3.00352, kl_loss: 3.00000, obs_loss: 0.00352, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95838\n","update_step:  30 model loss: 3.00053, kl_loss: 3.00000, obs_loss: 0.00053, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95892\n","update_step:  40 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95846\n","update_step:  50 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95813\n","elasped time for update: 55.14s\n","episode [ 450/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.45s\n","update_step:  10 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95840\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95845\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95820\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95804\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95884\n","elasped time for update: 55.15s\n","Total test reward at episode [ 450/1000] is -0.999200\n","elasped time for test: 1.33s\n","episode [ 451/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95963\n","update_step:  20 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95985\n","update_step:  30 model loss: 3.00105, kl_loss: 3.00000, obs_loss: 0.00105, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95964\n","update_step:  40 model loss: 3.01644, kl_loss: 3.00000, obs_loss: 0.01644, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95955\n","update_step:  50 model loss: 3.00565, kl_loss: 3.00000, obs_loss: 0.00565, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95878\n","elasped time for update: 55.35s\n","episode [ 452/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.46s\n","update_step:  10 model loss: 3.00111, kl_loss: 3.00000, obs_loss: 0.00111, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95869\n","update_step:  20 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95888\n","update_step:  30 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95896\n","update_step:  40 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95895\n","update_step:  50 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95895\n","elasped time for update: 55.08s\n","episode [ 453/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95884\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95867\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95872\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95888\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95895\n","elasped time for update: 55.25s\n","episode [ 454/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95920\n","update_step:  20 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95904\n","update_step:  30 model loss: 3.00176, kl_loss: 3.00000, obs_loss: 0.00176, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95891\n","update_step:  40 model loss: 3.01599, kl_loss: 3.00000, obs_loss: 0.01599, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95879\n","update_step:  50 model loss: 3.00374, kl_loss: 3.00000, obs_loss: 0.00374, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95918\n","elasped time for update: 55.20s\n","episode [ 455/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95967\n","update_step:  20 model loss: 3.00032, kl_loss: 3.00000, obs_loss: 0.00032, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95933\n","update_step:  30 model loss: 3.00022, kl_loss: 3.00000, obs_loss: 0.00022, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95896\n","update_step:  40 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95867\n","update_step:  50 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95866\n","elasped time for update: 55.11s\n","episode [ 456/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95877\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95870\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95861\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95903\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95899\n","elasped time for update: 55.31s\n","episode [ 457/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95860\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95848\n","update_step:  30 model loss: 3.00016, kl_loss: 3.00000, obs_loss: 0.00016, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95880\n","update_step:  40 model loss: 3.00592, kl_loss: 3.00000, obs_loss: 0.00592, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95860\n","update_step:  50 model loss: 3.00093, kl_loss: 3.00000, obs_loss: 0.00093, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95848\n","elasped time for update: 55.07s\n","episode [ 458/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00167, kl_loss: 3.00000, obs_loss: 0.00167, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95850\n","update_step:  20 model loss: 3.00121, kl_loss: 3.00000, obs_loss: 0.00121, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95835\n","update_step:  30 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95841\n","update_step:  40 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95845\n","update_step:  50 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95813\n","elasped time for update: 55.09s\n","episode [ 459/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95792\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95809\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95836\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95823\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95797\n","elasped time for update: 55.05s\n","episode [ 460/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.46s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95794\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95796\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95807\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95815\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95789\n","elasped time for update: 55.13s\n","Total test reward at episode [ 460/1000] is -0.999200\n","elasped time for test: 1.37s\n","episode [ 461/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95820\n","update_step:  20 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95851\n","update_step:  30 model loss: 3.00548, kl_loss: 3.00000, obs_loss: 0.00548, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95811\n","update_step:  40 model loss: 3.00148, kl_loss: 3.00000, obs_loss: 0.00148, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95764\n","update_step:  50 model loss: 3.00073, kl_loss: 3.00000, obs_loss: 0.00073, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95758\n","elasped time for update: 55.31s\n","episode [ 462/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.44s\n","update_step:  10 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95767\n","update_step:  20 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95752\n","update_step:  30 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95744\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95771\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95789\n","elasped time for update: 55.12s\n","episode [ 463/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.47s\n","update_step:  10 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95813\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95904\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95943\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95918\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95900\n","elasped time for update: 55.18s\n","episode [ 464/1000] is collected. Total reward is 0.088600\n","elasped time for interaction: 1.33s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95926\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95927\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95938\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95901\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95926\n","elasped time for update: 55.19s\n","episode [ 465/1000] is collected. Total reward is 0.768600\n","elasped time for interaction: 0.34s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95933\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95925\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95921\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95893\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95897\n","elasped time for update: 55.18s\n","episode [ 466/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.45s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95902\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95900\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95898\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95936\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95919\n","elasped time for update: 55.20s\n","episode [ 467/1000] is collected. Total reward is 0.118200\n","elasped time for interaction: 1.24s\n","update_step:  10 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95899\n","update_step:  20 model loss: 3.00234, kl_loss: 3.00000, obs_loss: 0.00234, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95915\n","update_step:  30 model loss: 3.01405, kl_loss: 3.00000, obs_loss: 0.01405, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95885\n","update_step:  40 model loss: 3.00036, kl_loss: 3.00000, obs_loss: 0.00036, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95828\n","update_step:  50 model loss: 3.00197, kl_loss: 3.00000, obs_loss: 0.00197, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95801\n","elasped time for update: 55.16s\n","episode [ 468/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95785\n","update_step:  20 model loss: 3.00022, kl_loss: 3.00000, obs_loss: 0.00022, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95766\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95754\n","update_step:  40 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95777\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95788\n","elasped time for update: 55.18s\n","episode [ 469/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.44s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95778\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95783\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95781\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95788\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95815\n","elasped time for update: 55.08s\n","episode [ 470/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95835\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95800\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95779\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95799\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95813\n","elasped time for update: 55.18s\n","Total test reward at episode [ 470/1000] is -0.999200\n","elasped time for test: 1.36s\n","episode [ 471/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95825\n","update_step:  20 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95841\n","update_step:  30 model loss: 3.01525, kl_loss: 3.00000, obs_loss: 0.01525, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95831\n","update_step:  40 model loss: 3.01101, kl_loss: 3.00000, obs_loss: 0.01101, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95811\n","update_step:  50 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95851\n","elasped time for update: 55.15s\n","episode [ 472/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00099, kl_loss: 3.00000, obs_loss: 0.00099, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95830\n","update_step:  20 model loss: 3.00054, kl_loss: 3.00000, obs_loss: 0.00054, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95779\n","update_step:  30 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95798\n","update_step:  40 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95820\n","update_step:  50 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95828\n","elasped time for update: 55.18s\n","episode [ 473/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.45s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95799\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95774\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95805\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95830\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95818\n","elasped time for update: 55.22s\n","episode [ 474/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95796\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95846\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95908\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95941\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95943\n","elasped time for update: 55.24s\n","episode [ 475/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95938\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95928\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95930\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95939\n","update_step:  50 model loss: 3.00025, kl_loss: 3.00000, obs_loss: 0.00025, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95956\n","elasped time for update: 55.15s\n","episode [ 476/1000] is collected. Total reward is 0.769400\n","elasped time for interaction: 0.34s\n","update_step:  10 model loss: 3.01162, kl_loss: 3.00000, obs_loss: 0.01162, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95957\n","update_step:  20 model loss: 3.00371, kl_loss: 3.00000, obs_loss: 0.00371, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95899\n","update_step:  30 model loss: 3.00309, kl_loss: 3.00000, obs_loss: 0.00309, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95873\n","update_step:  40 model loss: 3.00020, kl_loss: 3.00000, obs_loss: 0.00020, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95880\n","update_step:  50 model loss: 3.00052, kl_loss: 3.00000, obs_loss: 0.00052, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95883\n","elasped time for update: 55.09s\n","episode [ 477/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95887\n","update_step:  20 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95884\n","update_step:  30 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95878\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95870\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95854\n","elasped time for update: 55.13s\n","episode [ 478/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95857\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95853\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95853\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95867\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95852\n","elasped time for update: 55.23s\n","episode [ 479/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.44s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95822\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95828\n","update_step:  30 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95854\n","update_step:  40 model loss: 3.00042, kl_loss: 3.00000, obs_loss: 0.00042, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95866\n","update_step:  50 model loss: 3.01589, kl_loss: 3.00000, obs_loss: 0.01589, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95864\n","elasped time for update: 55.18s\n","episode [ 480/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00664, kl_loss: 3.00000, obs_loss: 0.00664, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95867\n","update_step:  20 model loss: 3.00125, kl_loss: 3.00000, obs_loss: 0.00125, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95846\n","update_step:  30 model loss: 3.00044, kl_loss: 3.00000, obs_loss: 0.00044, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95808\n","update_step:  40 model loss: 3.00040, kl_loss: 3.00000, obs_loss: 0.00040, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95789\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95800\n","elasped time for update: 55.21s\n","Total test reward at episode [ 480/1000] is -0.999200\n","elasped time for test: 1.39s\n","episode [ 481/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95794\n","update_step:  20 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95769\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95786\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95799\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95769\n","elasped time for update: 55.20s\n","episode [ 482/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95767\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95797\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95788\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95763\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95764\n","elasped time for update: 55.04s\n","episode [ 483/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95772\n","update_step:  20 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95759\n","update_step:  30 model loss: 3.00376, kl_loss: 3.00000, obs_loss: 0.00376, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95744\n","update_step:  40 model loss: 3.00484, kl_loss: 3.00000, obs_loss: 0.00484, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95742\n","update_step:  50 model loss: 3.00217, kl_loss: 3.00000, obs_loss: 0.00217, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95735\n","elasped time for update: 55.08s\n","episode [ 484/1000] is collected. Total reward is 0.393400\n","elasped time for interaction: 0.89s\n","update_step:  10 model loss: 3.00095, kl_loss: 3.00000, obs_loss: 0.00095, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95787\n","update_step:  20 model loss: 3.00029, kl_loss: 3.00000, obs_loss: 0.00029, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95893\n","update_step:  30 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95905\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95904\n","update_step:  50 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95898\n","elasped time for update: 55.17s\n","episode [ 485/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95884\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95904\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95916\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95893\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95886\n","elasped time for update: 55.18s\n","episode [ 486/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95891\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95876\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95889\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95881\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95841\n","elasped time for update: 55.14s\n","episode [ 487/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95860\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95869\n","update_step:  30 model loss: 3.00108, kl_loss: 3.00000, obs_loss: 0.00108, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95850\n","update_step:  40 model loss: 3.02928, kl_loss: 3.00000, obs_loss: 0.02928, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95809\n","update_step:  50 model loss: 3.00064, kl_loss: 3.00000, obs_loss: 0.00064, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95791\n","elasped time for update: 55.16s\n","episode [ 488/1000] is collected. Total reward is 0.555800\n","elasped time for interaction: 0.63s\n","update_step:  10 model loss: 3.00086, kl_loss: 3.00000, obs_loss: 0.00086, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95804\n","update_step:  20 model loss: 3.00095, kl_loss: 3.00000, obs_loss: 0.00095, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95780\n","update_step:  30 model loss: 3.00039, kl_loss: 3.00000, obs_loss: 0.00039, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95773\n","update_step:  40 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95778\n","update_step:  50 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95795\n","elasped time for update: 55.09s\n","episode [ 489/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.46s\n","update_step:  10 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95855\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95937\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95970\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95933\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95929\n","elasped time for update: 55.21s\n","episode [ 490/1000] is collected. Total reward is 0.433400\n","elasped time for interaction: 0.81s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95941\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95966\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95955\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95923\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95940\n","elasped time for update: 55.09s\n","Total test reward at episode [ 490/1000] is -0.999200\n","elasped time for test: 1.36s\n","episode [ 491/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.49s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95924\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95912\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95935\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95955\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95965\n","elasped time for update: 55.22s\n","episode [ 492/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95927\n","update_step:  20 model loss: 3.00023, kl_loss: 3.00000, obs_loss: 0.00023, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95897\n","update_step:  30 model loss: 3.01289, kl_loss: 3.00000, obs_loss: 0.01289, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95950\n","update_step:  40 model loss: 3.00625, kl_loss: 3.00000, obs_loss: 0.00625, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95885\n","update_step:  50 model loss: 3.00160, kl_loss: 3.00000, obs_loss: 0.00160, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95790\n","elasped time for update: 55.13s\n","episode [ 493/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00085, kl_loss: 3.00000, obs_loss: 0.00085, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95759\n","update_step:  20 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95787\n","update_step:  30 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95798\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95759\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95741\n","elasped time for update: 55.16s\n","episode [ 494/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95742\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95763\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95765\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95742\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95736\n","elasped time for update: 55.10s\n","episode [ 495/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95760\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95749\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95749\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95727\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95741\n","elasped time for update: 55.28s\n","episode [ 496/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95760\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95738\n","update_step:  30 model loss: 3.00058, kl_loss: 3.00000, obs_loss: 0.00058, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95723\n","update_step:  40 model loss: 3.02800, kl_loss: 3.00000, obs_loss: 0.02800, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95737\n","update_step:  50 model loss: 3.00501, kl_loss: 3.00000, obs_loss: 0.00501, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95744\n","elasped time for update: 55.22s\n","episode [ 497/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.48s\n","update_step:  10 model loss: 3.00269, kl_loss: 3.00000, obs_loss: 0.00269, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95733\n","update_step:  20 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95690\n","update_step:  30 model loss: 3.00021, kl_loss: 3.00000, obs_loss: 0.00021, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95671\n","update_step:  40 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95684\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95675\n","elasped time for update: 55.15s\n","episode [ 498/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95673\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95675\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95682\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95658\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95674\n","elasped time for update: 55.28s\n","episode [ 499/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.46s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95669\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95674\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95679\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95693\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95676\n","elasped time for update: 55.15s\n","episode [ 500/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95671\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95689\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95687\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95680\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95696\n","elasped time for update: 55.18s\n","Total test reward at episode [ 500/1000] is -0.999200\n","elasped time for test: 1.42s\n","episode [ 501/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.50s\n","update_step:  10 model loss: 3.00056, kl_loss: 3.00000, obs_loss: 0.00056, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95700\n","update_step:  20 model loss: 3.02658, kl_loss: 3.00000, obs_loss: 0.02658, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95679\n","update_step:  30 model loss: 3.00536, kl_loss: 3.00000, obs_loss: 0.00536, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95649\n","update_step:  40 model loss: 3.00234, kl_loss: 3.00000, obs_loss: 0.00234, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95646\n","update_step:  50 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95678\n","elasped time for update: 55.28s\n","episode [ 502/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00035, kl_loss: 3.00000, obs_loss: 0.00035, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95795\n","update_step:  20 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95857\n","update_step:  30 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95883\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95858\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95871\n","elasped time for update: 55.06s\n","episode [ 503/1000] is collected. Total reward is 0.414200\n","elasped time for interaction: 0.81s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95894\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95864\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95880\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95855\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95878\n","elasped time for update: 55.13s\n","episode [ 504/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95900\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95885\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95872\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95895\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95881\n","elasped time for update: 55.08s\n","episode [ 505/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95859\n","update_step:  20 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95875\n","update_step:  30 model loss: 3.00145, kl_loss: 3.00000, obs_loss: 0.00145, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95883\n","update_step:  40 model loss: 3.01966, kl_loss: 3.00000, obs_loss: 0.01966, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95870\n","update_step:  50 model loss: 3.00031, kl_loss: 3.00000, obs_loss: 0.00031, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95775\n","elasped time for update: 55.04s\n","episode [ 506/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00239, kl_loss: 3.00000, obs_loss: 0.00239, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95707\n","update_step:  20 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95699\n","update_step:  30 model loss: 3.00033, kl_loss: 3.00000, obs_loss: 0.00033, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95682\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95680\n","update_step:  50 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95684\n","elasped time for update: 55.13s\n","episode [ 507/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95667\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95661\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95683\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95679\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95671\n","elasped time for update: 55.14s\n","episode [ 508/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95695\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95705\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95680\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95692\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95720\n","elasped time for update: 55.16s\n","episode [ 509/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95728\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95695\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95688\n","update_step:  40 model loss: 3.00017, kl_loss: 3.00000, obs_loss: 0.00017, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95683\n","update_step:  50 model loss: 3.01400, kl_loss: 3.00000, obs_loss: 0.01400, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95709\n","elasped time for update: 55.08s\n","episode [ 510/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.01199, kl_loss: 3.00000, obs_loss: 0.01199, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95692\n","update_step:  20 model loss: 3.00024, kl_loss: 3.00000, obs_loss: 0.00024, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95742\n","update_step:  30 model loss: 3.00046, kl_loss: 3.00000, obs_loss: 0.00046, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95774\n","update_step:  40 model loss: 3.00055, kl_loss: 3.00000, obs_loss: 0.00055, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95735\n","update_step:  50 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95734\n","elasped time for update: 55.11s\n","Total test reward at episode [ 510/1000] is -0.999200\n","elasped time for test: 1.34s\n","episode [ 511/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95795\n","update_step:  20 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95785\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95741\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95773\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95818\n","elasped time for update: 55.16s\n","episode [ 512/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95794\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95801\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95822\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95817\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95787\n","elasped time for update: 55.11s\n","episode [ 513/1000] is collected. Total reward is 0.862200\n","elasped time for interaction: 0.21s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95803\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95830\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95815\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95802\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95789\n","elasped time for update: 55.16s\n","episode [ 514/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.44s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95790\n","update_step:  20 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95815\n","update_step:  30 model loss: 3.00094, kl_loss: 3.00000, obs_loss: 0.00094, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95829\n","update_step:  40 model loss: 3.01897, kl_loss: 3.00000, obs_loss: 0.01897, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95784\n","update_step:  50 model loss: 3.00411, kl_loss: 3.00000, obs_loss: 0.00411, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95728\n","elasped time for update: 55.19s\n","episode [ 515/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00025, kl_loss: 3.00000, obs_loss: 0.00025, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95744\n","update_step:  20 model loss: 3.00074, kl_loss: 3.00000, obs_loss: 0.00074, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95780\n","update_step:  30 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95748\n","update_step:  40 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95734\n","update_step:  50 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95740\n","elasped time for update: 55.09s\n","episode [ 516/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95737\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95744\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95768\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95778\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95761\n","elasped time for update: 55.10s\n","episode [ 517/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.54s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95745\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95717\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95734\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95754\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95750\n","elasped time for update: 55.21s\n","episode [ 518/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95724\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95752\n","update_step:  30 model loss: 3.00131, kl_loss: 3.00000, obs_loss: 0.00131, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95757\n","update_step:  40 model loss: 3.02285, kl_loss: 3.00000, obs_loss: 0.02285, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95749\n","update_step:  50 model loss: 3.00868, kl_loss: 3.00000, obs_loss: 0.00868, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95715\n","elasped time for update: 55.17s\n","episode [ 519/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00261, kl_loss: 3.00000, obs_loss: 0.00261, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95703\n","update_step:  20 model loss: 3.00091, kl_loss: 3.00000, obs_loss: 0.00091, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95699\n","update_step:  30 model loss: 3.00027, kl_loss: 3.00000, obs_loss: 0.00027, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95685\n","update_step:  40 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95670\n","update_step:  50 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95661\n","elasped time for update: 55.21s\n","episode [ 520/1000] is collected. Total reward is 0.767800\n","elasped time for interaction: 0.34s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95627\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95626\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95740\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95867\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95906\n","elasped time for update: 55.08s\n","Total test reward at episode [ 520/1000] is -0.999200\n","elasped time for test: 1.40s\n","episode [ 521/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95934\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95946\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95899\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95888\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95889\n","elasped time for update: 55.24s\n","episode [ 522/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.46s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95872\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95911\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95922\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95913\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95932\n","elasped time for update: 55.02s\n","episode [ 523/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95937\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95890\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95906\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95930\n","update_step:  50 model loss: 3.00016, kl_loss: 3.00000, obs_loss: 0.00016, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95931\n","elasped time for update: 55.25s\n","episode [ 524/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00633, kl_loss: 3.00000, obs_loss: 0.00633, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95880\n","update_step:  20 model loss: 3.00017, kl_loss: 3.00000, obs_loss: 0.00017, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95837\n","update_step:  30 model loss: 3.00238, kl_loss: 3.00000, obs_loss: 0.00238, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95817\n","update_step:  40 model loss: 3.00062, kl_loss: 3.00000, obs_loss: 0.00062, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95758\n","update_step:  50 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95737\n","elasped time for update: 55.11s\n","episode [ 525/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00017, kl_loss: 3.00000, obs_loss: 0.00017, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95779\n","update_step:  20 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95760\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95751\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95750\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95776\n","elasped time for update: 55.24s\n","episode [ 526/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95762\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95758\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95781\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95786\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95790\n","elasped time for update: 55.23s\n","episode [ 527/1000] is collected. Total reward is 0.235000\n","elasped time for interaction: 1.12s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95759\n","update_step:  20 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95769\n","update_step:  30 model loss: 3.00109, kl_loss: 3.00000, obs_loss: 0.00109, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95762\n","update_step:  40 model loss: 3.01854, kl_loss: 3.00000, obs_loss: 0.01854, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95731\n","update_step:  50 model loss: 3.00245, kl_loss: 3.00000, obs_loss: 0.00245, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95774\n","elasped time for update: 55.06s\n","episode [ 528/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.46s\n","update_step:  10 model loss: 3.00113, kl_loss: 3.00000, obs_loss: 0.00113, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95831\n","update_step:  20 model loss: 3.00041, kl_loss: 3.00000, obs_loss: 0.00041, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95785\n","update_step:  30 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95732\n","update_step:  40 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95739\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95745\n","elasped time for update: 55.11s\n","episode [ 529/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95752\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95732\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95729\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95730\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95737\n","elasped time for update: 55.15s\n","episode [ 530/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95722\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95735\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95748\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95741\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95738\n","elasped time for update: 55.06s\n","Total test reward at episode [ 530/1000] is -0.999200\n","elasped time for test: 1.42s\n","episode [ 531/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.45s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95731\n","update_step:  20 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95736\n","update_step:  30 model loss: 3.00313, kl_loss: 3.00000, obs_loss: 0.00313, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95742\n","update_step:  40 model loss: 3.00381, kl_loss: 3.00000, obs_loss: 0.00381, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95723\n","update_step:  50 model loss: 3.00406, kl_loss: 3.00000, obs_loss: 0.00406, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95744\n","elasped time for update: 55.25s\n","episode [ 532/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95747\n","update_step:  20 model loss: 3.00060, kl_loss: 3.00000, obs_loss: 0.00060, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95702\n","update_step:  30 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95796\n","update_step:  40 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95924\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95940\n","elasped time for update: 54.99s\n","episode [ 533/1000] is collected. Total reward is 0.707800\n","elasped time for interaction: 0.43s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95907\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95884\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95921\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95897\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95860\n","elasped time for update: 55.17s\n","episode [ 534/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95887\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95900\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95890\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95888\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95887\n","elasped time for update: 55.12s\n","episode [ 535/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95896\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95905\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95889\n","update_step:  40 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95902\n","update_step:  50 model loss: 3.01036, kl_loss: 3.00000, obs_loss: 0.01036, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95900\n","elasped time for update: 55.18s\n","episode [ 536/1000] is collected. Total reward is 0.345400\n","elasped time for interaction: 0.93s\n","update_step:  10 model loss: 3.00439, kl_loss: 3.00000, obs_loss: 0.00439, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95812\n","update_step:  20 model loss: 3.00186, kl_loss: 3.00000, obs_loss: 0.00186, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95802\n","update_step:  30 model loss: 3.00070, kl_loss: 3.00000, obs_loss: 0.00070, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95860\n","update_step:  40 model loss: 3.00016, kl_loss: 3.00000, obs_loss: 0.00016, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95827\n","update_step:  50 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95801\n","elasped time for update: 55.17s\n","episode [ 537/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.44s\n","update_step:  10 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95794\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95817\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95806\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95801\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95786\n","elasped time for update: 55.19s\n","episode [ 538/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95758\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95782\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95788\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95794\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95754\n","elasped time for update: 55.14s\n","episode [ 539/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95761\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95799\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95787\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95783\n","update_step:  50 model loss: 3.00032, kl_loss: 3.00000, obs_loss: 0.00032, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95801\n","elasped time for update: 55.14s\n","episode [ 540/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.02016, kl_loss: 3.00000, obs_loss: 0.02016, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95789\n","update_step:  20 model loss: 3.00920, kl_loss: 3.00000, obs_loss: 0.00920, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95773\n","update_step:  30 model loss: 3.00134, kl_loss: 3.00000, obs_loss: 0.00134, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95754\n","update_step:  40 model loss: 3.00017, kl_loss: 3.00000, obs_loss: 0.00017, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95758\n","update_step:  50 model loss: 3.00043, kl_loss: 3.00000, obs_loss: 0.00043, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95749\n","elasped time for update: 55.05s\n","Total test reward at episode [ 540/1000] is -0.999200\n","elasped time for test: 1.33s\n","episode [ 541/1000] is collected. Total reward is 0.779800\n","elasped time for interaction: 0.32s\n","update_step:  10 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95737\n","update_step:  20 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95719\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95717\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95724\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95728\n","elasped time for update: 55.13s\n","episode [ 542/1000] is collected. Total reward is 0.355000\n","elasped time for interaction: 0.93s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95736\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95725\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95702\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95715\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95722\n","elasped time for update: 55.08s\n","episode [ 543/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95709\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95716\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95732\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95717\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95706\n","elasped time for update: 55.16s\n","episode [ 544/1000] is collected. Total reward is 0.249400\n","elasped time for interaction: 1.07s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95715\n","update_step:  20 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95720\n","update_step:  30 model loss: 3.00310, kl_loss: 3.00000, obs_loss: 0.00310, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95714\n","update_step:  40 model loss: 3.00471, kl_loss: 3.00000, obs_loss: 0.00471, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95714\n","update_step:  50 model loss: 3.00026, kl_loss: 3.00001, obs_loss: 0.00025, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95726\n","elasped time for update: 55.04s\n","episode [ 545/1000] is collected. Total reward is 0.451000\n","elasped time for interaction: 0.77s\n","update_step:  10 model loss: 3.00124, kl_loss: 3.00000, obs_loss: 0.00124, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95818\n","update_step:  20 model loss: 3.00019, kl_loss: 3.00000, obs_loss: 0.00019, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95917\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95944\n","update_step:  40 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95949\n","update_step:  50 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95941\n","elasped time for update: 55.11s\n","episode [ 546/1000] is collected. Total reward is 0.718200\n","elasped time for interaction: 0.46s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95937\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95957\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95963\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95942\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95932\n","elasped time for update: 55.06s\n","episode [ 547/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95971\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95969\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95937\n","update_step:  40 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95933\n","update_step:  50 model loss: 3.00438, kl_loss: 3.00000, obs_loss: 0.00438, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95958\n","elasped time for update: 55.07s\n","episode [ 548/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00064, kl_loss: 3.00000, obs_loss: 0.00064, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95913\n","update_step:  20 model loss: 3.00255, kl_loss: 3.00000, obs_loss: 0.00255, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95846\n","update_step:  30 model loss: 3.00052, kl_loss: 3.00000, obs_loss: 0.00052, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95811\n","update_step:  40 model loss: 3.00021, kl_loss: 3.00000, obs_loss: 0.00021, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95806\n","update_step:  50 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95814\n","elasped time for update: 55.04s\n","episode [ 549/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95840\n","update_step:  20 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95864\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95830\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95801\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95798\n","elasped time for update: 55.10s\n","episode [ 550/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95833\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95839\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95833\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95796\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95797\n","elasped time for update: 55.05s\n","Total test reward at episode [ 550/1000] is -0.999200\n","elasped time for test: 1.38s\n","episode [ 551/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95822\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95828\n","update_step:  30 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95806\n","update_step:  40 model loss: 3.00282, kl_loss: 3.00000, obs_loss: 0.00282, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95827\n","update_step:  50 model loss: 3.00488, kl_loss: 3.00000, obs_loss: 0.00488, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95832\n","elasped time for update: 55.11s\n","episode [ 552/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00223, kl_loss: 3.00000, obs_loss: 0.00223, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95817\n","update_step:  20 model loss: 3.00057, kl_loss: 3.00000, obs_loss: 0.00057, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95791\n","update_step:  30 model loss: 3.00059, kl_loss: 3.00000, obs_loss: 0.00059, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95774\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95782\n","update_step:  50 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95776\n","elasped time for update: 54.99s\n","episode [ 553/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95758\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95758\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95793\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95765\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95737\n","elasped time for update: 55.12s\n","episode [ 554/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95744\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95767\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95782\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95777\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95773\n","elasped time for update: 55.18s\n","episode [ 555/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95758\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95761\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95763\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95760\n","update_step:  50 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95758\n","elasped time for update: 55.16s\n","episode [ 556/1000] is collected. Total reward is 0.879000\n","elasped time for interaction: 0.18s\n","update_step:  10 model loss: 3.00753, kl_loss: 3.00000, obs_loss: 0.00753, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95748\n","update_step:  20 model loss: 3.00470, kl_loss: 3.00000, obs_loss: 0.00470, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95780\n","update_step:  30 model loss: 3.00072, kl_loss: 3.00000, obs_loss: 0.00072, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95789\n","update_step:  40 model loss: 3.00137, kl_loss: 3.00000, obs_loss: 0.00137, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95819\n","update_step:  50 model loss: 3.00072, kl_loss: 3.00000, obs_loss: 0.00072, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95876\n","elasped time for update: 55.27s\n","episode [ 557/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95911\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95878\n","update_step:  30 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95842\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95837\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95863\n","elasped time for update: 55.03s\n","episode [ 558/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.45s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95877\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95856\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95844\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95855\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95839\n","elasped time for update: 55.11s\n","episode [ 559/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95844\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95870\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95865\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95838\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95858\n","elasped time for update: 55.15s\n","episode [ 560/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.45s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95866\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95843\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95841\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95830\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95851\n","elasped time for update: 55.04s\n","Total test reward at episode [ 560/1000] is -0.999200\n","elasped time for test: 1.37s\n","episode [ 561/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00065, kl_loss: 3.00000, obs_loss: 0.00065, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95852\n","update_step:  20 model loss: 3.02379, kl_loss: 3.00000, obs_loss: 0.02379, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95795\n","update_step:  30 model loss: 3.00152, kl_loss: 3.00000, obs_loss: 0.00152, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95742\n","update_step:  40 model loss: 3.00265, kl_loss: 3.00000, obs_loss: 0.00265, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95805\n","update_step:  50 model loss: 3.00037, kl_loss: 3.00000, obs_loss: 0.00037, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95803\n","elasped time for update: 55.16s\n","episode [ 562/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.48s\n","update_step:  10 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95779\n","update_step:  20 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95766\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95756\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95755\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95753\n","elasped time for update: 55.15s\n","episode [ 563/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95763\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95767\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95752\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95744\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95743\n","elasped time for update: 55.16s\n","episode [ 564/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95763\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95777\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95757\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95744\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95757\n","elasped time for update: 55.11s\n","episode [ 565/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95759\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95770\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95768\n","update_step:  40 model loss: 3.00064, kl_loss: 3.00000, obs_loss: 0.00064, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95759\n","update_step:  50 model loss: 3.02136, kl_loss: 3.00000, obs_loss: 0.02136, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95761\n","elasped time for update: 55.07s\n","episode [ 566/1000] is collected. Total reward is 0.011800\n","elasped time for interaction: 1.49s\n","update_step:  10 model loss: 3.00395, kl_loss: 3.00000, obs_loss: 0.00395, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95752\n","update_step:  20 model loss: 3.00163, kl_loss: 3.00000, obs_loss: 0.00163, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95734\n","update_step:  30 model loss: 3.00016, kl_loss: 3.00000, obs_loss: 0.00016, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95706\n","update_step:  40 model loss: 3.00036, kl_loss: 3.00000, obs_loss: 0.00036, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95697\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95708\n","elasped time for update: 55.03s\n","episode [ 567/1000] is collected. Total reward is 0.097400\n","elasped time for interaction: 1.28s\n","update_step:  10 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95696\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95691\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95707\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95693\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95679\n","elasped time for update: 55.19s\n","episode [ 568/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95701\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95697\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95713\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95696\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95683\n","elasped time for update: 55.05s\n","episode [ 569/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.44s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95691\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95709\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95707\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95713\n","update_step:  50 model loss: 3.00013, kl_loss: 3.00000, obs_loss: 0.00013, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95712\n","elasped time for update: 55.14s\n","episode [ 570/1000] is collected. Total reward is 0.495000\n","elasped time for interaction: 0.71s\n","update_step:  10 model loss: 3.00794, kl_loss: 3.00000, obs_loss: 0.00794, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95710\n","update_step:  20 model loss: 3.00316, kl_loss: 3.00000, obs_loss: 0.00316, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95675\n","update_step:  30 model loss: 3.00226, kl_loss: 3.00000, obs_loss: 0.00226, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95681\n","update_step:  40 model loss: 3.00094, kl_loss: 3.00000, obs_loss: 0.00094, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95712\n","update_step:  50 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95831\n","elasped time for update: 55.09s\n","Total test reward at episode [ 570/1000] is -0.999200\n","elasped time for test: 1.37s\n","episode [ 571/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00020, kl_loss: 3.00000, obs_loss: 0.00020, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95953\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95961\n","update_step:  30 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95930\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95942\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95953\n","elasped time for update: 55.25s\n","episode [ 572/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.37s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95967\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95954\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95946\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95960\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95952\n","elasped time for update: 54.99s\n","episode [ 573/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.44s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95949\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95954\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95944\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95908\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95919\n","elasped time for update: 55.15s\n","episode [ 574/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.46s\n","update_step:  10 model loss: 3.00033, kl_loss: 3.00000, obs_loss: 0.00034, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95938\n","update_step:  20 model loss: 3.01190, kl_loss: 3.00000, obs_loss: 0.01190, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95945\n","update_step:  30 model loss: 3.00463, kl_loss: 3.00000, obs_loss: 0.00463, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95861\n","update_step:  40 model loss: 3.00146, kl_loss: 3.00000, obs_loss: 0.00146, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95772\n","update_step:  50 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95754\n","elasped time for update: 55.04s\n","episode [ 575/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00024, kl_loss: 3.00000, obs_loss: 0.00024, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95763\n","update_step:  20 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95752\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95766\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95773\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95765\n","elasped time for update: 55.16s\n","episode [ 576/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95774\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95773\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95770\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95756\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95760\n","elasped time for update: 55.03s\n","episode [ 577/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95759\n","update_step:  20 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95745\n","update_step:  30 model loss: 3.00047, kl_loss: 3.00000, obs_loss: 0.00047, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95741\n","update_step:  40 model loss: 3.01250, kl_loss: 3.00000, obs_loss: 0.01250, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95778\n","update_step:  50 model loss: 3.00486, kl_loss: 3.00000, obs_loss: 0.00486, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95771\n","elasped time for update: 55.13s\n","episode [ 578/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00144, kl_loss: 3.00000, obs_loss: 0.00144, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95768\n","update_step:  20 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95789\n","update_step:  30 model loss: 3.00023, kl_loss: 3.00000, obs_loss: 0.00023, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95785\n","update_step:  40 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95755\n","update_step:  50 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95730\n","elasped time for update: 54.97s\n","episode [ 579/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.44s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95737\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95769\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95769\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95750\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95752\n","elasped time for update: 55.17s\n","episode [ 580/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95745\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95737\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95725\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95749\n","update_step:  50 model loss: 3.00029, kl_loss: 3.00000, obs_loss: 0.00029, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95758\n","elasped time for update: 54.96s\n","Total test reward at episode [ 580/1000] is -0.999200\n","elasped time for test: 1.42s\n","episode [ 581/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.44s\n","update_step:  10 model loss: 3.01345, kl_loss: 3.00000, obs_loss: 0.01345, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95756\n","update_step:  20 model loss: 3.00701, kl_loss: 3.00000, obs_loss: 0.00701, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95753\n","update_step:  30 model loss: 3.00056, kl_loss: 3.00000, obs_loss: 0.00056, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95728\n","update_step:  40 model loss: 3.00084, kl_loss: 3.00000, obs_loss: 0.00084, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95711\n","update_step:  50 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95710\n","elasped time for update: 55.29s\n","episode [ 582/1000] is collected. Total reward is 0.170200\n","elasped time for interaction: 1.19s\n","update_step:  10 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95711\n","update_step:  20 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95700\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95710\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95684\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95677\n","elasped time for update: 54.95s\n","episode [ 583/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95691\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95694\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95696\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95689\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95694\n","elasped time for update: 55.08s\n","episode [ 584/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95703\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95689\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95676\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95681\n","update_step:  50 model loss: 3.00082, kl_loss: 3.00000, obs_loss: 0.00082, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95683\n","elasped time for update: 55.02s\n","episode [ 585/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.01749, kl_loss: 3.00000, obs_loss: 0.01749, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95688\n","update_step:  20 model loss: 3.00346, kl_loss: 3.00000, obs_loss: 0.00346, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95682\n","update_step:  30 model loss: 3.00040, kl_loss: 3.00000, obs_loss: 0.00040, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95694\n","update_step:  40 model loss: 3.00052, kl_loss: 3.00000, obs_loss: 0.00052, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95690\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95773\n","elasped time for update: 55.16s\n","episode [ 586/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95920\n","update_step:  20 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95934\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95905\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95896\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95914\n","elasped time for update: 55.11s\n","episode [ 587/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.45s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95903\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95868\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95901\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95916\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95891\n","elasped time for update: 55.05s\n","episode [ 588/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95896\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95927\n","update_step:  30 model loss: 3.00023, kl_loss: 3.00000, obs_loss: 0.00023, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95919\n","update_step:  40 model loss: 3.01099, kl_loss: 3.00000, obs_loss: 0.01099, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95872\n","update_step:  50 model loss: 3.00506, kl_loss: 3.00000, obs_loss: 0.00506, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95828\n","elasped time for update: 55.05s\n","episode [ 589/1000] is collected. Total reward is 0.831800\n","elasped time for interaction: 0.24s\n","update_step:  10 model loss: 3.00139, kl_loss: 3.00000, obs_loss: 0.00139, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95833\n","update_step:  20 model loss: 3.00035, kl_loss: 3.00000, obs_loss: 0.00035, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95814\n","update_step:  30 model loss: 3.00026, kl_loss: 3.00000, obs_loss: 0.00026, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95801\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95781\n","update_step:  50 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95781\n","elasped time for update: 54.99s\n","episode [ 590/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95775\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95796\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95771\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95797\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95800\n","elasped time for update: 55.02s\n","Total test reward at episode [ 590/1000] is -0.999200\n","elasped time for test: 1.36s\n","episode [ 591/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95796\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95789\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95792\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95783\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95784\n","elasped time for update: 55.03s\n","episode [ 592/1000] is collected. Total reward is 0.331800\n","elasped time for interaction: 0.95s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95789\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95785\n","update_step:  30 model loss: 3.00031, kl_loss: 3.00000, obs_loss: 0.00031, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95780\n","update_step:  40 model loss: 3.02117, kl_loss: 3.00000, obs_loss: 0.02117, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95800\n","update_step:  50 model loss: 3.00656, kl_loss: 3.00000, obs_loss: 0.00656, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95793\n","elasped time for update: 55.03s\n","episode [ 593/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.48s\n","update_step:  10 model loss: 3.00280, kl_loss: 3.00000, obs_loss: 0.00280, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95770\n","update_step:  20 model loss: 3.00023, kl_loss: 3.00000, obs_loss: 0.00023, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95745\n","update_step:  30 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95756\n","update_step:  40 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95755\n","update_step:  50 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95750\n","elasped time for update: 55.03s\n","episode [ 594/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95737\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95750\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95759\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95738\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95737\n","elasped time for update: 55.05s\n","episode [ 595/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95733\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95737\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95750\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95742\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95727\n","elasped time for update: 55.01s\n","episode [ 596/1000] is collected. Total reward is 0.225400\n","elasped time for interaction: 1.08s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95750\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95766\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95734\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95728\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95730\n","elasped time for update: 55.10s\n","episode [ 597/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.37s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95754\n","update_step:  20 model loss: 3.00015, kl_loss: 3.00000, obs_loss: 0.00015, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95749\n","update_step:  30 model loss: 3.00917, kl_loss: 3.00000, obs_loss: 0.00917, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95731\n","update_step:  40 model loss: 3.00420, kl_loss: 3.00000, obs_loss: 0.00420, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95704\n","update_step:  50 model loss: 3.00157, kl_loss: 3.00000, obs_loss: 0.00157, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95719\n","elasped time for update: 54.87s\n","episode [ 598/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00067, kl_loss: 3.00000, obs_loss: 0.00067, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95725\n","update_step:  20 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95708\n","update_step:  30 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95697\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95723\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95843\n","elasped time for update: 55.07s\n","episode [ 599/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.46s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95965\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95988\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95965\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95995\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95992\n","elasped time for update: 55.12s\n","episode [ 600/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.45s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95982\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95979\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95992\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95987\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95992\n","elasped time for update: 55.05s\n","Total test reward at episode [ 600/1000] is -0.999200\n","elasped time for test: 1.37s\n","episode [ 601/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96017\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96002\n","update_step:  30 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95993\n","update_step:  40 model loss: 3.00616, kl_loss: 3.00000, obs_loss: 0.00616, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95996\n","update_step:  50 model loss: 3.00502, kl_loss: 3.00000, obs_loss: 0.00502, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95936\n","elasped time for update: 55.24s\n","episode [ 602/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00029, kl_loss: 3.00000, obs_loss: 0.00029, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95834\n","update_step:  20 model loss: 3.00039, kl_loss: 3.00000, obs_loss: 0.00039, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95858\n","update_step:  30 model loss: 3.00028, kl_loss: 3.00000, obs_loss: 0.00028, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95896\n","update_step:  40 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95881\n","update_step:  50 model loss: 3.00008, kl_loss: 3.00000, obs_loss: 0.00008, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95860\n","elasped time for update: 54.94s\n","episode [ 603/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95861\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95881\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95870\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95861\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95845\n","elasped time for update: 55.11s\n","episode [ 604/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95840\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95852\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95877\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95865\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95860\n","elasped time for update: 54.99s\n","episode [ 605/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95850\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95852\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95852\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95836\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95849\n","elasped time for update: 55.14s\n","episode [ 606/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.50s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95867\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95878\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95865\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95850\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95851\n","elasped time for update: 55.00s\n","episode [ 607/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95859\n","update_step:  20 model loss: 3.00214, kl_loss: 3.00000, obs_loss: 0.00214, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95856\n","update_step:  30 model loss: 3.00473, kl_loss: 3.00000, obs_loss: 0.00473, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95841\n","update_step:  40 model loss: 3.00565, kl_loss: 3.00000, obs_loss: 0.00565, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95827\n","update_step:  50 model loss: 3.00112, kl_loss: 3.00000, obs_loss: 0.00112, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95826\n","elasped time for update: 54.95s\n","episode [ 608/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95809\n","update_step:  20 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95797\n","update_step:  30 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95789\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95791\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95812\n","elasped time for update: 54.97s\n","episode [ 609/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95801\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95804\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95801\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95773\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95786\n","elasped time for update: 55.06s\n","episode [ 610/1000] is collected. Total reward is 0.187800\n","elasped time for interaction: 1.15s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95781\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95772\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95808\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95816\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95798\n","elasped time for update: 54.94s\n","Total test reward at episode [ 610/1000] is -0.999200\n","elasped time for test: 1.40s\n","episode [ 611/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.45s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95791\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95797\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95802\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95797\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95812\n","elasped time for update: 55.13s\n","episode [ 612/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95801\n","update_step:  20 model loss: 3.00100, kl_loss: 3.00000, obs_loss: 0.00100, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95781\n","update_step:  30 model loss: 3.01581, kl_loss: 3.00000, obs_loss: 0.01581, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95777\n","update_step:  40 model loss: 3.00070, kl_loss: 3.00000, obs_loss: 0.00070, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95773\n","update_step:  50 model loss: 3.00169, kl_loss: 3.00000, obs_loss: 0.00169, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95770\n","elasped time for update: 54.90s\n","episode [ 613/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95752\n","update_step:  20 model loss: 3.00022, kl_loss: 3.00000, obs_loss: 0.00022, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95736\n","update_step:  30 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95751\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95754\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95742\n","elasped time for update: 54.99s\n","episode [ 614/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.45s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95796\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95920\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96012\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96033\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96016\n","elasped time for update: 54.95s\n","episode [ 615/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96009\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96018\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96027\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96015\n","update_step:  50 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96029\n","elasped time for update: 55.07s\n","episode [ 616/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00145, kl_loss: 3.00000, obs_loss: 0.00145, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96046\n","update_step:  20 model loss: 3.01010, kl_loss: 3.00000, obs_loss: 0.01010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96004\n","update_step:  30 model loss: 3.00058, kl_loss: 3.00000, obs_loss: 0.00058, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95920\n","update_step:  40 model loss: 3.00079, kl_loss: 3.00000, obs_loss: 0.00079, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95874\n","update_step:  50 model loss: 3.00042, kl_loss: 3.00000, obs_loss: 0.00042, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95882\n","elasped time for update: 55.04s\n","episode [ 617/1000] is collected. Total reward is 0.911000\n","elasped time for interaction: 0.13s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95904\n","update_step:  20 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95905\n","update_step:  30 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95875\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95854\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95901\n","elasped time for update: 55.00s\n","episode [ 618/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95909\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95896\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95867\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95867\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95883\n","elasped time for update: 55.03s\n","episode [ 619/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95883\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95869\n","update_step:  30 model loss: 3.00017, kl_loss: 3.00000, obs_loss: 0.00017, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95848\n","update_step:  40 model loss: 3.00726, kl_loss: 3.00000, obs_loss: 0.00726, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95884\n","update_step:  50 model loss: 3.00152, kl_loss: 3.00000, obs_loss: 0.00152, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95883\n","elasped time for update: 55.07s\n","episode [ 620/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.45s\n","update_step:  10 model loss: 3.00256, kl_loss: 3.00000, obs_loss: 0.00256, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95905\n","update_step:  20 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95919\n","update_step:  30 model loss: 3.00024, kl_loss: 3.00000, obs_loss: 0.00024, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95876\n","update_step:  40 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95853\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95856\n","elasped time for update: 54.98s\n","Total test reward at episode [ 620/1000] is -0.999200\n","elasped time for test: 1.38s\n","episode [ 621/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.46s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95856\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95846\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95834\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95836\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95872\n","elasped time for update: 55.02s\n","episode [ 622/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95860\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95843\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95856\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95868\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95846\n","elasped time for update: 54.86s\n","episode [ 623/1000] is collected. Total reward is 0.346200\n","elasped time for interaction: 0.94s\n","update_step:  10 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95850\n","update_step:  20 model loss: 3.00240, kl_loss: 3.00000, obs_loss: 0.00240, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95874\n","update_step:  30 model loss: 3.00457, kl_loss: 3.00000, obs_loss: 0.00457, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95845\n","update_step:  40 model loss: 3.00156, kl_loss: 3.00000, obs_loss: 0.00156, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95805\n","update_step:  50 model loss: 3.00073, kl_loss: 3.00000, obs_loss: 0.00073, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95807\n","elasped time for update: 55.00s\n","episode [ 624/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.46s\n","update_step:  10 model loss: 3.00028, kl_loss: 3.00000, obs_loss: 0.00028, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95806\n","update_step:  20 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95789\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95798\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95785\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95767\n","elasped time for update: 54.98s\n","episode [ 625/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95794\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95782\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95775\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95780\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95781\n","elasped time for update: 55.00s\n","episode [ 626/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.44s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95775\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95807\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95805\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95801\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95760\n","elasped time for update: 55.03s\n","episode [ 627/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95773\n","update_step:  20 model loss: 3.00033, kl_loss: 3.00000, obs_loss: 0.00033, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95803\n","update_step:  30 model loss: 3.01733, kl_loss: 3.00000, obs_loss: 0.01733, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95781\n","update_step:  40 model loss: 3.00646, kl_loss: 3.00000, obs_loss: 0.00646, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95749\n","update_step:  50 model loss: 3.00050, kl_loss: 3.00000, obs_loss: 0.00050, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95753\n","elasped time for update: 55.02s\n","episode [ 628/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00042, kl_loss: 3.00000, obs_loss: 0.00042, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95759\n","update_step:  20 model loss: 3.00025, kl_loss: 3.00000, obs_loss: 0.00025, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95759\n","update_step:  30 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95744\n","update_step:  40 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95716\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95736\n","elasped time for update: 54.96s\n","episode [ 629/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95757\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95739\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95731\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95741\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95730\n","elasped time for update: 54.96s\n","episode [ 630/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95746\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95733\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95742\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95743\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95732\n","elasped time for update: 54.89s\n","Total test reward at episode [ 630/1000] is -0.999200\n","elasped time for test: 1.33s\n","episode [ 631/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95737\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95743\n","update_step:  30 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95738\n","update_step:  40 model loss: 3.00423, kl_loss: 3.00000, obs_loss: 0.00423, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95746\n","update_step:  50 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95746\n","elasped time for update: 55.10s\n","episode [ 632/1000] is collected. Total reward is 0.213400\n","elasped time for interaction: 1.12s\n","update_step:  10 model loss: 3.00349, kl_loss: 3.00002, obs_loss: 0.00346, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95739\n","update_step:  20 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95845\n","update_step:  30 model loss: 3.00049, kl_loss: 3.00000, obs_loss: 0.00049, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95912\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95926\n","update_step:  50 model loss: 3.00005, kl_loss: 3.00000, obs_loss: 0.00005, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95930\n","elasped time for update: 54.84s\n","episode [ 633/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95972\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95969\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95962\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95965\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95977\n","elasped time for update: 54.98s\n","episode [ 634/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95986\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95991\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95999\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96022\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95995\n","elasped time for update: 54.80s\n","episode [ 635/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95967\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95995\n","update_step:  30 model loss: 3.00051, kl_loss: 3.00000, obs_loss: 0.00051, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96041\n","update_step:  40 model loss: 3.01550, kl_loss: 3.00000, obs_loss: 0.01550, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96026\n","update_step:  50 model loss: 3.00429, kl_loss: 3.00000, obs_loss: 0.00429, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95943\n","elasped time for update: 54.89s\n","episode [ 636/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00037, kl_loss: 3.00000, obs_loss: 0.00037, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95989\n","update_step:  20 model loss: 3.00062, kl_loss: 3.00000, obs_loss: 0.00062, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96029\n","update_step:  30 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96014\n","update_step:  40 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96003\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96019\n","elasped time for update: 54.89s\n","episode [ 637/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96014\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95980\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95986\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95996\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96010\n","elasped time for update: 54.92s\n","episode [ 638/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96027\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96007\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95985\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95979\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95991\n","elasped time for update: 55.02s\n","episode [ 639/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96000\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96000\n","update_step:  30 model loss: 3.00061, kl_loss: 3.00000, obs_loss: 0.00061, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96012\n","update_step:  40 model loss: 3.01875, kl_loss: 3.00000, obs_loss: 0.01875, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95998\n","update_step:  50 model loss: 3.00239, kl_loss: 3.00000, obs_loss: 0.00239, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95980\n","elasped time for update: 54.92s\n","episode [ 640/1000] is collected. Total reward is 0.323000\n","elasped time for interaction: 0.96s\n","update_step:  10 model loss: 3.00144, kl_loss: 3.00000, obs_loss: 0.00144, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95965\n","update_step:  20 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95956\n","update_step:  30 model loss: 3.00025, kl_loss: 3.00000, obs_loss: 0.00025, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95939\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95943\n","update_step:  50 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95942\n","elasped time for update: 54.81s\n","Total test reward at episode [ 640/1000] is -0.999200\n","elasped time for test: 1.35s\n","episode [ 641/1000] is collected. Total reward is 0.041400\n","elasped time for interaction: 1.44s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95959\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95942\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95925\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95948\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95965\n","elasped time for update: 54.92s\n","episode [ 642/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95945\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95909\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95944\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95972\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95951\n","elasped time for update: 54.76s\n","episode [ 643/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95941\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95953\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95943\n","update_step:  40 model loss: 3.00023, kl_loss: 3.00000, obs_loss: 0.00023, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95934\n","update_step:  50 model loss: 3.01383, kl_loss: 3.00000, obs_loss: 0.01383, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95919\n","elasped time for update: 54.87s\n","episode [ 644/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00752, kl_loss: 3.00000, obs_loss: 0.00752, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95926\n","update_step:  20 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95937\n","update_step:  30 model loss: 3.00081, kl_loss: 3.00000, obs_loss: 0.00081, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95931\n","update_step:  40 model loss: 3.00030, kl_loss: 3.00000, obs_loss: 0.00030, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95911\n","update_step:  50 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95901\n","elasped time for update: 54.92s\n","episode [ 645/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95873\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95884\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95891\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95874\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95877\n","elasped time for update: 54.82s\n","episode [ 646/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95864\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95856\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95885\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95890\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95899\n","elasped time for update: 54.82s\n","episode [ 647/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.45s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95888\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95860\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95859\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95887\n","update_step:  50 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95872\n","elasped time for update: 54.92s\n","episode [ 648/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.45s\n","update_step:  10 model loss: 3.00115, kl_loss: 3.00000, obs_loss: 0.00115, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95862\n","update_step:  20 model loss: 3.01192, kl_loss: 3.00000, obs_loss: 0.01192, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95861\n","update_step:  30 model loss: 3.00101, kl_loss: 3.00000, obs_loss: 0.00101, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95873\n","update_step:  40 model loss: 3.00075, kl_loss: 3.00000, obs_loss: 0.00075, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95865\n","update_step:  50 model loss: 3.00043, kl_loss: 3.00000, obs_loss: 0.00043, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95866\n","elasped time for update: 54.82s\n","episode [ 649/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.44s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95846\n","update_step:  20 model loss: 3.00007, kl_loss: 3.00000, obs_loss: 0.00007, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95832\n","update_step:  30 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95846\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95852\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95844\n","elasped time for update: 54.85s\n","episode [ 650/1000] is collected. Total reward is 0.071800\n","elasped time for interaction: 1.27s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95816\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95821\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95845\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95842\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95839\n","elasped time for update: 54.81s\n","Total test reward at episode [ 650/1000] is -0.999200\n","elasped time for test: 1.41s\n","episode [ 651/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95814\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95810\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95833\n","update_step:  40 model loss: 3.00045, kl_loss: 3.00000, obs_loss: 0.00045, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95808\n","update_step:  50 model loss: 3.01523, kl_loss: 3.00000, obs_loss: 0.01523, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95816\n","elasped time for update: 54.95s\n","episode [ 652/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00516, kl_loss: 3.00000, obs_loss: 0.00516, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95841\n","update_step:  20 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95837\n","update_step:  30 model loss: 3.00069, kl_loss: 3.00000, obs_loss: 0.00069, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95817\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95807\n","update_step:  50 model loss: 3.00010, kl_loss: 3.00000, obs_loss: 0.00010, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95818\n","elasped time for update: 54.81s\n","episode [ 653/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95797\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95793\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95816\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95819\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95785\n","elasped time for update: 54.89s\n","episode [ 654/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.42s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95773\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95803\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95814\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95807\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95794\n","elasped time for update: 54.82s\n","episode [ 655/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.37s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95793\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95789\n","update_step:  30 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95801\n","update_step:  40 model loss: 3.00310, kl_loss: 3.00000, obs_loss: 0.00310, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95814\n","update_step:  50 model loss: 3.00117, kl_loss: 3.00000, obs_loss: 0.00117, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95794\n","elasped time for update: 54.91s\n","episode [ 656/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00222, kl_loss: 3.00000, obs_loss: 0.00222, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95806\n","update_step:  20 model loss: 3.00044, kl_loss: 3.00000, obs_loss: 0.00044, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95804\n","update_step:  30 model loss: 3.00030, kl_loss: 3.00000, obs_loss: 0.00030, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95785\n","update_step:  40 model loss: 3.00009, kl_loss: 3.00000, obs_loss: 0.00009, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95764\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95764\n","elasped time for update: 54.78s\n","episode [ 657/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95777\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95786\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95768\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95775\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95777\n","elasped time for update: 54.89s\n","episode [ 658/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.46s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95768\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95752\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95775\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95796\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95780\n","elasped time for update: 54.86s\n","episode [ 659/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.46s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95766\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00000, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95780\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95792\n","update_step:  40 model loss: 3.00031, kl_loss: 3.00000, obs_loss: 0.00031, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95784\n","update_step:  50 model loss: 3.02154, kl_loss: 3.00000, obs_loss: 0.02154, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95755\n","elasped time for update: 54.84s\n","episode [ 660/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00365, kl_loss: 3.00000, obs_loss: 0.00365, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95757\n","update_step:  20 model loss: 3.00271, kl_loss: 3.00000, obs_loss: 0.00271, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95797\n","update_step:  30 model loss: 3.00055, kl_loss: 3.00000, obs_loss: 0.00055, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95764\n","update_step:  40 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95758\n","update_step:  50 model loss: 3.00014, kl_loss: 3.00000, obs_loss: 0.00014, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96002\n","elasped time for update: 54.88s\n","Total test reward at episode [ 660/1000] is -0.999200\n","elasped time for test: 1.38s\n","episode [ 661/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96238\n","update_step:  20 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96256\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96239\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96281\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96304\n","elasped time for update: 55.06s\n","episode [ 662/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.37s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96270\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96246\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96233\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96240\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96237\n","elasped time for update: 54.74s\n","episode [ 663/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96225\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96235\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96242\n","update_step:  40 model loss: 3.00042, kl_loss: 3.00000, obs_loss: 0.00042, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96249\n","update_step:  50 model loss: 3.00742, kl_loss: 3.00000, obs_loss: 0.00742, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96238\n","elasped time for update: 54.89s\n","episode [ 664/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00244, kl_loss: 3.00000, obs_loss: 0.00244, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96133\n","update_step:  20 model loss: 3.00087, kl_loss: 3.00000, obs_loss: 0.00087, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95959\n","update_step:  30 model loss: 3.00018, kl_loss: 3.00000, obs_loss: 0.00018, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95921\n","update_step:  40 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95957\n","update_step:  50 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95960\n","elasped time for update: 54.78s\n","episode [ 665/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.41s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95940\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95947\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95930\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95963\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95969\n","elasped time for update: 54.95s\n","episode [ 666/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95951\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95960\n","update_step:  30 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95965\n","update_step:  40 model loss: 3.00140, kl_loss: 3.00000, obs_loss: 0.00140, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95942\n","update_step:  50 model loss: 3.01019, kl_loss: 3.00000, obs_loss: 0.01019, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95917\n","elasped time for update: 54.84s\n","episode [ 667/1000] is collected. Total reward is 0.868600\n","elasped time for interaction: 0.19s\n","update_step:  10 model loss: 3.00061, kl_loss: 3.00000, obs_loss: 0.00061, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95961\n","update_step:  20 model loss: 3.00106, kl_loss: 3.00000, obs_loss: 0.00106, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96008\n","update_step:  30 model loss: 3.00036, kl_loss: 3.00000, obs_loss: 0.00036, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.96001\n","update_step:  40 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95964\n","update_step:  50 model loss: 3.00006, kl_loss: 3.00000, obs_loss: 0.00006, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95949\n","elasped time for update: 54.87s\n","episode [ 668/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95959\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95943\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95944\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95948\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95962\n","elasped time for update: 54.85s\n","episode [ 669/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95937\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95927\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95946\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95970\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95933\n","elasped time for update: 54.89s\n","episode [ 670/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95911\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95950\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95942\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95915\n","update_step:  50 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95927\n","elasped time for update: 54.85s\n","Total test reward at episode [ 670/1000] is -0.999200\n","elasped time for test: 1.39s\n","episode [ 671/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n","update_step:  10 model loss: 3.00765, kl_loss: 3.00000, obs_loss: 0.00765, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95940\n","update_step:  20 model loss: 3.00351, kl_loss: 3.00000, obs_loss: 0.00351, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95926\n","update_step:  30 model loss: 3.00151, kl_loss: 3.00000, obs_loss: 0.00151, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95893\n","update_step:  40 model loss: 3.00072, kl_loss: 3.00000, obs_loss: 0.00072, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95852\n","update_step:  50 model loss: 3.00004, kl_loss: 3.00000, obs_loss: 0.00004, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95854\n","elasped time for update: 54.94s\n","episode [ 672/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00012, kl_loss: 3.00000, obs_loss: 0.00012, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95853\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95849\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95864\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95870\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95875\n","elasped time for update: 54.92s\n","episode [ 673/1000] is collected. Total reward is 0.151800\n","elasped time for interaction: 1.21s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95862\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95875\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95864\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95850\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95851\n","elasped time for update: 54.85s\n","episode [ 674/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.50s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95865\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95857\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95846\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95844\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95823\n","elasped time for update: 54.89s\n","episode [ 675/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95849\n","update_step:  20 model loss: 3.00130, kl_loss: 3.00000, obs_loss: 0.00130, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95856\n","update_step:  30 model loss: 3.01100, kl_loss: 3.00000, obs_loss: 0.01100, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95842\n","update_step:  40 model loss: 3.00176, kl_loss: 3.00000, obs_loss: 0.00176, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95823\n","update_step:  50 model loss: 3.00030, kl_loss: 3.00000, obs_loss: 0.00030, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95819\n","elasped time for update: 54.85s\n","episode [ 676/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.38s\n","update_step:  10 model loss: 3.00065, kl_loss: 3.00000, obs_loss: 0.00065, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95831\n","update_step:  20 model loss: 3.00011, kl_loss: 3.00000, obs_loss: 0.00011, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95823\n","update_step:  30 model loss: 3.00002, kl_loss: 3.00000, obs_loss: 0.00002, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95805\n","update_step:  40 model loss: 3.00003, kl_loss: 3.00000, obs_loss: 0.00003, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95798\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95811\n","elasped time for update: 54.85s\n","episode [ 677/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.43s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95807\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95803\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95798\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95793\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95793\n","elasped time for update: 54.77s\n","episode [ 678/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.39s\n","update_step:  10 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95809\n","update_step:  20 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95808\n","update_step:  30 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95784\n","update_step:  40 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95759\n","update_step:  50 model loss: 3.00001, kl_loss: 3.00000, obs_loss: 0.00001, reward_loss: 0.00000, value_loss: 0.00000 action_loss: 0.95792\n","elasped time for update: 54.92s\n","episode [ 679/1000] is collected. Total reward is -0.999200\n","elasped time for interaction: 1.40s\n"]}],"source":["!python3 dreamer/main.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1YTuBCbyP-0n","outputId":"23a67d52-d112-4253-da36-0b8d9377fde4"},"outputs":[{"name":"stdout","output_type":"stream","text":["[INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n","[INFO] Connected new brain: AnimalAI?team=0\n","[WARNING] The environment contains multiple observations. You must define allow_multiple_obs=True to receive them all. Otherwise, only the first visual observation (or vector observation ifthere are no visual observations) will be provided in the observation.\n","Using cuda device\n","Wrapping the env with a `Monitor` wrapper\n","Wrapping the env in a DummyVecEnv.\n","Wrapping the env in a VecTransposeImage.\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/gym/logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n","  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"]},{"name":"stdout","output_type":"stream","text":["Logging to ./tensorboard/dqn_baseline/DQN_2\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 776      |\n","|    ep_rew_mean      | -1.18    |\n","|    exploration_rate | 0.971    |\n","| time/               |          |\n","|    episodes         | 4        |\n","|    fps              | 180      |\n","|    time_elapsed     | 17       |\n","|    total_timesteps  | 3105     |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 802      |\n","|    ep_rew_mean      | -1.21    |\n","|    exploration_rate | 0.939    |\n","| time/               |          |\n","|    episodes         | 8        |\n","|    fps              | 178      |\n","|    time_elapsed     | 35       |\n","|    total_timesteps  | 6417     |\n","----------------------------------\n","----------------------------------\n","| rollout/            |          |\n","|    ep_len_mean      | 804      |\n","|    ep_rew_mean      | -1.22    |\n","|    exploration_rate | 0.908    |\n","| time/               |          |\n","|    episodes         | 12       |\n","|    fps              | 189      |\n","|    time_elapsed     | 50       |\n","|    total_timesteps  | 9652     |\n","----------------------------------\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-16-569001307e35\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mreset_num_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_saves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 43\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"results/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrunname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/model_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mno_steps\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mreset_num_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/dqn/dqn.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0meval_log_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_log_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 267\u001b[0;31m             \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         )\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/off_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    359\u001b[0m                 \u001b[0mlearning_starts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_starts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m                 \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 361\u001b[0;31m                 \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_interval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m             )\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/off_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m             \u001b[0;31m# Rescale and perform action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 587\u001b[0;31m             \u001b[0mnew_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \"\"\"\n\u001b[1;32m    161\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u003e\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/vec_env/vec_transpose.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u003e\u001b[0m \u001b[0mVecEnvStepReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 95\u001b[0;31m         \u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# Transpose the terminal observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             obs, self.buf_rews[env_idx], self.buf_dones[env_idx], self.buf_infos[env_idx] = self.envs[env_idx].step(\n\u001b[0;32m---\u003e 44\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             )\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/monitor.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tried to step environment that needs reset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 90\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines/bench/monitor.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tried to step environment that needs reset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 91\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym_unity/envs/__init__.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 201\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0mdecision_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_agents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecision_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterminal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mlagents_envs/timers.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mhierarchical_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 305\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mlagents_envs/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_behavior_specs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0mrl_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrl_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 353\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrl_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mlagents_envs/environment.py\u001b[0m in \u001b[0;36m_update_state\u001b[0;34m(self, output)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0magent_info_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magentInfos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 self._env_state[brain_name] = steps_from_proto(\n\u001b[0;32m--\u003e 307\u001b[0;31m                     \u001b[0magent_info_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env_specs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m                 )\n\u001b[1;32m    309\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mlagents_envs/timers.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mhierarchical_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 305\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mlagents_envs/rpc_utils.py\u001b[0m in \u001b[0;36msteps_from_proto\u001b[0;34m(agent_info_list, behavior_spec)\u001b[0m\n\u001b[1;32m    329\u001b[0m             decision_obs_list.append(\n\u001b[1;32m    330\u001b[0m                 _process_maybe_compressed_observation(\n\u001b[0;32m--\u003e 331\u001b[0;31m                     \u001b[0mobs_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecision_agent_info_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m                 )\n\u001b[1;32m    333\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mlagents_envs/timers.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mhierarchical_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 305\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mlagents_envs/rpc_utils.py\u001b[0m in \u001b[0;36m_process_maybe_compressed_observation\u001b[0;34m(obs_index, observation_spec, agent_info_list)\u001b[0m\n\u001b[1;32m    257\u001b[0m         batched_visual = [\n\u001b[1;32m    258\u001b[0m             \u001b[0m_observation_to_np_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_obs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobs_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 259\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0magent_obs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magent_info_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         ]\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mlagents_envs/rpc_utils.py\u001b[0m in \u001b[0;36m\u003clistcomp\u003e\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    257\u001b[0m         batched_visual = [\n\u001b[1;32m    258\u001b[0m             \u001b[0m_observation_to_np_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_obs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobs_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 259\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0magent_obs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magent_info_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         ]\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mlagents_envs/timers.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mhierarchical_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 305\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mlagents_envs/rpc_utils.py\u001b[0m in \u001b[0;36m_observation_to_np_array\u001b[0;34m(obs, expected_shape)\u001b[0m\n\u001b[1;32m    236\u001b[0m         )\n\u001b[1;32m    237\u001b[0m         \u001b[0;31m# Compare decompressed image size to observation shape and make sure they match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 238\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m             raise UnityObservationException(\n\u001b[1;32m    240\u001b[0m                 \u001b[0;34mf\"Decompressed observation did not have the expected shape - \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["competition_folder = \"./configs/competition/\"\n","configuration_files = os.listdir(competition_folder)\n","configuration_random = random.randint(0, len(configuration_files))\n","configuration_file = (\n","    competition_folder + configuration_files[configuration_random]\n",")\n","\n","aai_env = AnimalAIEnvironment(\n","        seed = 123,\n","        file_name=\"./env/AnimalAI\",\n","        arenas_configurations=configuration_file,\n","        play=False,\n","        base_port=5000,\n","        inference=False,\n","        useCamera=True,\n","        resolution=36,\n","        useRayCasts=True,\n","        no_graphics=True,\n","        # raysPerSide=1,\n","        # rayMaxDegrees = 30,\n","        worker_id = 3,\n","    )\n","\n","# env = UnityToGymWrapper(aai_env, uint8_visual=False, allow_multiple_obs=True, flatten_branched=False)\n","# def make_env():\n","#     def _thunk():\n","#         env = UnityToGymWrapper(aai_env, uint8_visual=False, allow_multiple_obs=True, flatten_branched=True)\n","#         return env\n","#     return _thunk\n","# env = DummyVecEnv([make_env()])\n","runname = \"dqn_baseline\"\n","log_dir = \"./logs/\" + runname\n","env = UnityToGymWrapper(aai_env, uint8_visual=True, allow_multiple_obs=False, flatten_branched=True)\n","env = Monitor(env, log_dir)\n","\n","policy_kwargs = dict(activation_fn=th.nn.ReLU)\n","model = DQN(\"CnnPolicy\", env, policy_kwargs=policy_kwargs, verbose=1, tensorboard_log=(\"./tensorboard/\" + runname))\n","\n","no_saves = 100\n","no_steps = 1000000\n","reset_num_timesteps = True\n","for i in range(no_saves):\n","    model.learn(no_steps, reset_num_timesteps=reset_num_timesteps)\n","    model.save(\"results/\" + runname + \"/model_\" + str( (i+1)*no_steps ))\n","    reset_num_timesteps = False\n","env.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"44PUTwxOP-0o","outputId":"18764556-5aa1-4afa-b9b1-ea6f77de8245"},"outputs":[{"ename":"NameError","evalue":"name 'env' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-12-1baceacf4cb1\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"]}],"source":["env.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5oYb_2vOP-0o"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"name":"execution.ipynb","version":""},"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"},"kernelspec":{"display_name":"Python 3.7.12 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"}},"nbformat":4,"nbformat_minor":0}